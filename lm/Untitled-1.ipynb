{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPositionBias(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_module_kwargs(kwargs):\n",
    "        return {\n",
    "            'dim': kwargs.get('dpos_dim', 64),\n",
    "            'depth': kwargs.get('dpos_depth', 2),\n",
    "            'log_distance': kwargs.get('dpos_log_distance', False),\n",
    "            'norm': kwargs.get('dpos_norm', False)\n",
    "        }\n",
    "\n",
    "\n",
    "    def forward(self, i, j, device, dtype):\n",
    "        # get the (i x j) matrix of distances\n",
    "        assert i >= 1 and j >= 1 and i <= j, 'I should be in the range [1, j] and j >= 1'\n",
    "        seq_arange = torch.arange(i, device = device)\n",
    "        context_arange = torch.arange(j, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (j-1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-i + 1, (j+i), device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "     \n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicPositionBias(\n",
       "  (mlp): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=222, bias=True)\n",
       "      (1): Identity()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=222, out_features=222, bias=True)\n",
       "      (1): Identity()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Linear(in_features=222, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DynamicPositionBias(\n",
    "    heads=8,\n",
    "    **DynamicPositionBias.fetch_module_kwargs({'dpos_dim':222})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "I should be in the range [1, j] and j >= 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pos \u001b[38;5;241m=\u001b[39m DynamicPositionBias(\u001b[38;5;241m64\u001b[39m, heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, log_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m pos \u001b[38;5;241m=\u001b[39m pos(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      3\u001b[0m pos\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [46], line 27\u001b[0m, in \u001b[0;36mDynamicPositionBias.forward\u001b[0;34m(self, i, j, device, dtype)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, j, device, dtype):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# get the (i x j) matrix of distances\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m j \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m j, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI should be in the range [1, j] and j >= 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     28\u001b[0m     seq_arange \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(i, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m     29\u001b[0m     context_arange \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(j, device \u001b[38;5;241m=\u001b[39m device)\n",
      "\u001b[0;31mAssertionError\u001b[0m: I should be in the range [1, j] and j >= 1"
     ]
    }
   ],
   "source": [
    "pos = DynamicPositionBias(64, heads = 8, depth = 2, log_distance = False)\n",
    "pos = pos(4,3, device = 'cpu', dtype = torch.float32)\n",
    "pos.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(t, groups = 1, dim = -1):\n",
    "    if groups == 1:\n",
    "        return F.normalize(t, p = 2, dim = dim)\n",
    "    t = rearrange(t, '... (g d) -> ... g d', g = groups)\n",
    "    t = F.normalize(t, p = 2, dim = dim)\n",
    "    return rearrange(t, '... g d -> ... (g d)')\n",
    "\n",
    "\n",
    "class CosineAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.shared_kv = kwargs.get('shared_kv', False)\n",
    "        self.talking_heads = kwargs.get('talking_heads', False)\n",
    "        self.cache_kv = kwargs.get('cache_kv', False) # whether prev key and values are used \n",
    "        \n",
    "\n",
    "        self.n_feats, self.head_dim, self.n_heads = n_feats, head_dim, n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        self.causal = causal\n",
    "\n",
    "        if self.talking_heads:\n",
    "            self._head_proj = nn.Conv2d(n_heads, n_heads, (1, 1))\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        if not self.shared_kv:\n",
    "            self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "            self.qkv = lambda x: rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=n_heads, d=head_dim)\n",
    "        else:\n",
    "            self.q_proj, self.kv_proj = [nn.Linear(n_feats, el, bias=bias) for el in [n_heads * head_dim, 2 * head_dim]]\n",
    "            map_q, map_kv = lambda q: rearrange(q, 'b n (h d) -> b h n d', h=n_heads), lambda kv: rearrange(kv, 'b n (kv d) -> kv b () n d', kv=2, d=head_dim)\n",
    "            self.qkv = lambda x: (map_q(self.q_proj(x)), *map_kv(self.kv_proj(x)))\n",
    "\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "        if self.cache_kv:\n",
    "            cache_heads = n_heads if not self.shared_kv else 1\n",
    "            self.cache_vector = torch.nn.Parameter(torch.zeros(1, cache_heads, 1, head_dim), requires_grad=True)\n",
    "            print(self.cache_vector.shape)\n",
    "    \n",
    "    def head_proj(self, dots):\n",
    "        if not self.talking_heads:\n",
    "            return dots\n",
    "        dots = self._head_proj(dots)\n",
    "        return dots      \n",
    "\n",
    "    def attend(self, query, key, value, mask, k_mask, pos_fn):\n",
    "        dots = einsum('bhid,bhjd->bhij', query, key) * self.temperature\n",
    "        dots = self.head_proj(dots)\n",
    "\n",
    "        dots += pos_fn(i=dots.shape[-2], j=dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        qmask, kmask = ~mask, ~k_mask\n",
    "        attn_mask = ~(rearrange(qmask, \"b n -> b () n ()\") * rearrange(kmask, \"b n -> b () () n\"))\n",
    "    \n",
    "        if self.causal: # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = self.activation(dots)\n",
    "     \n",
    "        attn = self.dropout(attn)\n",
    "        return einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "\n",
    "    def lengths_from_mask(self, x, mask):\n",
    "        if mask is None:\n",
    "            return x.shape[-2]\n",
    "        return (~mask).sum(dim=-1)\n",
    "\n",
    "    def attach_cache(self, k, v, mask, cache_kv, cache_mask):\n",
    "        if cache_kv is None:\n",
    "            return k, v, mask\n",
    "        \n",
    "        cache_k, cache_v = cache_kv\n",
    "        \n",
    "        cache_k, cache_v = cache_k.to(k.device), cache_v.to(k.device)\n",
    "        cache_vector = self.cache_vector.to(k.device)\n",
    "        cache_k, cache_v = cache_k + cache_vector, cache_v + cache_vector\n",
    "        cache_lens = self.lengths_from_mask(cache_k, cache_mask)\n",
    "        max_cache_len = cache_lens.max()    \n",
    "        x_lens = self.lengths_from_mask(k, mask)\n",
    "        new_lens = x_lens + cache_lens\n",
    "        max_new_len = new_lens.max()\n",
    "        # so we want to remove excess padding and only have padding at the end of the sequence\n",
    "        # otherwise things get weird with the position encoding\n",
    "        # lets used gather to do this (not sure if there is a faster way)\n",
    "        # fk ill use a for loop to get the indices\n",
    "        indices = []\n",
    "        new_k, new_v = torch.cat([cache_k, k], dim=-2), torch.cat([cache_v, v], dim=-2)\n",
    "   \n",
    "        # add zero to last dimension to use to fetch the padding\n",
    "        B,H,N,D= new_k.shape\n",
    "        zero_vector = torch.zeros((B,H,1,D), device=new_k.device)\n",
    "        new_k,new_v = torch.cat([new_k, zero_vector], dim=-2), torch.cat([new_v, zero_vector], dim=-2)\n",
    "    \n",
    "\n",
    "        for i in range(new_k.shape[0]):\n",
    "            cache_indices = torch.arange(cache_lens[i], device='cpu')\n",
    "            total_length = cache_lens[i] + x_lens[i]\n",
    "            diff_from_max_len = max_new_len - total_length\n",
    "            x_indices = torch.arange(x_lens[i]+diff_from_max_len, device='cpu') + cache_k.shape[-2]\n",
    "            if diff_from_max_len > 0:\n",
    "                x_indices[-diff_from_max_len:] = new_k.shape[-2] - 1\n",
    "            new_indices = torch.cat([cache_indices, x_indices])\n",
    "            indices.append(new_indices)\n",
    "\n",
    "        indices = torch.stack(indices, dim=0)\n",
    "        # NOW LETS GATHER\n",
    "        indices = rearrange(indices, 'b n -> () b () n ()').expand(2,B,H,N,D)\n",
    "        new_kv = torch.stack([new_k, new_v], dim=0) # avoid double gather\n",
    "        new_k, new_v = torch.gather(new_kv, dim=-2, index=indices)\n",
    "       \n",
    "        # just create the new mask\n",
    "        new_mask = torch.arange(max_new_len, device=mask.device) >= new_lens[:, None]\n",
    "        return new_k, new_v, new_mask\n",
    "       \n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, pos_fn, mask=None, cached_kv=None, cached_mask=None):\n",
    "        assert pos_fn is not None, 'pls provide a position function'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "        #print(x.shape, mask.shape)\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, N, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        q, k, v = self.qkv(x)\n",
    "        q, k = map(l2norm, (q, k))\n",
    "\n",
    "        if self.cache_kv:\n",
    "            k, v, k_mask = self.attach_cache(k, v, mask, cached_kv, cached_mask)\n",
    "     \n",
    "        out = self.attend(q, k, v, mask, k_mask, pos_fn)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out if not self.cache_kv else (out, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(2, 1, 8, 256)\n",
    "v = torch.randn(2, 1, 1, 256)\n",
    "(v.expand_as(z) + 0).shape\n",
    "a = nn.Identity(d=2)\n",
    "a(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 32])\n",
      "torch.Size([2, 3, 1, 92, 32])\n",
      "torch.Size([2, 3, 1, 92, 32]) torch.Size([3, 92])\n"
     ]
    }
   ],
   "source": [
    "pos_fn = DynamicPositionBias(dim = 64, heads = 8, log_distance = False, depth = 2)\n",
    "cAttn = CosineAttention(n_feats=256, head_dim=32, n_heads=8, temperature=15.5, cache_kv=True, shared_kv=True, talking_heads=True)\n",
    "x = torch.randn(3, 128, 256)\n",
    "cached_kv = torch.randn(1,3,1,92,32).repeat(2,1,1,1,1)\n",
    "print(cached_kv.shape)\n",
    "cached_mask = torch.zeros(3, 92, dtype=torch.bool)\n",
    "cached_mask[0, -3:] = True\n",
    "print(cached_kv.shape, cached_mask.shape)\n",
    "cached_kv.masked_fill_(rearrange(cached_mask, 'b n -> () b () n ()'), 0)\n",
    "mask = torch.zeros(3, 128, dtype=torch.bool)\n",
    "\n",
    "out = cAttn(x, pos_fn, mask, cached_kv, cached_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_kv[0,0,0,:,0][89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4142, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0,0,:,0][89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6663, -0.3249, -0.4769,  0.2107, -0.3240,  0.4785,  0.9103,  0.3106,\n",
       "         0.2695, -1.3441, -0.2908,  0.7281,  0.1858, -0.2704,  0.1194, -1.1572,\n",
       "        -0.7689, -0.6562, -0.6445, -0.5988,  0.3206, -0.5878,  0.2903, -0.3617,\n",
       "        -0.3463, -0.4374, -0.2675,  0.4168, -0.0143, -0.7390, -0.7874, -0.8283,\n",
       "        -0.7971,  1.3905, -0.3582,  0.5042,  0.0984,  0.5525, -0.8537, -0.7623,\n",
       "        -0.0656,  0.4852,  0.4649, -0.2858, -1.0995, -0.5842,  0.0458, -0.5944,\n",
       "        -0.0380,  0.7912,  0.8925,  0.4276,  0.3902, -0.3050, -0.0830,  1.5923,\n",
       "        -1.3425,  0.0548,  1.2778,  0.2260, -1.4920, -0.6034, -0.5314,  0.4827,\n",
       "        -0.8079, -0.0481,  0.9936,  0.0947,  0.0714,  0.5514, -0.2284,  0.8921,\n",
       "        -0.1457,  0.5982, -0.6773,  0.0830, -0.2965,  0.5944, -0.5165, -0.4533,\n",
       "         0.5581, -1.0282,  0.3890, -0.2361,  0.6638, -0.2990,  0.4568, -0.9507,\n",
       "        -0.1035, -0.4476,  0.0744, -0.0944, -0.2519, -0.0906,  0.2232, -0.3658,\n",
       "         1.7562, -0.5497, -0.4318, -0.2935,  0.0175,  0.7128, -0.2720,  0.6831,\n",
       "        -0.8305, -1.3042,  0.3719, -0.3150, -0.1711,  0.1338,  0.3252,  0.9303,\n",
       "        -1.0017, -1.3004, -0.4476, -0.8663,  0.2672,  0.0961, -0.0229, -0.1404,\n",
       "        -0.1007,  0.0239,  0.0676, -0.1445,  0.4488, -0.9334,  0.8147, -0.0882],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[-2][0,0,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6012, -0.6469, -0.3006, -1.0750,  1.1811,  0.6629, -0.3181, -1.3653,\n",
       "        -2.1848, -0.7446,  0.6312,  0.1165,  0.3485, -0.7637, -0.0511, -0.1435,\n",
       "        -0.4605, -0.5894, -0.3992,  0.1334,  0.1373,  0.9801, -0.9805,  1.1095,\n",
       "        -0.8132, -0.3352,  1.6133, -3.2370, -1.1195,  0.6321,  0.7622, -0.1708],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn, torch\n",
    "\n",
    "class HydraAttention(nn.Module):\n",
    "    def __init__(self, d_model, output_layer='scale_and_bias'):\n",
    "        '''\n",
    "        output_layer: 'scale_and_bias' | 'linear' | 'none'\n",
    "        '''\n",
    "        super(HydraAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        if output_layer == 'scale_and_bias':\n",
    "            self.scale = nn.Parameter(torch.ones(1, 1, d_model))\n",
    "            self.bias = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            self.out = lambda x: x * self.scale + self.bias\n",
    "        elif output_layer == 'linear':\n",
    "            self.out = nn.Linear(d_model, d_model)\n",
    "        elif output_layer == 'none':\n",
    "            self.out = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''x: (B, T, D)'''\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        q = q / q.norm(dim=-1, keepdim=True)\n",
    "        k = k / k.norm(dim=-1, keepdim=True)\n",
    "        kv = (k * v).sum(dim=-2, keepdim=True)\n",
    "        out = q * kv\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.6228e-04,  1.3423e-01,  3.1196e-02,  ..., -5.5381e-02,\n",
       "           2.3028e-02, -1.9155e-02],\n",
       "         [-8.7564e-06,  8.4757e-02,  1.5684e-02,  ...,  1.5657e-02,\n",
       "          -5.4329e-03, -1.1544e-02],\n",
       "         [-9.8780e-05, -1.9900e-02,  2.5104e-02,  ..., -4.6537e-02,\n",
       "          -2.8425e-02, -1.3837e-02],\n",
       "         ...,\n",
       "         [ 3.7448e-04, -7.5378e-02, -6.1634e-02,  ...,  2.0541e-02,\n",
       "           1.8774e-03, -7.0934e-02],\n",
       "         [ 3.6962e-04, -4.1401e-02,  4.3923e-03,  ..., -1.4942e-02,\n",
       "           3.6164e-02, -1.0721e-02],\n",
       "         [-1.0050e-04, -7.9818e-03, -3.8386e-02,  ..., -3.7135e-02,\n",
       "           1.2968e-02, -7.7314e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HydraAttention(256)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/exp/exp1/acp21rjf/deliberation/speachy/tedlium\n"
     ]
    }
   ],
   "source": [
    "%cd ../tedlium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import non_iid_dataloader as niiddl, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'non_iid_dataloader' from '/exp/exp1/acp21rjf/deliberation/speachy/tedlium/non_iid_dataloader.py'>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload as rl\n",
    "rl(niiddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tools.load_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = tools.load_tokenizer('./tokenizers/tokenizer_spe_bpe_v128/tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = niiddl.get_data_loader(\n",
    "    split = corpus['train'],\n",
    "    tokenizer = tkn,\n",
    "    batch_size = 15,\n",
    "    max_duration = 30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dl:\n",
    "    z = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subbatches(audio, audio_lens, tokens, token_lens, segment_lens): # for loops ):\n",
    "    max_segment_len = segment_lens.max()\n",
    "\n",
    "    culm_seglens = segment_lens.cumsum(dim=0)\n",
    "    cur_positions = culm_seglens - segment_lens\n",
    "    sub_batches_indices = []\n",
    "\n",
    "    # first get indices for each sub batch of the \"rnn\"\n",
    "    for ix in range(max_segment_len):\n",
    "        indices = []\n",
    "        for iz in range(len(segment_lens)):\n",
    "            pos = cur_positions[iz].item()\n",
    "            if pos < culm_seglens[iz]:\n",
    "                indices.append(pos)\n",
    "                cur_positions[iz] += 1\n",
    "            else:\n",
    "                indices.append(-1)\n",
    "        sub_batches_indices.append(torch.tensor(indices, dtype=torch.long))\n",
    "    ####\n",
    "    ### after each forward pass the model will return the cached kvs\n",
    "    # this gets the indices of the correct kvs for the next forward pass\n",
    "    non_empty_indices = torch.arange(len(segment_lens), dtype=torch.long)\n",
    "    prev_non_empty_fetch = []\n",
    "    for i in range(len(sub_batches_indices)):\n",
    "        cur = sub_batches_indices[i]\n",
    "        cur = cur[sub_batches_indices[i-1] != -1] if i > 0 else cur\n",
    "        non_empty_indices = non_empty_indices[cur != -1]\n",
    "        prev_non_empty_fetch.append(non_empty_indices.clone())\n",
    "        non_empty_indices = torch.arange(len(non_empty_indices), dtype=torch.long)\n",
    "    ####\n",
    "    sub_batches = []\n",
    "    for i, ix in enumerate(sub_batches_indices):\n",
    "        sbi = ix[ix != -1]\n",
    "        cur_audio, cur_audio_lens, cur_tokens, cur_token_lens = audio[sbi], audio_lens[sbi], tokens[sbi], token_lens[sbi]\n",
    "        # trim audio and tokens to max length in sub batch\n",
    "        max_cur_audio_len, max_cur_token_len = cur_audio_lens.max(), cur_token_lens.max()\n",
    "        cur_audio, cur_tokens = cur_audio[:, :max_cur_audio_len], cur_tokens[:, :max_cur_token_len]\n",
    "        sub_batches.append({\n",
    "            'audio': cur_audio,\n",
    "            'audio_lens': cur_audio_lens,\n",
    "            'tokens': cur_tokens,\n",
    "            'token_lens': cur_token_lens,\n",
    "            'prev_state_indices': prev_non_empty_fetch[i] if i > 0 else None, # for the first sub batch there is no previous state  \n",
    "        })\n",
    "        \n",
    "    return sub_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(sub_batch, device):\n",
    "    for k, v in sub_batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            sub_batch[k] = v.to(device)\n",
    "    return sub_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = create_subbatches(**z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['audio', 'audio_lens', 'tokens', 'token_lens', 'prev_state_indices'])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 15040,  49600, 119200], dtype=torch.int32)"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb[3]['audio_lens'][sb[4]['prev_state_indices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([133440, 112480,  15040,  49600, 119200,  93280, 125440],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb[3]['audio_lens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3991e-07, 9.9999e-01, 8.4997e-07, 2.7006e-06, 3.9144e-07, 3.5621e-07,\n",
       "        1.0080e-06, 1.0284e-06, 6.0161e-06, 1.2595e-06, 2.9979e-07, 6.0915e-07])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(1, 12,256)\n",
    "k = q.clone() + torch.randn(1, 12,256)*0.1\n",
    "dots = torch.einsum('bnd,bmd->bnm', q, k) / q.shape[-1]**0.5\n",
    "dots.softmax(dim=-1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'unique_id': 'f6a3719a-1aa1-48c0-9792-4a28ffd69cb8',\n",
       "  'timings': {'segment_start': 437.65, 'segment_end': 441.61},\n",
       "  'recording_id': 'GeverTulley_2007U',\n",
       "  'utterance_id': 'GeverTulley_2007U-31',\n",
       "  'speaker': 'GeverTulley_2007U'}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z['metadata'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['audio', 'audio_lens', 'tokens', 'token_lens', 'segment_lens'])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 265920])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z['audio'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
