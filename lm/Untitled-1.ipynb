{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPositionBias(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, i, j, device, dtype):\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(i, device = device)\n",
    "        context_arange = torch.arange(j, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (j-1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-i + 1, (j+i), device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "    \n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 7])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = DynamicPositionBias(64, heads = 8, depth = 2, log_distance = False)\n",
    "pos = pos(6,7, device = 'cpu', dtype = torch.float32)\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(t, groups = 1, dim = -1):\n",
    "    if groups == 1:\n",
    "        return F.normalize(t, p = 2, dim = dim)\n",
    "    t = rearrange(t, '... (g d) -> ... g d', g = groups)\n",
    "    t = F.normalize(t, p = 2, dim = dim)\n",
    "    return rearrange(t, '... g d -> ... (g d)')\n",
    "\n",
    "\n",
    "class CosineAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.shared_kv = kwargs.get('shared_kv', False)\n",
    "        self.talking_heads = kwargs.get('talking_heads', False)\n",
    "        self.cache_kv = kwargs.get('cache_kv', False) # whether prev key and values are used \n",
    "        \n",
    "\n",
    "        self.n_feats, self.head_dim, self.n_heads = n_feats, head_dim, n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        self.causal = causal\n",
    "\n",
    "        if self.talking_heads:\n",
    "            self._head_proj = nn.Conv2d(n_heads, n_heads, (1, 1))\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        if not self.shared_kv:\n",
    "            self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "            self.qkv = lambda x: rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=n_heads, d=head_dim)\n",
    "        else:\n",
    "            self.q_proj, self.kv_proj = [nn.Linear(n_feats, el, bias=bias) for el in [n_heads * head_dim, 2 * head_dim]]\n",
    "            map_q, map_kv = lambda q: rearrange(q, 'b n (h d) -> b h n d', h=n_heads), lambda kv: rearrange(kv, 'b n (kv d) -> kv b () n d', kv=2, d=head_dim)\n",
    "            self.qkv = lambda x: (map_q(self.q_proj(x)), *map_kv(self.kv_proj(x)))\n",
    "\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "        if self.cache_kv:\n",
    "            cache_heads = n_heads if not self.shared_kv else 1\n",
    "            self.cache_vector = torch.nn.Parameter(torch.zeros(1, cache_heads, 1, head_dim), requires_grad=True)\n",
    "            print(self.cache_vector.shape)\n",
    "    \n",
    "    def head_proj(self, dots):\n",
    "        if not self.talking_heads:\n",
    "            return dots\n",
    "        dots = self._head_proj(dots)\n",
    "        return dots      \n",
    "\n",
    "    def attend(self, query, key, value, mask, pos_fn):\n",
    "        dots = einsum('bhid,bhjd->bhij', query, key) * self.temperature\n",
    "        dots = self.head_proj(dots)\n",
    "\n",
    "        dots += pos_fn(i=dots.shape[-2], j=dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        qkmask = ~mask\n",
    "        attn_mask = ~(rearrange(qkmask, \"b n -> b () n ()\") * rearrange(qkmask, \"b n -> b () () n\"))\n",
    "    \n",
    "        if self.causal: # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = self.activation(dots)\n",
    "     \n",
    "        attn = self.dropout(attn)\n",
    "        return einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "\n",
    "    def lengths_from_mask(self, x, mask):\n",
    "        if mask is None:\n",
    "            return x.shape[-2]\n",
    "        return (~mask).sum(dim=-1)\n",
    "\n",
    "    def attach_cache(self, k, v, mask, cache_kv, cache_mask):\n",
    "        if cache_kv is None:\n",
    "            return k, v, mask\n",
    "        \n",
    "        cache_k, cache_v = cache_kv\n",
    "        \n",
    "        cache_k, cache_v = cache_k.to(k.device), cache_v.to(k.device)\n",
    "        cache_vector = self.cache_vector.to(k.device)\n",
    "        cache_k, cache_v = cache_k + cache_vector, cache_v + cache_vector\n",
    "        cache_lens = self.lengths_from_mask(cache_k, cache_mask)\n",
    "        max_cache_len = cache_lens.max()    \n",
    "        x_lens = self.lengths_from_mask(k, mask)\n",
    "        new_lens = x_lens + cache_lens\n",
    "        max_new_len = new_lens.max()\n",
    "        # so we want to remove excess padding and only have padding at the end of the sequence\n",
    "        # otherwise things get weird with the position encoding\n",
    "        # lets used gather to do this (not sure if there is a faster way)\n",
    "        # fk ill use a for loop to get the indices\n",
    "        indices = []\n",
    "        new_k, new_v = torch.cat([cache_k, k], dim=-2), torch.cat([cache_v, v], dim=-2)\n",
    "   \n",
    "        # add zero to last dimension to use to fetch the padding\n",
    "        B,H,N,D= new_k.shape\n",
    "        zero_vector = torch.zeros((B,H,1,D), device=new_k.device)\n",
    "        new_k,new_v = torch.cat([new_k, zero_vector], dim=-2), torch.cat([new_v, zero_vector], dim=-2)\n",
    "    \n",
    "\n",
    "        for i in range(new_k.shape[0]):\n",
    "            cache_indices = torch.arange(cache_lens[i], device='cpu')\n",
    "            total_length = cache_lens[i] + x_lens[i]\n",
    "            diff_from_max_len = max_new_len - total_length\n",
    "            x_indices = torch.arange(x_lens[i]+diff_from_max_len, device='cpu') + cache_k.shape[-2]\n",
    "            if diff_from_max_len > 0:\n",
    "                x_indices[-diff_from_max_len:] = new_k.shape[-2] - 1\n",
    "            new_indices = torch.cat([cache_indices, x_indices])\n",
    "            indices.append(new_indices)\n",
    "\n",
    "        indices = torch.stack(indices, dim=0)\n",
    "        # NOW LETS GATHER\n",
    "        indices = rearrange(indices, 'b n -> () b () n ()').expand(2,B,H,N,D)\n",
    "        new_kv = torch.stack([new_k, new_v], dim=0) # avoid double gather\n",
    "        new_kv = torch.gather(new_kv, dim=-2, index=indices)\n",
    "        new_k, new_v = new_kv\n",
    "        # just create the new mask\n",
    "        new_mask = torch.arange(max_new_len, device=mask.device) >= new_lens[:, None]\n",
    "        return new_k, new_v, new_mask, k, v\n",
    "       \n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, pos_fn, mask=None, cached_kv=None, cached_mask=None):\n",
    "        assert pos_fn is not None, 'pls provide a position function'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "        #print(x.shape, mask.shape)\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, N, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        q, k, v = self.qkv(x)\n",
    "        q, k = map(l2norm, (q, k))\n",
    "        if self.cache_kv:\n",
    "            new_k, new_v, new_mask, k, v = self.attach_cache(k, v, mask, cached_kv, cached_mask)\n",
    "        return new_k, new_v, new_mask, k, v\n",
    "        out = self.attend(q, k, v, mask, pos_fn)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out if not self.cache_kv else (out, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128+92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "pos_fn = DynamicPositionBias(dim = 64, heads = 8, log_distance = False, depth = 2)\n",
    "cAttn = CosineAttention(n_feats=256, head_dim=32, n_heads=8, temperature=15.5, cache_kv=True, shared_kv=True, talking_heads=True)\n",
    "x = torch.randn(3, 128, 256)\n",
    "cached_kv = torch.randn(1,3,1,92,32).expand(2,-1,-1,-1,-1)\n",
    "cached_mask = torch.zeros(3, 92, dtype=torch.bool)\n",
    "cached_mask[0, -3:] = True\n",
    "mask = torch.zeros(3, 128, dtype=torch.bool)\n",
    "\n",
    "out = cAttn(x, pos_fn, mask, cached_kv, cached_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3906,  0.0710,  0.0172, -0.2074, -0.0701,  0.2890,  0.3494, -0.0264,\n",
       "        -0.1111, -0.4142, -0.0788,  0.0519, -0.0026,  0.0767, -0.0388, -0.0247,\n",
       "        -0.1080,  0.2853,  0.0020, -0.1601, -0.2944,  0.0677, -0.1763, -0.0125,\n",
       "         0.1275, -0.2477, -0.1884, -0.0234,  0.1822, -0.0721, -0.3212,  0.1506,\n",
       "         0.2194, -0.1693,  0.0064,  0.0831, -0.1904,  0.3044,  0.1207, -0.1494,\n",
       "        -0.0674,  0.1864,  0.0760,  0.1781,  0.0145, -0.1953,  0.2129,  0.0245,\n",
       "        -0.3183,  0.1486, -0.3075,  0.0647, -0.0644, -0.1859, -0.1389,  0.0577,\n",
       "        -0.0197, -0.1308,  0.2411,  0.3444,  0.1453,  0.0641,  0.2817, -0.0930,\n",
       "         0.0313, -0.0715, -0.1922, -0.0799, -0.3564,  0.1434,  0.2053,  0.2263,\n",
       "        -0.1312,  0.0874, -0.0185,  0.2556, -0.2807,  0.1241,  0.2018,  0.0927,\n",
       "        -0.1649, -0.2063, -0.0261,  0.0648,  0.0185,  0.0333, -0.1019, -0.1100,\n",
       "         0.1062, -0.0849,  0.0339, -0.1424,  0.0999, -0.1539, -0.0010, -0.0622,\n",
       "         0.2464,  0.0281,  0.0983, -0.0470,  0.2797, -0.0613,  0.0159,  0.2647,\n",
       "        -0.1888, -0.2220,  0.0296, -0.1954,  0.1442, -0.0590, -0.0006,  0.0856,\n",
       "        -0.1605, -0.0462, -0.3190,  0.0735,  0.2566,  0.0826,  0.2430, -0.0759,\n",
       "         0.3073,  0.0619,  0.0927,  0.1672,  0.5555, -0.1282,  0.1279, -0.1037],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[-2][0,0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2074, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0,0,:,0][92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn, torch\n",
    "\n",
    "class HydraAttention(nn.Module):\n",
    "    def __init__(self, d_model, output_layer='scale_and_bias'):\n",
    "        '''\n",
    "        output_layer: 'scale_and_bias' | 'linear' | 'none'\n",
    "        '''\n",
    "        super(HydraAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        if output_layer == 'scale_and_bias':\n",
    "            self.scale = nn.Parameter(torch.ones(1, 1, d_model))\n",
    "            self.bias = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            self.out = lambda x: x * self.scale + self.bias\n",
    "        elif output_layer == 'linear':\n",
    "            self.out = nn.Linear(d_model, d_model)\n",
    "        elif output_layer == 'none':\n",
    "            self.out = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''x: (B, T, D)'''\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        q = q / q.norm(dim=-1, keepdim=True)\n",
    "        k = k / k.norm(dim=-1, keepdim=True)\n",
    "        kv = (k * v).sum(dim=-2, keepdim=True)\n",
    "        out = q * kv\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.6228e-04,  1.3423e-01,  3.1196e-02,  ..., -5.5381e-02,\n",
       "           2.3028e-02, -1.9155e-02],\n",
       "         [-8.7564e-06,  8.4757e-02,  1.5684e-02,  ...,  1.5657e-02,\n",
       "          -5.4329e-03, -1.1544e-02],\n",
       "         [-9.8780e-05, -1.9900e-02,  2.5104e-02,  ..., -4.6537e-02,\n",
       "          -2.8425e-02, -1.3837e-02],\n",
       "         ...,\n",
       "         [ 3.7448e-04, -7.5378e-02, -6.1634e-02,  ...,  2.0541e-02,\n",
       "           1.8774e-03, -7.0934e-02],\n",
       "         [ 3.6962e-04, -4.1401e-02,  4.3923e-03,  ..., -1.4942e-02,\n",
       "           3.6164e-02, -1.0721e-02],\n",
       "         [-1.0050e-04, -7.9818e-03, -3.8386e-02,  ..., -3.7135e-02,\n",
       "           1.2968e-02, -7.7314e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HydraAttention(256)(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
