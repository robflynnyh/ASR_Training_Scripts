{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import non_iid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-01-04 12:43:03 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-01-04 12:43:04 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = tools.load_tokenizer(model_path='tedlium/tokenizers/tokenizer_spe_bpe_v128/tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import TokenizerCollator, isfalse, istrue, exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetings = non_iid_dataloader.prepare_partition(tools.load_corpus()['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = non_iid_dataloader.prepare_samples(\n",
    "    meetings=meetings, \n",
    "    max_duration=46.0,\n",
    "    split_speakers=False,\n",
    "    gap=0.1,\n",
    "    speaker_gap=False,\n",
    "    single_speaker_with_gaps=False,\n",
    "    max_allowed_utterance_gap=10.0,\n",
    "    concat_samples=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from non_iid_dataloader import collate_audio\n",
    "class Minimal_IID_Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, tokenizer: TokenizerCollator, all_cuts, text_only=False):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.all_cuts = all_cuts\n",
    "    self.text_only = text_only\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.all_cuts)\n",
    "\n",
    "  def __getitem__(self, idx) -> dict:\n",
    "    text_only = self.text_only\n",
    "    cuts = self.all_cuts[idx]\n",
    "    audios, audio_lens = collate_audio(cuts) if isfalse(text_only) else (None, None)\n",
    "    #print(len(cuts))\n",
    "    tokens, token_lens = self.tokenizer(cuts=cuts) if isfalse(text_only) else self.tokenizer(cuts=None, text=cuts)\n",
    "    print(tokens.shape)\n",
    "    return {\n",
    "        \"audio\": audios,\n",
    "        \"audio_lens\": audio_lens,\n",
    "        \"tokens\": tokens,\n",
    "        \"token_lens\": token_lens,\n",
    "    } if isfalse(text_only) else {\n",
    "        \"tokens\": tokens,\n",
    "        \"token_lens\": token_lens,\n",
    "        \"text\": cuts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(cutlist):\n",
    "    all_text = [\"\"]\n",
    "    for i in range(len(cutlist)):\n",
    "        meeting = cutlist[i]\n",
    "        for z in range(len(meeting)):\n",
    "            utt = meeting[z]\n",
    "            for supervision in utt.supervisions:\n",
    "                all_text[-1] += supervision.text.strip() if all_text[-1] == \"\" else \" \" + supervision.text.strip()\n",
    "            if z < len(meeting) - 1:\n",
    "                all_text[-1] += \" //ut_end//\" # add a separator between contiguous utterances (concat samples vs. split samples)\n",
    "        all_text.append(\"\") if i < len(cutlist) - 1 else None \n",
    "    \n",
    "    all_text = [tools.transform_txt(el) for el in all_text]\n",
    "    all_text = [el.strip() for el in all_text]\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencollator = TokenizerCollator(tkn, text_only=True, pad_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Minimal_IID_Dataset(tokencollator, get_text(samples), text_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2], [1, 3]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(list, zip(*[[0,1],[2,3]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 469])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[ 90,   1, 109, 115,  21,  22,   2,  10, 122,  17,  54,  51,   7, 115,\n",
       "          106, 113,  15,  25, 106, 101,  66,  36,  22,  65, 119, 119,  10,  29,\n",
       "          107,  19,  46,   9, 113, 113,  80,   8, 111,  22,  18, 101,  82,   7,\n",
       "           44, 101,  64,  47,  25,  42,   1, 103,  87,  23,  50,  47, 112,   8,\n",
       "           31,   7, 115, 106,  79, 109,  25, 105, 126,  21,  42,  98,  87,  22,\n",
       "           26,  28,  84,  22, 117,  61, 108,  13,  42,  98,  87,   7,  21,  21,\n",
       "           22, 117,  61, 108,  13,  19,  69,   4, 118,  43, 123,  30,  23,  56,\n",
       "           95, 119,   5,  38, 108,  62,   4,   5,  14,  67,  98,  87,  22,  41,\n",
       "           59,  48, 101,  16, 115,   6,   5,  70,  37,  45,  20,  43,   3, 119,\n",
       "          119, 110, 103, 112, 111,  36,  37,  43,  26,  75,  46,  31,   3,   1,\n",
       "          115, 119,  48,  53,  41,  32, 102, 108,   3, 116,  13,  48,  28,   5,\n",
       "           70,  37,  51,  43,  42,   8, 100,  96, 112, 113,  14, 107,  19,  31,\n",
       "          113, 104, 108, 101,   8, 106, 102,  43,   2,  10, 122,  43,  15,  14,\n",
       "           67,  85,  46,   4,   5,  14,  67,  98,  87,  22,  94, 101,   4, 118,\n",
       "           62,   3, 112, 111, 105,  20,  84,   5,  14,  67,  41, 104,  43,  65,\n",
       "          121,  20, 123, 102,  23,  96,  66,  48,  36,   3, 107,  24,  39,   3,\n",
       "          107,   4,  83, 106,  54,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0]]),\n",
       " 'token_lens': tensor([229], dtype=torch.int32),\n",
       " 'text': \"was trying to think how is sync connected to happiness and it occurred to me for some reason we take pleasure in synchronizing we like to dance together we like singing together and so if you'll put up with this i would like to notice by the way that when you applauded that you did it in a typical north american way that is you were raucous and incoherent you think you could do it i would like to see if this audience would no you haven't practiced as far as i know\"}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = get_text(samples)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"was trying to think how is sync connected to happiness and it occurred to me for some reason we take pleasure in synchronizing we like to dance together we like singing together and so if you'll put up with this i would like to notice by the way that when you applauded that you did it in a typical north american way that is you were raucous and incoherent you think you could do it i would like to see if this audience would no you haven't practiced as far as i know\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = non_iid_dataloader.get_data_loader(\n",
    "    split = tools.load_corpus()['train'],\n",
    "    tokenizer = tkn,\n",
    "    batch_size = 100,\n",
    "    concat_samples = False,\n",
    "    max_duration = 30,\n",
    "    text_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[  6,   5,  32,  ...,  57, 101,  46],\n",
       "         [ 42,  41, 101,  ...,   0,   0,   0],\n",
       "         [ 42,   7, 108,  ...,   0,   0,   0],\n",
       "         [ 19,  46, 123,  ...,   0,   0,   0]]),\n",
       " 'token_lens': tensor([282, 215, 180, 150], dtype=torch.int32),\n",
       " 'text': ['the world during the same year every flash is an edit somebody somewhere looking at the open street map and realizing it could be better you can see europe some places perhaps not as much as they should be here focusing in on haiti the map of port au prince at the end of two thousand and nine was not all it could be not as good as the map of california fortunately just after the earthquake geoeye a commercial company released satellite imagery with a license which allowed the open source community to use it',\n",
       "  \"we need more sophisticated technologies that are automated that will speed up the process of finding connectomes now you've probably seen pictures of neurons before you can recognize them instantly by their fantastic shapes they extend long and delicate branches and in short they look like trees but this is just a single neuron in order to find connectomes we have to see all the neurons at the same time\",\n",
       "  'we should just get rid of them but in the entire rest of government right now and for the last at least thirty years there has been a culture of deregulation that is caused directly by the people who we need to be protected from buying the government out from under us complained that they were already bidding defiance to the laws of our country',\n",
       "  \"and it's something that people have wanted forever lots of people are used to having a spiritual tribe or a church tribe having a work tribe having a community tribe but now thanks to the internet thanks to the explosion of mass media tribes are everywhere the internet was\"]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 314])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in dl:\n",
    "    v=z\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 40, 256)\n",
    "mask = torch.ones(3, 40).bool()\n",
    "x = x.masked_fill(mask.unsqueeze(-1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.nn.Conv1d(256, 256, 40, groups=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 40])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ami\t\t\t\t     log.txt\t\t    speachy.egg-info\n",
      "checkpoints\t\t\t     model_configs\t    sweep.yaml\n",
      "checkpoints_done\t\t     model_utils.py\t    tedlium\n",
      "compute_rescore_wer.py\t\t     non_iid_dataloader.py  tedlium_hyps.pkl\n",
      "distance_bias_LM_not_pretrained.svg  ppls.pkl\t\t    tools.py\n",
      "eval_perplexity.py\t\t     __pycache__\t    train_LM.py\n",
      "experiment_configs\t\t     README.md\t\t    train_lm.sh\n",
      "ipynbs\t\t\t\t     rescore_with_TLM.py    wandb\n",
      "lm\t\t\t\t     setup.py\n",
      "lm_utils.py\t\t\t     speachy\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tedlium_hyps.pkl','rb') as f:\n",
    "    a=pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.7083, dtype=torch.float64)\n",
      "tensor(-0.7083, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "am = torch.tensor(a[0]['beams'][0][0]['am_score'])\n",
    "am = (am.exp()*0.02).log()\n",
    "am = am /10\n",
    "print(am)\n",
    "print((am.exp()).log())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
