{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "        print(pos.shape)\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNSPPositionBias(nn.Module):\n",
    "    '''Adapted From Phil Wang's x-transformers library for specific case of cross-attention'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, qn, kn, device, dtype, lengths): # set dtype and device to the same as q and k\n",
    "\n",
    "        max_len, min_len = lengths.max().item(), lengths.min().item()\n",
    "        padding_lens = max_len - lengths\n",
    "        max_padding = padding_lens.max().item()\n",
    "\n",
    "        # get the (qn x kn) matrix of distances\n",
    "        seq_arange = torch.arange(kn, device = device, dtype = torch.long) - kn # -kn ... -1\n",
    "        seq_arange = repeat(seq_arange, 'k -> q k', q = qn) # repeat for each query\n",
    "        seq_arange = seq_arange - torch.arange(qn, device = device, dtype = torch.long).unsqueeze(-1) # matrix of relative distance between query and keys\n",
    "\n",
    "        seq_arange = repeat(seq_arange, 'q k -> b q k', b = len(lengths)) # repeat for each el in batch\n",
    "        seq_arange = seq_arange + padding_lens[:, None, None] # add padding offsets\n",
    "        \n",
    "        minval = seq_arange.min()\n",
    "        seq_arange -= minval # shift to positive values\n",
    "\n",
    "        pos = torch.arange(minval, max_padding, device = device, dtype = dtype).unsqueeze(-1)\n",
    "        #return pos[seq_arange].squeeze() # shows the positions that are being used (look at this to debug)\n",
    "       \n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        bias = pos[seq_arange]\n",
    "      \n",
    "        return rearrange(bias, 'b q k h -> b h q k') # add this to dot product of q and k\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 512\n",
    "n_heads = 8\n",
    "dpos = DynamicNSPPositionBias(\n",
    "    dim = dim_model // 4,\n",
    "    heads = n_heads,\n",
    "    depth = 2,\n",
    "    log_distance = False,\n",
    "    norm = False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)[:,None,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lengths \u001b[38;5;241m-\u001b[39m lengths\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lengths' is not defined"
     ]
    }
   ],
   "source": [
    "lengths - lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 30, 150])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140,\n",
       "        -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128,\n",
       "        -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116,\n",
       "        -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104,\n",
       "        -103, -102, -101, -100,  -99,  -98,  -97,  -96,  -95,  -94,  -93,  -92,\n",
       "         -91,  -90,  -89,  -88,  -87,  -86,  -85,  -84,  -83,  -82,  -81,  -80,\n",
       "         -79,  -78,  -77,  -76,  -75,  -74,  -73,  -72,  -71,  -70,  -69,  -68,\n",
       "         -67,  -66,  -65,  -64,  -63,  -62,  -61,  -60,  -59,  -58,  -57,  -56,\n",
       "         -55,  -54,  -53,  -52,  -51,  -50,  -49,  -48,  -47,  -46,  -45,  -44,\n",
       "         -43,  -42,  -41,  -40,  -39,  -38,  -37,  -36,  -35,  -34,  -33,  -32,\n",
       "         -31,  -30,  -29,  -28,  -27,  -26,  -25,  -24,  -23,  -22,  -21,  -20,\n",
       "         -19,  -18,  -17,  -16,  -15,  -14,  -13,  -12,  -11,  -10,   -9,   -8,\n",
       "          -7,   -6,   -5,   -4,   -3,   -2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.squeeze()[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 36,  73, 134, 136,  54, 144, 144, 114,  60, 150])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 64 is out of bounds for dimension 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos[pos[pos \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 64 is out of bounds for dimension 0 with size 10"
     ]
    }
   ],
   "source": [
    "pos[pos > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23,\n",
       "        -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10,  -9,\n",
       "         -8,  -7,  -6,  -5,  -4,  -3,  -2,  -1,   0,   1,   2,   3,   4,   5,\n",
       "          6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,\n",
       "         20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,\n",
       "         34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,\n",
       "         48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n",
       "         62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,\n",
       "         76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
       "         90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.squeeze()[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos\u001b[38;5;241m.\u001b[39msqueeze()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos' is not defined"
     ]
    }
   ],
   "source": [
    "pos.squeeze()[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lengths \u001b[38;5;241m-\u001b[39m lengths\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lengths' is not defined"
     ]
    }
   ],
   "source": [
    "lengths - lengths.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos\u001b[38;5;241m.\u001b[39msqueeze()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos' is not defined"
     ]
    }
   ],
   "source": [
    "pos.squeeze()[-2][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos' is not defined"
     ]
    }
   ],
   "source": [
    "pos.min(-1).values.min(-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m repeat(pos, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq k -> b q k\u001b[39m\u001b[38;5;124m'\u001b[39m, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos' is not defined"
     ]
    }
   ],
   "source": [
    "repeat(pos, 'q k -> b q k', b = 10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m qn, kn \u001b[38;5;241m=\u001b[39m dots\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m      8\u001b[0m pos \u001b[38;5;241m=\u001b[39m dpos(qn, kn, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, lengths\u001b[38;5;241m=\u001b[39mlengths) \u001b[38;5;66;03m# position bias only needs to be calculated once and can be reused for all layers\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m dots \u001b[38;5;241m=\u001b[39m dots \u001b[38;5;241m+\u001b[39m pos\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "q = torch.randn(10, 8, 30, 32)\n",
    "k = torch.randn(10, 8, 150, 32)\n",
    "lengths = torch.randint(30, 150, (10,))\n",
    "lengths[-1] = 150\n",
    "\n",
    "dots = torch.einsum('b h i d, b h j d -> b h i j', q, k) # cross attention for dot product\n",
    "qn, kn = dots.shape[-2:]\n",
    "pos = dpos(qn, kn, device='cpu', dtype=torch.float32, lengths=lengths) # position bias only needs to be calculated once and can be reused for all layers\n",
    "dots = dots + pos # POSITIONIFIED \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
