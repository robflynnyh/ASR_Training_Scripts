{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleit(obj, path):\n",
    "    import pickle as pkl\n",
    "    with open(path, 'wb') as f:\n",
    "        pkl.dump(obj, f)\n",
    "\n",
    "def loadit(path):\n",
    "    import pickle as pkl\n",
    "    with open(path, 'rb') as f:\n",
    "        return pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'qknorm_attention' from '/exp/exp1/acp21rjf/deliberation/speachy/ipynbs/qknorm_attention.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload as rl\n",
    "import qknorm_attention \n",
    "rl(qknorm_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "from torch.utils.checkpoint import checkpoint # # gradient/activation checkpointing\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# token shifting\n",
    "# lucidrains implementation: https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py\n",
    "# BlinkDL idea from RWKV-LM https://github.com/BlinkDL/RWKV-LM\n",
    "def shift(t, amount, mask = None):\n",
    "    if amount == 0:\n",
    "        return t\n",
    "    else:\n",
    "        amount = min(amount, t.shape[1])\n",
    "\n",
    "    if exists(mask):\n",
    "        t = t.masked_fill(~mask[..., None], 0.)\n",
    "\n",
    "    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n",
    "\n",
    "class ShiftTokens(nn.Module):\n",
    "    def __init__(self, shifts, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.shifts = tuple(shifts)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        mask = kwargs.get('mask', None)\n",
    "        shifts = self.shifts\n",
    "        segments = len(shifts)\n",
    "        feats_per_shift = x.shape[-1] // segments\n",
    "        splitted = x.split(feats_per_shift, dim = -1)\n",
    "        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n",
    "        segments_to_shift = list(map(lambda args: shift(*args, mask = mask), zip(segments_to_shift, shifts)))\n",
    "        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            activation()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                activation()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "\n",
    "    def forward(self, pos, indices, device, dtype):\n",
    "        pos = pos.to(device=device, dtype=dtype)\n",
    "        \n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos) \n",
    "      \n",
    "        bias = pos[indices]\n",
    "        #print(bias.shape)\n",
    "        bias = rearrange(bias, 'b i j h -> b h i j')\n",
    "        return bias\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1,))\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device = device).type_as(self.inv_freq)\n",
    "        sinu = einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n",
    "        return emb * self.scale\n",
    "\n",
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(t, groups = 1, dim = -1):\n",
    "    if groups == 1:\n",
    "        return F.normalize(t, p = 2, dim = dim)\n",
    "    t = rearrange(t, '... (g d) -> ... g d', g = groups)\n",
    "    t = F.normalize(t, p = 2, dim = dim)\n",
    "    return rearrange(t, '... g d -> ... (g d)')\n",
    "\n",
    "class CosineAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.shared_kv = kwargs.get('shared_kv', False)\n",
    "        self.talking_heads = kwargs.get('talking_heads', False)\n",
    "\n",
    "        self.n_feats, self.head_dim, self.n_heads = n_feats, head_dim, n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        self.causal = causal\n",
    "\n",
    "        if self.talking_heads:\n",
    "            self._head_proj = nn.Conv2d(n_heads, n_heads, (1, 1))\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        if not self.shared_kv:\n",
    "            self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "            self.qkv = lambda x: rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=n_heads, d=head_dim)\n",
    "        else:\n",
    "            self.q_proj, self.kv_proj = [nn.Linear(n_feats, el, bias=bias) for el in [n_heads * head_dim, 2 * head_dim]]\n",
    "            map_q, map_kv = lambda q: rearrange(q, 'b n (h d) -> b h n d', h=n_heads), lambda kv: rearrange(kv, 'b n (kv d) -> kv b () n d', kv=2, d=head_dim)\n",
    "            self.qkv = lambda x: (map_q(self.q_proj(x)), *map_kv(self.kv_proj(x)))\n",
    "\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "    \n",
    "    def head_proj(self, dots):\n",
    "        if not self.talking_heads:\n",
    "            return dots\n",
    "        dots = self._head_proj(dots)\n",
    "        return dots      \n",
    "\n",
    "    def attend(self, query, key, value, attn_mask, pos_bias):\n",
    "        query, key = map(l2norm, (query, key))\n",
    "        \n",
    "        dots = einsum('bhid,bhjd->bhij', query, key) * self.temperature\n",
    "        dots = self.head_proj(dots)\n",
    "        #print(dots.shape, pos_bias.shape)\n",
    "\n",
    "        dots += pos_bias  \n",
    "\n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "\n",
    "        attn = self.activation(dots)\n",
    "     \n",
    "        attn = self.dropout(attn)\n",
    "        return einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attach_cache(kv, cache, cache_indices):\n",
    "        kv = torch.stack(kv, dim=0)\n",
    "        if cache is None:\n",
    "            return kv\n",
    "        zero_vector = torch.zeros_like(kv[:, :, :, :1, :])\n",
    "        kv_w_cache = torch.cat([cache, kv, zero_vector], dim=-2)\n",
    "        kv_w_cache = torch.gather(kv_w_cache, dim=-2, index=cache_indices) # we do this to remove unnecessary padding\n",
    "        return kv_w_cache\n",
    "\n",
    "    def forward(self, x, pos_bias, mask, cache=None, cache_indices=None):\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "    \n",
    "        q, k, v  = self.qkv(x)\n",
    "        kv = self.attach_cache([k, v], cache, cache_indices)\n",
    "        k, v = kv\n",
    "\n",
    "        out = self.attend(q, k, v, mask, pos_bias)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out, kv\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(self.norm(x), *args, **kwargs)\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim = -1)\n",
    "        return x * self.act(gate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim, \n",
    "            depth, \n",
    "            heads, \n",
    "            dim_head, \n",
    "            causal=True,\n",
    "            temperature=15.5,\n",
    "            shared_temperture=False,\n",
    "            intermediate_loss=True,\n",
    "            dropout = 0.1,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            intermediate_loss = False\n",
    "\n",
    "        ff_mult = kwargs.get('ff_mult', 4)\n",
    "        self.checkpoint_every_n = kwargs.get('checkpoint_every_n', 0)\n",
    "        self.token_shift = kwargs.get('token_shift', False)\n",
    "        self.causal = causal\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature), requires_grad=True) if shared_temperture else temperature\n",
    "    \n",
    "\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.depth = depth\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = dim // 4,\n",
    "            heads = heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.token_shifter = lambda x: x\n",
    "        if self.token_shift:\n",
    "            self.token_shifter = ShiftTokens(range(0, 2), nn.Identity())\n",
    "        self.token_shift = lambda x: self.token_shifter(x)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, CosineAttention(\n",
    "                    dim, \n",
    "                    n_heads=heads, \n",
    "                    head_dim=dim_head, \n",
    "                    causal=causal,\n",
    "                    temperature=self.temperature,\n",
    "                    dropout=dropout,\n",
    "                    **kwargs\n",
    "                )),\n",
    "                PreNorm(dim, self.ff(dim, mult=ff_mult))\n",
    "            ]))\n",
    "\n",
    "    @staticmethod\n",
    "    def ff(dim, mult=4, dropout=0.1):\n",
    "        return nn.Sequential(\n",
    "            GLU(dim, dim * mult, nn.SiLU()),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_custom_forward(module):\n",
    "        def custom_forward(*args, **kwargs):\n",
    "            return module(*args, **kwargs)\n",
    "        return custom_forward\n",
    "\n",
    "    def checkpoint(self, layer, module, *args, **kwargs):\n",
    "        condition = self.training and self.checkpoint_every_n != 0 and layer < self.depth - 1 and layer % self.checkpoint_every_n == 0\n",
    "        return checkpoint(self.create_custom_forward(module), *args, **kwargs) if condition else module(*args, **kwargs)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cache(cache, layer):\n",
    "        if cache is None:\n",
    "            return None\n",
    "        return cache['cache'][layer]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cache_indices(x_lens, cache_lens, cache_kv, x):  \n",
    "        # used later w/ gather to remove padding when cache is concatenated with current input to remove padding\n",
    "        max_new_len = (x_lens + cache_lens).max()\n",
    "\n",
    "        B, H, N, D = x.shape[0], 1, (x.shape[1] + cache_kv.shape[-2]), cache_kv.shape[-1]\n",
    "        indices = []\n",
    "        for i in range(B): # stinky for loop to sort out indices for gather \n",
    "            cache_indices = torch.arange(cache_lens[i], device='cpu')\n",
    "            total_length = cache_lens[i] + x_lens[i]\n",
    "            diff_from_max_len = max_new_len - total_length\n",
    "            x_indices = torch.arange(x_lens[i]+diff_from_max_len, device='cpu') + cache_kv.shape[-2]\n",
    "            if diff_from_max_len > 0:\n",
    "                x_indices[-diff_from_max_len:] = N # last index will be used for padding\n",
    "            new_indices = torch.cat([cache_indices, x_indices])\n",
    "            indices.append(new_indices)\n",
    "\n",
    "        indices = torch.stack(indices, dim=0)\n",
    "        \n",
    "        indices = rearrange(indices, 'b n -> () b () n ()').expand(2, B, H,-1, D) # 2 for key and value\n",
    "        return indices.to(x.device)\n",
    "\n",
    "\n",
    "    def create_masks_and_positions(self, x, length, cache): \n",
    "        x_len = length if length is not None else torch.tensor(x.shape[-2]).expand(x.shape[0])\n",
    "        cache_len = cache['cache_lengths'] if exists(cache) else 0\n",
    "        total_len = x_len + cache_len\n",
    "        kv_mask = torch.arange(total_len.max(), device=x.device).expand(len(total_len), -1) >= total_len.unsqueeze(-1)\n",
    "        q_mask = torch.arange(x_len.max(), device=x.device).expand(len(x_len), -1) >= x_len.unsqueeze(-1)\n",
    "        attn_mask = ~(rearrange(~q_mask, \"b n -> b () n ()\") * rearrange(~kv_mask, \"b n -> b () () n\"))\n",
    "        ##\n",
    "        ##\n",
    "        causal_mask = repeat(torch.arange(total_len.max()), 'i -> b r i', b=len(total_len), r=x_len.max())\n",
    "        cache_offset = cache_len[:,None,None] if exists(cache) else cache_len\n",
    "        diagonal_offset = torch.arange(x_len.max())[None,:,None]\n",
    "        ##\n",
    "        ## positional stuff ##\n",
    "        positional_grid = (causal_mask - cache_offset - diagonal_offset) * -1 \n",
    "        pos = torch.arange(positional_grid.min(), positional_grid.max()+1)[:,None]\n",
    "        min_cache_len = 0 if cache_len.__class__ == int else cache_len.min()\n",
    "        positional_indices = (positional_grid + (total_len.max() - min_cache_len - 1)) # shift so zero is the smallest number\n",
    "        pos_bias = self.positional_bias(pos=pos, indices=positional_indices, dtype=x.dtype, device=x.device)\n",
    "        ## positional stuff ##\n",
    "        ##\n",
    "        if self.causal:\n",
    "            causal_mask = causal_mask >= (cache_offset + diagonal_offset + 1)\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask[:,None])\n",
    "        ##\n",
    "        return q_mask, attn_mask, total_len, x_len, cache_len, pos_bias\n",
    "\n",
    "    def forward(self, x, length=None, self_condtioning=None, cache=None):\n",
    "        intermediate_logits = []\n",
    "        cached_kvs = []\n",
    "    \n",
    "        mask, attn_mask, total_lens, x_len, cache_len, pos_bias = self.create_masks_and_positions(x, length, cache)\n",
    "    \n",
    "        #print(cache_len if exists(cache) else None)\n",
    "        cache_indices = self.get_cache_indices(x_len, cache_len, cache['cache'], x) if exists(cache) else None\n",
    "    \n",
    "        for i, (attn, ff) in enumerate(self.layers):\n",
    "\n",
    "            x = self.token_shift(x)\n",
    "            a_out, kv = self.checkpoint(i, attn, x, pos_bias, attn_mask, self.get_cache(cache, layer=i), cache_indices)\n",
    "            x = a_out + x\n",
    "            cached_kvs.append(kv)\n",
    "            x = self.checkpoint(i, ff, x) + x   \n",
    "\n",
    "            if i < self.depth - 1 and self_condtioning is not None:\n",
    "                x, logits = self_condtioning(x)\n",
    "                intermediate_logits.append(logits)\n",
    "\n",
    "        if len(intermediate_logits) > 0: # stack intermediate logits\n",
    "            intermediate_logits = torch.stack(intermediate_logits, dim=0) # D x B x N x L\n",
    "\n",
    "        cached_kvs = torch.stack(cached_kvs, dim=0) if len(cached_kvs) > 0 else None\n",
    "        cached_kvs = {'cache_lengths': total_lens, 'cache': cached_kvs} if exists(cached_kvs) else None\n",
    "\n",
    "\n",
    "        return x, intermediate_logits, cached_kvs\n",
    "\n",
    "class shared_embedding_output_layer(nn.Module):\n",
    "    '''Pass a embedding layer and then use this module as the output layer'''\n",
    "    def __init__(self, embedding_layer, bias=False):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.use_bias = bias\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(embedding_layer.weight.shape[0]))#\n",
    "            nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, weight=self.embedding_layer.weight, bias=self.bias if self.use_bias else None)\n",
    "\n",
    "\n",
    "class transformer_lm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        causal=True,\n",
    "        temperature=15.5,\n",
    "        dropout=0.,\n",
    "        shared_temperture=True,\n",
    "        self_conditioning=False,\n",
    "        intermediate_loss=True,\n",
    "        use_abs_pos=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self_conditioning == False\n",
    "\n",
    "        self.self_conditioning = True if self_conditioning else None\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.use_abs_pos = use_abs_pos\n",
    "        if self.use_abs_pos:\n",
    "            self.abs_pos_fn = ScaledSinuEmbedding(dim=dim)\n",
    "        self.abs_pos = lambda x: x + self.abs_pos_fn(x) if self.use_abs_pos else x\n",
    "\n",
    "        if self_conditioning:\n",
    "            self.reprojection_layer = nn.Linear(vocab_size, dim)\n",
    "\n",
    "        self.layers = transformer(\n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = heads, \n",
    "            dim_head = dim_head, \n",
    "            causal = causal, \n",
    "            dropout = dropout,\n",
    "            temperature = temperature,\n",
    "            shared_temperture = shared_temperture,\n",
    "            intermediate_loss = intermediate_loss,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.tie_embedding = kwargs.get('tie_embedding', False)\n",
    "        print('Tie embedding:', self.tie_embedding) if self.tie_embedding else None\n",
    " \n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "\n",
    "        self.to_logits = shared_embedding_output_layer(self.embedding) if self.tie_embedding else nn.Linear(dim, vocab_size)\n",
    "\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "\n",
    "    def self_condition_fn(self):\n",
    "        def self_condition(x):\n",
    "            logits = self.to_logits(self.post_norm(x))\n",
    "            if self.self_conditioning: # not effective for LMs (intermediate loss is tho)\n",
    "                z = F.softmax(logits, dim=-1)\n",
    "                z = self.reprojection_layer(z)\n",
    "                x = z + x\n",
    "            return x, logits\n",
    "        return self_condition if (self.self_conditioning or self.intermediate_loss) and self.training else None\n",
    "\n",
    "\n",
    "    def forward(self, x, length=None, cache:Dict=None):\n",
    "        '''\n",
    "        x: [B, N] (embedding indices)\n",
    "        length: [B] (length of each sequence)\n",
    "        cache: {cache_lengths: [B, N], cache: [L, KV, B, H, N, D]} KV: key and value (2)\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        x = self.abs_pos(x) \n",
    "  \n",
    "        x, interim_logits, cached_kvs = self.layers(x, length, self_condtioning=self.self_condition_fn(), cache=cache)\n",
    "        x = self.post_norm(x)\n",
    "        x = self.to_logits(x)\n",
    "\n",
    "        return  x, interim_logits, cached_kvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def collate_fn(tensors:List[torch.Tensor], pad_token:int):\n",
    "    max_len = max([t.shape[0] for t in tensors])\n",
    "    lengths = torch.tensor([t.shape[0] for t in tensors])\n",
    "    padded_tensors = [torch.cat([t, torch.full((max_len - t.shape[0],), pad_token, dtype=t.dtype)], dim=0) for t in tensors]\n",
    "    return torch.stack(padded_tensors, dim=0), lengths\n",
    "    \n",
    "class CharacterTokenizer(): # only for testing\n",
    "    def __init__(self):\n",
    "        self.vocab = ['#', '/'] + list(string.ascii_lowercase) + [' ',\"'\"] # bos/eos -> /, pad -> #\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.tokenize(text)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.token_to_id[token] for token in text]\n",
    "\n",
    "tokenizer = CharacterTokenizer()\n",
    "model = transformer_lm(\n",
    "    dim = 256,\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    depth = 10,\n",
    "    heads = 1,\n",
    "    dim_head = 32,\n",
    "    dropout=0.0,\n",
    "    causal = True,\n",
    "    shared_kv = True,\n",
    ")\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speachy.rescoring.tools import (\n",
    "        sort_hypothesis_by_recording, \n",
    "        order_recordings_by_start_time,\n",
    "        interpolate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "beams = loadit('test_beams.pkl')\n",
    "beams = sort_hypothesis_by_recording(beams)\n",
    "beams = order_recordings_by_start_time(beams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unique_id': 'ef221fbd-1cb6-4ac6-a6a3-06d52337b7ad',\n",
       " 'timings': {'segment_start': 149.51, 'segment_end': 158.66},\n",
       " 'recording_id': 'AimeeMullins_2009P',\n",
       " 'utterance_id': 'AimeeMullins_2009P-19',\n",
       " 'speaker': 'AimeeMullins_2009P'}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beams['AimeeMullins_2009P'][13]['meta_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_b1, s2_b1, s3_b1 = torch.tensor(tokenizer('/hi')), torch.tensor(tokenizer('/buenos')), torch.tensor(tokenizer('/whats'))\n",
    "s1_b2, s2_b2, s3_b2 = torch.tensor(tokenizer(' there')), torch.tensor(tokenizer(' dias')), torch.tensor(tokenizer(' up'))\n",
    "s1_b3, s2_b3, s3_b3 = torch.tensor(tokenizer(' how')), torch.tensor(tokenizer(' captain')), torch.tensor(tokenizer(' donkey'))\n",
    "s1_b4, s2_b4, s3_b4 = torch.tensor(tokenizer(' u/')), torch.tensor(tokenizer(' hook/')), torch.tensor(tokenizer(' man/'))\n",
    "#\n",
    "b1, b1_lengths = collate_fn([s1_b1, s2_b1, s3_b1], pad_token=tokenizer.token_to_id['#'])\n",
    "b2, b2_lengths = collate_fn([s1_b2, s2_b2, s3_b2], pad_token=tokenizer.token_to_id['#'])\n",
    "b3, b3_lengths = collate_fn([s1_b3, s2_b3, s3_b3], pad_token=tokenizer.token_to_id['#'])\n",
    "b4, b4_lengths = collate_fn([s1_b4, s2_b4, s3_b4], pad_token=tokenizer.token_to_id['#'])\n",
    "# comparsion set\n",
    "f_1, f_2, f_3 = torch.tensor(tokenizer('/hi there how u/')), torch.tensor(tokenizer('/buenos dias captain hook/')), torch.tensor(tokenizer('/whats up donkey man/'))\n",
    "fb, fb_lengths = collate_fn([f_1, f_2, f_3], pad_token=tokenizer.token_to_id['#'])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_s1, interim_logits, cached_kvs = model(b1, length=b1_lengths)\n",
    "    logits_s2, interim_logits, cached_kvs_s2 = model(b2, length=b2_lengths, cache=cached_kvs)\n",
    "    logits_s3, interim_logits, cached_kvs_s3 = model(b3, length=b3_lengths, cache=cached_kvs_s2)\n",
    "    logits_s4, interim_logits, cached_kvs_s4 = model(b4, length=b4_lengths, cache=cached_kvs_s3)\n",
    "    logits_fs, interim_logits, cached_kvs_fs = model(fb, length=fb_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1, 10, 29, 14, 28,  4,  6, 13,  6,  3, 19,  2, 21,  6,  5, 28,  7,\n",
       "           16, 19, 28, 21,  9,  6, 28, 16, 17, 17, 16, 19, 21, 22, 15, 10, 21,\n",
       "           10,  6, 20, 28,  2, 15,  5, 28,  2,  5, 23,  6, 15, 21, 22, 19,  6,\n",
       "           20, 28, 14, 26, 28, 13, 10,  7,  6, 28,  9,  2, 23,  6, 28, 17, 19,\n",
       "            6,  4,  2, 19,  6,  5, 28, 20, 16, 28, 10, 28, 10, 14, 14,  6,  5,\n",
       "           10,  2, 21,  6, 13, 26]]]),\n",
       " tensor([1]))"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn([t1], pad_token=tokenizer.token_to_id['#'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = beams['AimeeMullins_2009P'][13]['beams'][0][0]['text']\n",
    "t2 = beams['AimeeMullins_2009P'][14]['beams'][0][0]['text']\n",
    "t3 = beams['AimeeMullins_2009P'][15]['beams'][0][0]['text']\n",
    "tall = t1 + ' ' + t2 + ' ' + t3\n",
    "\n",
    "t1, t1_lengths = collate_fn([torch.tensor(tokenizer('/'+t1))], pad_token=tokenizer.token_to_id['#'])\n",
    "t2, t2_lengths = collate_fn([torch.tensor(tokenizer(' '+t2))], pad_token=tokenizer.token_to_id['#'])\n",
    "t3, t3_lengths = collate_fn([torch.tensor(tokenizer(' '+t3+'/'))], pad_token=tokenizer.token_to_id['#'])\n",
    "tall, tall_lengths = collate_fn([torch.tensor(tokenizer('/'+tall+'/'))], pad_token=tokenizer.token_to_id['#'])\n",
    "assert torch.allclose(torch.cat([t1,t2,t3], dim=1), tall), 'problem'\n",
    "model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    logits_t1, interim_logits, cached_kvs = model(t1, length=t1_lengths)\n",
    "    logits_t2, interim_logits, cached_kvs_t2 = model(t2, length=t2_lengths, cache=cached_kvs)\n",
    "    logits_t3, interim_logits, cached_kvs_t3 = model(t3, length=t3_lengths, cache=cached_kvs_t2)\n",
    "    logits_tall, interim_logits, cached_kvs_tall = model(tall, length=tall_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([91])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:  torch.Size([10, 2, 1, 1, 359, 32]) torch.Size([10, 2, 1, 1, 359, 32])\n",
      "tensor([-0.3114,  0.0542,  0.2616,  0.2090,  0.0317,  1.2295, -0.7231, -0.6544,\n",
      "        -0.1883, -0.6242, -0.1923,  0.7391,  0.2027,  1.3316, -0.2743,  1.0983,\n",
      "         0.6546,  0.1309,  0.1635,  0.1180,  0.4662,  0.3145, -0.6290, -0.0828,\n",
      "         0.8946, -0.5393, -0.0604, -0.3130,  0.0901,  0.4395, -1.7591,  0.8176])\n",
      "\n",
      "tensor([-0.3114,  0.0542,  0.2616,  0.2090,  0.0317,  1.2295, -0.7231, -0.6544,\n",
      "        -0.1883, -0.6242, -0.1923,  0.7391,  0.2027,  1.3316, -0.2743,  1.0983,\n",
      "         0.6546,  0.1309,  0.1635,  0.1180,  0.4662,  0.3145, -0.6290, -0.0828,\n",
      "         0.8946, -0.5393, -0.0604, -0.3130,  0.0901,  0.4395, -1.7591,  0.8176])\n"
     ]
    }
   ],
   "source": [
    "print('shapes: ', cached_kvs_tall['cache'].shape, cached_kvs_t3['cache'].shape)\n",
    "c_lens = cached_kvs_tall['cache_lengths']\n",
    "mask = torch.arange(c_lens.max())[:,None] < c_lens[None,:]\n",
    "mask = ~mask.T\n",
    "mask = rearrange(mask, 'b i -> () () b () i ()')\n",
    "fs_cache =  cached_kvs_tall['cache'].masked_fill(mask, 0)\n",
    "\n",
    "N = -1\n",
    "L = 3\n",
    "I = 0\n",
    "kv = 0\n",
    "print(fs_cache[L,kv,I,0,N])\n",
    "print()\n",
    "print(cached_kvs_t3['cache'][L,kv,I,0,N])\n",
    "assert torch.allclose(fs_cache[L,kv,I,0,N], cached_kvs_t3['cache'][L,kv,I,0,N], rtol=0.0001), 'failed custom check'\n",
    "assert torch.allclose(fs_cache, cached_kvs_t3['cache'], atol=0.0001), 'failed full check'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:  torch.Size([10, 2, 3, 1, 26, 32]) torch.Size([10, 2, 3, 1, 26, 32])\n",
      "tensor([ 0.3459, -1.1566, -0.2918,  0.4598,  0.7112, -0.0111,  0.0634,  0.8649,\n",
      "        -0.4751,  0.4753, -0.2709, -0.4881,  0.3553, -0.2075, -0.6533, -0.4618,\n",
      "        -0.8889,  0.3626,  0.3111, -0.6107, -1.2687,  0.4969, -1.2984, -0.1768,\n",
      "         0.4505, -0.2144, -0.0749, -0.3803, -0.4090,  0.0089, -0.3851,  0.6563])\n",
      "\n",
      "tensor([ 0.3459, -1.1566, -0.2918,  0.4598,  0.7112, -0.0111,  0.0634,  0.8649,\n",
      "        -0.4751,  0.4753, -0.2709, -0.4881,  0.3553, -0.2075, -0.6533, -0.4618,\n",
      "        -0.8889,  0.3626,  0.3111, -0.6107, -1.2687,  0.4969, -1.2984, -0.1768,\n",
      "         0.4505, -0.2144, -0.0749, -0.3803, -0.4090,  0.0089, -0.3851,  0.6563])\n"
     ]
    }
   ],
   "source": [
    "print('shapes: ', cached_kvs_fs['cache'].shape, cached_kvs_s4['cache'].shape)\n",
    "c_lens = cached_kvs_fs['cache_lengths']\n",
    "mask = torch.arange(c_lens.max())[:,None] < c_lens[None,:]\n",
    "mask = ~mask.T\n",
    "mask = rearrange(mask, 'b i -> () () b () i ()')\n",
    "fs_cache =  cached_kvs_fs['cache'].masked_fill(mask, 0)\n",
    "\n",
    "N = 10\n",
    "L = 3\n",
    "I = 0\n",
    "kv = 0\n",
    "print(fs_cache[L,kv,I,0,N])\n",
    "print()\n",
    "print(cached_kvs_s4['cache'][L,kv,I,0,N])\n",
    "assert torch.allclose(fs_cache[L,kv,I,0,N], cached_kvs_s4['cache'][L,kv,I,0,N], rtol=0.0001), 'failed custom check'\n",
    "assert torch.allclose(fs_cache, cached_kvs_s4['cache'], atol=0.0001), 'failed full check'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1, f_2, f_3 = torch.tensor(tokenizer('/hi there how u/')), torch.tensor(tokenizer('/buenos dias captain hook/')), torch.tensor(tokenizer('/whats up donkey man/'))\n",
    "fb, fb_lengths = collate_fn([f_1, f_2, f_3], pad_token=tokenizer.token_to_id['#'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_utils\n",
    "msk = lm_utils.token_lens_to_mask(fb_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "OOOM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits \u001b[38;5;241m=\u001b[39m model2(fb, mask\u001b[38;5;241m=\u001b[39mmsk)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/ipynbs/qknorm_attention.py:411\u001b[0m, in \u001b[0;36mtransformer_lm.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    408\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m    409\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mabs_pos(x)\n\u001b[0;32m--> 411\u001b[0m x, interim_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x, mask\u001b[39m=\u001b[39;49m\u001b[39m~\u001b[39;49mmask \u001b[39mif\u001b[39;49;00m mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, self_condtioning\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_condition_fn())\n\u001b[1;32m    412\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_norm(x)\n\u001b[1;32m    413\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_logits(x)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/ipynbs/qknorm_attention.py:312\u001b[0m, in \u001b[0;36mtransformer.forward\u001b[0;34m(self, x, mask, self_condtioning)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mfor\u001b[39;00m i, (attn, ff) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m    311\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_shift(x)\n\u001b[0;32m--> 312\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint(i, attn, x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpositional_bias, mask) \u001b[39m+\u001b[39m x\n\u001b[1;32m    313\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint(i, ff, x) \u001b[39m+\u001b[39m x   \n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m self_condtioning \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/ipynbs/qknorm_attention.py:305\u001b[0m, in \u001b[0;36mtransformer.checkpoint\u001b[0;34m(self, layer, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheckpoint\u001b[39m(\u001b[39mself\u001b[39m, layer, module, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    304\u001b[0m     condition \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_every_n \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m layer \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m layer \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_every_n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mreturn\u001b[39;00m checkpoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_custom_forward(module), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mif\u001b[39;00m condition \u001b[39melse\u001b[39;00m module(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/ipynbs/qknorm_attention.py:215\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/ipynbs/qknorm_attention.py:202\u001b[0m, in \u001b[0;36mCosineAttention.forward\u001b[0;34m(self, x, pos_fn, mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(B, N, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbool)\n\u001b[1;32m    200\u001b[0m q, k, v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqkv(x)\n\u001b[0;32m--> 202\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattend(q, k, v, mask, pos_fn)\n\u001b[1;32m    204\u001b[0m out \u001b[39m=\u001b[39m rearrange(out, \u001b[39m\"\u001b[39m\u001b[39mb h n d -> b n (h d)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_proj(out)\n",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/ipynbs/qknorm_attention.py:176\u001b[0m, in \u001b[0;36mCosineAttention.attend\u001b[0;34m(self, query, key, value, mask, pos_fn)\u001b[0m\n\u001b[1;32m    173\u001b[0m dots \u001b[39m=\u001b[39m einsum(\u001b[39m'\u001b[39m\u001b[39mbhid,bhjd->bhij\u001b[39m\u001b[39m'\u001b[39m, query, key) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemperature\n\u001b[1;32m    174\u001b[0m dots \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_proj(dots)\n\u001b[0;32m--> 176\u001b[0m dots \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pos_fn(dots\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], device\u001b[39m=\u001b[39;49mdots\u001b[39m.\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49mdots\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    177\u001b[0m qkmask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mmask\n\u001b[1;32m    178\u001b[0m attn_mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39m(rearrange(qkmask, \u001b[39m\"\u001b[39m\u001b[39mb n -> b () n ()\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m*\u001b[39m rearrange(qkmask, \u001b[39m\"\u001b[39m\u001b[39mb n -> b () () n\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/ipynbs/qknorm_attention.py:83\u001b[0m, in \u001b[0;36mDynamicPositionBias.forward\u001b[0;34m(self, n, device, dtype)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpos_og.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m     pkl\u001b[39m.\u001b[39mdump({\u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m:pos,\u001b[39m'\u001b[39m\u001b[39mindices\u001b[39m\u001b[39m'\u001b[39m:indices}, f)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mMemoryError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOOOM\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_distance:\n\u001b[1;32m     86\u001b[0m     pos \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msign(pos) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mlog(pos\u001b[39m.\u001b[39mabs() \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)  \u001b[39m# log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\u001b[39;00m\n",
      "\u001b[0;31mMemoryError\u001b[0m: OOOM"
     ]
    }
   ],
   "source": [
    "logits = model2(fb, mask=msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [128], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pos_n \u001b[38;5;241m=\u001b[39m loadit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(pos_og\u001b[38;5;241m.\u001b[39mshape, pos_n\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "pos_og = loadit('pos_og.pkl')\n",
    "pos_n = loadit('pos.pkl')\n",
    "import matplotlib.pyplot as plt\n",
    "print(pos_og.shape, pos_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_b1, s2_b1, s3_b1 = torch.tensor(tokenizer('/hi')), torch.tensor(tokenizer('/buenos')), torch.tensor(tokenizer('/whats'))\n",
    "s1_b2, s2_b2, s3_b2 = torch.tensor(tokenizer(' there')), torch.tensor(tokenizer(' dias')), torch.tensor(tokenizer(' up'))\n",
    "s1_b3, s2_b3, s3_b3 = torch.tensor(tokenizer(' how')), torch.tensor(tokenizer(' captain')), torch.tensor(tokenizer(' donkey'))\n",
    "s1_b4, s2_b4, s3_b4 = torch.tensor(tokenizer(' u/')), torch.tensor(tokenizer(' hook/')), torch.tensor(tokenizer(' man/'))\n",
    "#\n",
    "b1, b1_lengths = collate_fn([s1_b1, s2_b1, s3_b1], pad_token=tokenizer.token_to_id['#'])\n",
    "b2, b2_lengths = collate_fn([s1_b2, s2_b2, s3_b2], pad_token=tokenizer.token_to_id['#'])\n",
    "b3, b3_lengths = collate_fn([s1_b3, s2_b3, s3_b3], pad_token=tokenizer.token_to_id['#'])\n",
    "b4, b4_lengths = collate_fn([s1_b4, s2_b4, s3_b4], pad_token=tokenizer.token_to_id['#'])\n",
    "# comparsion set\n",
    "f_1, f_2, f_3 = torch.tensor(tokenizer('/hi there how u/')), torch.tensor(tokenizer('/buenos dias captain hook/')), torch.tensor(tokenizer('/whats up donkey man/'))\n",
    "fb, fb_lengths = collate_fn([f_1, f_2, f_3], pad_token=tokenizer.token_to_id['#'])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_s1, interim_logits, cached_kvs = model(b1, length=b1_lengths)\n",
    "    logits_s2, interim_logits, cached_kvs_s2 = model(b2, length=b2_lengths, cache=cached_kvs)\n",
    "    logits_s3, interim_logits, cached_kvs_s3 = model(b3, length=b3_lengths, cache=cached_kvs_s2)\n",
    "    logits_s4, interim_logits, cached_kvs_s4 = model(b4, length=b4_lengths, cache=cached_kvs_s3)\n",
    "    logits_fs, interim_logits, cached_kvs_fs = model(fb, length=fb_lengths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:  torch.Size([10, 2, 1, 1, 359, 32]) torch.Size([10, 2, 1, 1, 359, 32])\n",
      "tensor([ 0.5536, -0.3304,  0.0476,  0.2446, -0.0482, -0.4085, -0.3490,  0.4924,\n",
      "        -0.8021,  1.2335,  0.3822,  0.2436, -1.0480,  0.4469,  0.0447, -1.2148,\n",
      "         0.0777, -0.0473,  0.2860,  0.3998, -0.2467, -0.1733,  0.6739,  0.6215,\n",
      "        -0.4094,  1.0916, -0.6580, -0.0763,  0.1811,  0.5381,  0.1956, -0.2564])\n",
      "\n",
      "tensor([ 0.7813, -0.0427,  0.2886,  0.4575, -0.1542, -0.2837, -0.2342,  0.7809,\n",
      "        -1.0759,  1.4217, -0.0597,  0.2150, -1.1848,  0.2526,  0.2526, -1.4922,\n",
      "         0.2373,  0.0745,  0.1201,  0.4343, -0.2039, -0.2317,  0.3664,  0.5032,\n",
      "        -0.3036,  1.1762, -0.4488, -0.1990,  0.2627,  0.6553,  0.0775, -0.2171])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "failed custom check",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [78], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(cached_kvs_t3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m][L,kv,I,\u001b[38;5;241m0\u001b[39m,N])\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(fs_cache[L,kv,I,\u001b[38;5;241m0\u001b[39m,N], cached_kvs_t3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m][L,kv,I,\u001b[38;5;241m0\u001b[39m,N], rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed custom check\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(fs_cache, cached_kvs_t3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m], atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed full check\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: failed custom check"
     ]
    }
   ],
   "source": [
    "print('shapes: ', cached_kvs_tall['cache'].shape, cached_kvs_t3['cache'].shape)\n",
    "c_lens = cached_kvs_tall['cache_lengths']\n",
    "mask = torch.arange(c_lens.max())[:,None] < c_lens[None,:]\n",
    "mask = ~mask.T\n",
    "mask = rearrange(mask, 'b i -> () () b () i ()')\n",
    "fs_cache =  cached_kvs_tall['cache'].masked_fill(mask, 0)\n",
    "\n",
    "N = 101\n",
    "L = 3\n",
    "I = 0\n",
    "kv = 0\n",
    "print(fs_cache[L,kv,I,0,N])\n",
    "print()\n",
    "print(cached_kvs_t3['cache'][L,kv,I,0,N])\n",
    "assert torch.allclose(fs_cache[L,kv,I,0,N], cached_kvs_t3['cache'][L,kv,I,0,N], rtol=0.0001), 'failed custom check'\n",
    "assert torch.allclose(fs_cache, cached_kvs_t3['cache'], atol=0.0001), 'failed full check'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for dimension 4 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cached_kvs_fs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m][L,kv,I,\u001b[38;5;241m0\u001b[39m,N][\u001b[38;5;241m0\u001b[39m], cached_kvs_s2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m][L,kv,I,\u001b[38;5;241m0\u001b[39m,N][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 15 is out of bounds for dimension 4 with size 12"
     ]
    }
   ],
   "source": [
    "cached_kvs_fs['cache'][L,kv,I,0,N][0], cached_kvs_s2['cache'][L,kv,I,0,N][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 3, 1, 21, 32])\n",
      "torch.Size([10, 2, 3, 1, 12, 32])\n"
     ]
    }
   ],
   "source": [
    "print(cached_kvs_fs['cache'].shape)\n",
    "print(cached_kvs_s2['cache'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7635, -0.1934,  0.6801, -0.2448, -0.5811,  0.6995, -0.2468]) tensor([ 0.7635, -0.1934,  0.6801, -0.2448, -0.5811,  0.6995, -0.2468])\n"
     ]
    }
   ],
   "source": [
    "print(logits_s1[B, :, D], logits_fs[B, :logits_s1.shape[1], D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0880,  1.2568,  0.6429, -0.5517, -0.0935]) tensor([-0.0880,  1.2568,  0.6429, -0.5517, -0.0935])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x_og, i_logits, cache_kv = model(torch.tensor(tokenizer('/hello bro/')).unsqueeze(0))\n",
    "\n",
    "    x, i_logits, cache_kv = model(torch.tensor(tokenizer('/hello')).unsqueeze(0))\n",
    "    x_c, i_logits, cache_kv = model(torch.tensor(tokenizer(' bro/')).unsqueeze(0), cache=cache_kv)\n",
    "        #print(cache_kv.shape)\n",
    "\n",
    "    #print(x_og.shape, x_c.shape)\n",
    "print(x_og[0,6:, 0], x_c[0,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  9,  6, 13, 13, 16, 28,  3, 19, 16,  1])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor(tokenizer('/hello')), torch.tensor(tokenizer(' bro/'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 29]) torch.Size([1, 5, 29])\n",
      "tensor([-0.0880,  1.2568,  0.6429, -0.5517, -0.0935]) tensor([-0.0880,  1.2568,  0.6429, -0.5517, -0.0935])\n"
     ]
    }
   ],
   "source": [
    "print(x_og.shape, x_c.shape)\n",
    "print(x_og[0,6:, 0], x_c[0,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(x, length, cache):\n",
    "    x_len = length if length is not None else torch.tensor(x.shape[-2]).expand(x.shape[0])\n",
    "    cache_len = 0 if cache is None else cache['length']\n",
    "    total_len = x_len + cache_len\n",
    "    kv_mask = torch.arange(total_len.max(), device=x.device).expand(len(total_len), -1) >= total_len.unsqueeze(-1)\n",
    "    q_mask = torch.arange(x_len.max(), device=x.device).expand(len(x_len), -1) >= x_len.unsqueeze(-1)\n",
    "    attn_mask = ~(rearrange(~q_mask, \"b n -> b () n ()\") * rearrange(~kv_mask, \"b n -> b () () n\"))\n",
    "\n",
    "    if 1==1: #causal\n",
    "        causal_mask = torch.ones(attn_mask.shape[-2], attn_mask.shape[-1], device=x.device).triu(1 + attn_mask.shape[-2] - attn_mask.shape[-1]).bool()\n",
    "        attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "    return q_mask, attn_mask, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_masks(x=torch.rand(1, 10,3), length=None, cache={'length': torch.tensor([5])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_len = 0# torch.tensor([5, 10, 15])\n",
    "x_len = torch.tensor([10, 13, 15])\n",
    "total_len = x_len + cache_len\n",
    "cache = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = repeat(torch.arange(total_len.max()), 'i -> b r i', b=len(total_len), r=x_len.max())\n",
    "cache_offset = cache_len[:,None,None] if exists(cache) else cache_len\n",
    "diagonal_offset = torch.arange(x_len.max())[None,:,None]\n",
    "##\n",
    "## positional stuff ##\n",
    "positional_grid = causal_mask - cache_offset - diagonal_offset \n",
    "pos = torch.arange(positional_grid.min(), positional_grid.max()+1)[:,None]\n",
    "min_cache_len = 0 if cache_len.__class__ == int else cache_len.min()\n",
    "positional_indices = ((positional_grid*-1) + (total_len.max() - min_cache_len - 1)) # shift so zero is the smallest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_og = loadit('pos_og.pkl')\n",
    "pos_n = loadit('pos_n.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(torch.allclose(pos_og['pos'],pos_n['pos']))\n",
    "print(torch.allclose(pos_og['indices'],pos_n['indices'][0]))\n",
    "print(torch.allclose(pos_og['bias'],pos_n['bias'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 51, 51])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_og['bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1cb5e71c40>]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pos_og['bias'].squeeze()[0, -1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1cb5e263a0>]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0XElEQVR4nO3de3iU9Z3//9dMkpnMJJkcIAcC4XwIHkAKSkNrRWDVttuylaXWpVWqlapQq/Dda6E/xbq9WOy5Yg/24Ba66HpaXW1ru6uCuBUEjUYEIRwEEsgRQmZymkMy9++PyQxEkpDAJDNz5/m4rrnmdM/knftC87o+h/dtMQzDEAAAgIlYY10AAABAtBFwAACA6RBwAACA6RBwAACA6RBwAACA6RBwAACA6RBwAACA6RBwAACA6STHuoBYCAaDqqqqUkZGhiwWS6zLAQAAfWAYhpqamlRYWCirtfcxmiEZcKqqqlRUVBTrMgAAwAWorKzUqFGjej1mSAacjIwMSaET5HK5YlwNAADoC4/Ho6Kiosjf8d4MyYATnpZyuVwEHAAAEkxflpewyBgAAJgOAQcAAJgOAQcAAJgOAQcAAJgOAQcAAJgOAQcAAJgOAQcAAJgOAQcAAJgOAQcAAJgOAQcAAJgOAQcAAJgOAQcAAJjOkLzYJgAAA+nIyRa9VFalnLQUTSlwaUpBhjIdKbEua0gh4AAAECWt/nb9fMsh/e7/jsjfEezy3ojMVE0pyNCUggxN7Qw9E3LTZUtmMmUgEHAAALhIhmHoT7ur9W8v71O12ytJKhk/TA5bksprmnSisU3Vbq+q3V69Xl4f+Zwtyap75k/U8msnymKxxKp8UyLgAABwEcprmvTgS3v01kcNkqSiHIce+Pwl+rtL8iOhxeMN6EBNk/bVNKm8xqPymibtr2lSk7ddP/rfA6p2e/WvCy9TkpWQEy0EHAAALoC7LaCfvXpAf9hxTB1BQ/Zkq+6eO1HfvGa8UlOSuhzrSk3RrLE5mjU2J/KaYRja/NYxrX1pr57YWaGTzT498pUZ53wWF4aAAwBAPwSDhv7r3eP6/l/362SzX5J0w6UF+v8+P1VFOc4+f4/FYtHXSsZqeLpd336qTP+zt1a3PL5Lv711FguSo4CAAwBAH7nbAlr5dJle218nSZqQm6bvfvFSXT0p94K/87OXj1B2mk13bHpHu4426MuP7dCm265SQWZqtMoekli6DQBAH3xY5dEXf/43vba/TrZkq9Z8tlh/+fZnLirchH1y/DA9c2eJ8jLsKq9t0o2/fFOH6pqiUPXQRcABAOA8nn/3uG781Zs6dqpVo7Idev6uOfrmNROiusV76giXnr97jsbnpqnK7dU/PrZDpcdOR+37hxoCDgAAPfC3B/XAf+/RymfelzcQ1DWTc/XHFZ/WZSMzB+Tnjcp26rk75+iKoiw1tga05Hdv6bV9tQPys8yOgAMAQDeq3W266Tc79B9vHZMkfXv+JP370iuVnWYb0J+bk2bTk3fM1rVTcuUNBLXsP0r1wnvHB/RnmhEBBwCAj9l++KT+fsPf9F5Fo1ypyfr3pbN0399NHrQ+NU5bsn5zyyz948xR6ggaWvXM+3r5g+pB+dlmwS4qAAA6GYahX7/xkX7w1/0KGtIlI1x67KszNXpY37d/R0tKklU/WDRNSRaLnn6nUvf853tKTbFqXnH+oNeSiBjBAQBA0rFTLfrq4zv18F9C4WbRJ0bp+bvnxCTchFmtFv3bjZfri9ML1R40dOfmd7X90MmY1ZNICDgAgCGtvSOox7Yd1nU/fUNvHjql1BSr1n3pMv1o8bS46CqcZLXox1+err+7JF/+9qC+8Yd3VHqsIdZlxT0CDgBgyPrguFtf/Pmbevgv++VrD+rTE4frf+79jJbMHhNXF79MSbLq5/80Q1dPGq5Wf4eW/vvb2nPCHeuy4hoBBwAw5LT5O/RvL+/Twl/8TR9We5TlTNGPFk/Xf9x+lcYMS4t1ed2yJyfpN1+bpavG5qjJ166vPb5T5TU0A+wJAQcAMKT838F6XfezbfrNGx8paEhfnF6oV1deo3+cOSquRm2647Al6fGlszS9KEunWwP66uM7deRkS6zLiksEHADAkFDr8WrlM2X62uO7VNnQppFZDv1+6ZXacPMMDU+3x7q8PstITdGmr1+p4oIM1Tf5tOS3b+n46dZYlxV3LIZhGLEuYrB5PB5lZmbK7XbL5XLFuhwAwAA6crJFv3njsP6r9IT8HUFZLNLSOWP1/66bojR74nZLOdns05d/vUMf1bdozDCnnvlmifJd5r5AZ3/+fhNwCDgAYEp7Trj1q9cP6+U91Qr/pbtybLa+87mpmjE6O7bFRUmN26vFv96uyoY2TcnP0DPfLFGmMyXWZQ0YAs55EHAAwJwMw9COj07pV68f1v8dPNMvZn5xnu6aO0GzxubEsLqBUdnQqkW/2q66Jp9mjsnW5ttny2GL/fb2gUDAOQ8CDgCYS0fQ0Kv7avWr1w+rrLJRUqh/zBenF+qb14xXcYG5/19fXtOkxY9tl8fbrrlTcvXbW2YpJcl8y2wJOOdBwAGAxGcYht4/7tZLZVX64+4q1Tf5JEn2ZKtuurJId1w9XkU5setCPNhKjzVoye92yhsIauEVhfrpl6+QdZCunTVY+vP3O3FXVwEAhqRDdc16qeyEXny/SsdOndk9lOVM0ZLZo/X1T41LqF1R0TJzTI5+tWSm7vjDO3qxrErZTpse/MIlcb/1faAQcAAAca/a3aY/vl+lF8uqtLfKE3ndkZKk6y7N18IrCvXpibmyJZtvWqY/ri3O048WT9e9T5dp4/ajGpZm07fmT4p1WTFBwAEAxJ1AR1DvHjutNw7W640DJ/XBWZclSLZa9JnJuVp4RaEWTM1P6K3eA+EfZozU6Va/Hvrjh/rxKweUnWbTVz85JtZlDTr+VQAA4kLFqVZtO1ivNw7Ua8fhU2r2tXd5/6qxOfriFYX63OUjlJNmi1GVieHrnxqnhha/Ht1ySA+8uEdZzhT9/bTCWJc1qAg4AICYqPN49c6x03rro1N640C9jp7q2o03J82mqycN12cm5erqycOVl2HuJnbRtvLvJutUi19P7qzQfU+XKdORoqsn5ca6rEFDwAEADLhg0NDBuma9c6xBpUdP6+1jDapsaOtyTLLVok+MztZnJg/XNZPzdGmhy3S7gAaTxWLR9xZepsZWv17+oEbf/I9SPbXsk5o2KivWpQ0KAg4AIOrcbQHtOeFWWWWj3jnaoNJjp+Xxdp1yslik4gKXZo3J1qcnDdecCcOUkWreLryxkGS16Kc3XSFP2zv626GT+s4LH+iPKz49JHZWEXAAABel2deuvSfc+uCEW7uPu7X7eOM5001SaMfTjNFZmjUmWzPH5mjG6Cy5CDQDzp6cpA03z9Cnv79Fe0549Hp5va4tzot1WQOOgAMA6DN3a0AfVnv0YbVHe6tCgeZwfbO6axk7Osepy0dlaubobM0am62pI1ym7K6bCHI6d1L95o2PtGHLQc2dkmv6URwCDgDgHIZhqLKhTR9Wu/VhdZM+rPJoX7VHJxrbuj2+MDNVl4/K1LRRWbp8ZKYuH5mpbHY6xZVvXD1Om7Yf1XsVjXrz0Cl9etLwWJc0oAg4ADDEudsCOlDbpP3VHu2vaVJ5563pY9u0w4pyHLpkhEtTR7g0bVSmLhuZyQ6nBJCXkaqbrxqtjduPasOWgwQcAIA5+No79FF9SyjM1IQCTXlNk6rc3m6PtyVZNbkgPRJmLhnhUvEIlzIdrJtJVN+8Zrye3FmhXUcatPOjU5o9flisSxowBBwAMJn2jqCONbTqQE2TymubdKC2SQdqm3XkZIs6gt1fX7kwM1VTCjJUPMKl4oIMTSnI0ITcdNbMmMyITIf+cdYoPbmzQj/feoiAAwCIP8GgocrTrTpQ29wZYkJB5nBds/wdwW4/k5GarCn5oQBT3BloJudnMCozhNx1zQQ983al/u/gSb1XcVozRmfHuqQBMWABZ926dfrzn/+ssrIy2Ww2NTY2nnPMPffcozfffFN79uzR1KlTVVZWdt7v9Xq9WrVqlZ566in5fD5df/31+uUvf6n8/Pzo/xIAEAeCQUMnGtt0sK4pEmYO1jbrYF2TvIHug4wjJUmT89M1OT8jdCvI0JT8DOW77KbfPYPeFeU49aUZI/Vs6XE9uuWQ/n3plbEuaUAMWMDx+/1avHixSkpK9Pjjj/d43G233aadO3dq9+7dffre++67T3/+85/17LPPKjMzUytWrNCNN96oN998M1qlA0BMGIahGo83FGJqOkdk6pp1qLZJLf6Obj9jS7ZqQm66puSna1JnmJmSn6FR2Q66AKNHd187Uf/17nFt2V+nPSfcumxkZqxLiroBCzgPPfSQJGnjxo09HrNhwwZJUn19fZ8Cjtvt1uOPP64nn3xS8+bNkyT9/ve/19SpU/XWW2/pk5/85MUXDgCD4GSzr8samfKaJh2sa1aTt/udSylJFo0fnq5JZ4/K5KdrdI5TyayTQT+NG56mL0wv1ItlVXp0y0H9+muzYl1S1CXUGpzS0lIFAgEtWLAg8lpxcbFGjx6tHTt29BhwfD6ffD5f5LnH4xnwWgFACm3BPljbGWQigaZZDS3+bo9Pslo0bniaJuena1Je54hMQbrGDEtjwS+iasW1E/XS+1X6n7212l/jUXGBK9YlRVVCBZyamhrZbDZlZWV1eT0/P181NTU9fm79+vWRESUAGAjeQIcO1TWrvHNqaX/nfXUPW7AtFmlMjlOTOqeUJuWna0pBhsYNT5M9OWmQq8dQNCk/Q5+9rEAvf1CjX2w9rEdvnhHrkqKqXwFn9erV+v73v9/rMfv27VNxcfFFFRVta9as0cqVKyPPPR6PioqKYlgRgEQV3oJdflZDvAO1TTp6qkU97MDWiMzUzpGYM2tkJualy2EjyCC2Vlw7SS9/UKM/7a7SvQsmaUJueqxLipp+BZxVq1Zp6dKlvR4zfvz4i6mnVwUFBfL7/WpsbOwyilNbW6uCgoIeP2e322W32wesLgDmE17wu7+mc2qpc3rpYF2z/O3d71zKcqZEtmBPKQiPzLAFG/HrkkKXFkzN16v7avWLrYf0ky9fEeuSoqZfASc3N1e5ubkDVct5zZw5UykpKXrttde0aNEiSVJ5ebkqKipUUlISs7oAJC7DMFTf7NPBj/WSOVDb1OOC37O3YJ8dZnIz2IKNxPOteRP16r5avVhWpXvnT9boYc5YlxQVA7YGp6KiQg0NDaqoqFBHR0ekx83EiROVnh4aAjt06JCam5tVU1Ojtra2yDGXXHKJbDabTpw4ofnz5+sPf/iDrrrqKmVmZur222/XypUrlZOTI5fLpW9961sqKSlhBxWAXoVHZA7XtehwfXNovUxtkw7WNul0a6DbzyRZLRo/PE2TCzJU3NlLprggQ0XZTrZgwzSmF2XpM5Nz9caBev3y9UN6eNG0WJcUFQMWcNauXatNmzZFns+YEVq8tHXrVs2dO1eS9I1vfEPbtm0755gjR45o7NixCgQCKi8vV2tra+SYn/70p7JarVq0aFGXRn8AIIUW+x471arD9aGOvofrm3W4vkUf1Tf32EuGBb8Y6u6ZN1FvHKjXf717XN+aP0kjsxyxLumiWQzD6GFZnHl5PB5lZmbK7XbL5TLXtjjA7ILB0JRSRUOrKk61qvJ0qyoaWlXZELqv9fh6/GyS1aIxOU6Nz03XhLw0TensJzMhlwW/wM2/eUs7Pjql2z89Tg/8/SWxLqdb/fn7nVDbxAGYmzfQoVqPV7UeX+e9t8vzuiafqhrb5OthkW9Yhj1ZE/LSNaEzyIwfnq6JeWkanZMmWzK9ZIDu3HRlkXZ8dEofHHfHupSoIOAAiCrDMORrD8rjDajJ2955C8jdFtDp1oBOt/h1utXfeR/Q6Va/Glr8amwNqNnX/aLej7NapMIsh0bnODU6x6mizvvwLcuZwmJfoJ/GdC4uPn669TxHJgYCDgBJoamf1kCHmr3tavYF1OwLP+68eQNq8XeoyduuFt/Zr4fum84KND1dybov7MlWFWSmKj8jVXkuuwpcqcp3nXlckJmqwiwHXX2BKBuVHQo41R6v/O3BhB/tJOAAJhDoCKrJ2y5PW0Aeb0CetnZ5vAE1e9sjIynhEBK6b5fH2xlafB1q9rWrxd+uaK7Is1ikdHuyXKkpykgN3WenpSjbaVN2mk3Zzs7HZz0flmaXy5HM6AsQA8PTbUpNscobCKra3aYxw9JiXdJFIeAAccLX3iF3a0CNbaHpHHdr531b6DVP25nnHw8yrT3sDroQSVaL0u3JkVuaPUnpqSnKCD+2pyjdnqT01GSlnXWcyxEKMhmdgSbdlsxWaiCBWCwWjcp26lBdsyobCDgAPiYYNOTxhtabhNaW+LusPWnsDC+nW0PrTho7X4tGSEmzJcnlSImMmmSkJofCSefjDPtZAcSerPTUZGXYU5SeeiaopKZYGUEBhqiibIcO1TWbYh0OAQfohWEYavV3qKEltBC2odWvhuZQUDnVElooe/Z74cDS0zWJzsdqkVyOFGU5UpTpSJGr8z7TkaIs55nHrtQz77tSU+RyhMJJMutSAFyE8DqcSgIOkHha/e061ezXyWafTjX7darFp1MtoeDS0BIKLqdafGpoDj0+35bknqTbk5XlPHfNSZYzFGCy02ydwSX0XpbDpoxUpnUAxE5RTqjB3/HTbTGu5OIRcJDwgkFDjW0BnWz26WSTT/XNPp1s9uvUWQHmZPi+ya+2QP+nguzJVg1LCwWVnM5bttPW5bUsZ0roPadNmc4UOuACSDiREZwGRnCAAXF2aKlv8kXu6yPP/ZHXG1r86ujnnJAt2arcdLuGpYfCybC0M49z0mwanm5TTppdw9JsGpZuk9PGfyoAzG9UNiM4wAVp83eovsmnuiZvl8BS5znzOBxc2vsZWrKcKRqebtfwdJuGpdtDASbNpuEZ4aBy5r00WxILaQHgY4o6R3DqmnzyBjqUmpK4I9EEHFw0wzDk8barvinUUr+uyRsKLE0+1XWGmbomn+o9PjX1sVNtWJYzRbnpdg1Ptys34+z7UHAJv5eTZkv4plQAEGtZzhSl2ZLU4u/QicY2TchNj3VJF4yAgx4ZhqFmX3sotHi8qm3qek2gus77Wo9X3kDfF+KmpliVl5Gq3M6AkpthV15G6D70OFXDM0LTRoQWABg8FotFRTlO7a9pUmVDKwEHiccb6FCdx6faJq9q3N5IaDn7ca3H26/eLK7UZOW7UiOBJc+VGgkueZ1t93Mz7Mqw06kWAOLVqGyH9tc0Jfw6HAKOyRiGocbWgGo8oeBS4/Gq2u1Vnccbea3W49Xp1kCfvzOjM7gUdAaWPFeq8l2h0BK+z3PZE3quFgAQEt5JRcDBoOkIGjrZ7FO126sad1voPhxkznrc174ttmRr54UM7ZEAc/ZFDcOP2UEEAENHeCdVojf74y9XnGjvCKquKRReqt1tqnF7O4PMmee1Tb4+b4fOSbNFrrwcDi8FmaHRl4LOW5YzhakiAEAXRTmM4KCPgkFD9c0+VTWGRl3C9zVur6rC4cXj7VN7/ySrRXkZdhVkhkLKiEyHCjLtKsh0RIIL00UAgAsV6YWT4M3+CDhRVNnQqi3761TlblN1Y2jkpaoxFF760tMl2WpRvitVhVmpKsh0qDAzNAIzIjP0fERmqoan25VEK38AwAAJr8E51eJXq789YZcpJGbVcepAbZMefGlvt+9ZLVK+KxRWRmSFwsuITEeXMDOM8AIAiLHQBX2T5fG26/jpNk3Oz4h1SReEgBNF43PTdf2l+ZHgcvZ9XoadKz0DABJCUY5Te6s8On66lYADadzwNP36a7NiXQYAABdlVLZDe6s8qmxI3IXGDCkAAIAuzvTCSdyFxgQcAADQRVG4Fw4jOAAAwCwiIziNjOAAAACTCDf7YwQHAACYRrjZn7stII+379cujCcEHAAA0EWaPVk5aTZJ0vEEHcUh4AAAgHNELtmQoDupCDgAAOAcRZ0LjSsT9KKbBBwAAHAORnAAAIDpjErwnVQEHAAAcA5GcAAAgOmE1+CcON0mwzBiXE3/EXAAAMA5wiM4Tb52udsSrxcOAQcAAJwjNSVJuRl2SdLxBNxJRcABAADdGhW56GbircMh4AAAgG5FLrrJCA4AADCLovAITgLupCLgAACAbjGCAwAATKcohzU4AADAZM4ewUm0XjgEHAAA0K3CrFRZLFJboEOnWvyxLqdfCDgAAKBb9uQk5WekSkq8dTgEHAAA0KNEXYdDwAEAAD1K1J1UBBwAANCjRO2FQ8ABAAA9YgQHAACYzqjONTjHWYMDAADMoig8gtPYpmAwcXrhEHAAAECPRmSmKslqkb89qPpmX6zL6TMCDgAA6FFyklUFrnAvnMSZpiLgAACAXo3q3EmVSAuNCTgAAKBXRTmhdTiJ1OyPgAMAAHrFCA4AADCd8E6qRGr2R8ABAAC9YgTnLOvWrdOcOXPkdDqVlZXV7TH33HOPZs6cKbvdriuuuKJP3zt37lxZLJYutzvvvDN6hQMAgC7Ca3CqGtvUkSC9cAYs4Pj9fi1evFh33XVXr8fddtttuummm/r13XfccYeqq6sjtx/84AcXUyoAAOhFvitVKUkWBToM1Xq8sS6nT5IH6osfeughSdLGjRt7PGbDhg2SpPr6eu3evbvP3+10OlVQUHBR9QEAgL5JslpUmOXQsVOtqmxoVWGWI9YlnVdCrsF54oknNHz4cF122WVas2aNWlt7X/Tk8/nk8Xi63AAAQN8l2jqcARvBGSj/9E//pDFjxqiwsFC7d+/Wv/zLv6i8vFzPP/98j59Zv359ZEQJAAD0X2gn1amE2UnVrxGc1atXn7PA9+O3/fv3D1StkqRly5bp+uuv1+WXX64lS5boD3/4g1544QUdPny4x8+sWbNGbrc7cqusrBzQGgEAMBtTj+CsWrVKS5cu7fWY8ePHX0w9/TZ79mxJ0qFDhzRhwoRuj7Hb7bLb7YNZFgAAppJo3Yz7FXByc3OVm5s7ULVckLKyMknSiBEjYlsIAAAmlmgjOAO2yLiiokJlZWWqqKhQR0eHysrKVFZWpubm5sgxhw4dUllZmWpqatTW1hY5xu/3S5JOnDih4uJi7dq1S5J0+PBhfe9731NpaamOHj2ql156Sbfccos+85nPaNq0aQP1qwAAMOSFuxlXu9sU6AjGuJrzG7BFxmvXrtWmTZsiz2fMmCFJ2rp1q+bOnStJ+sY3vqFt27adc8yRI0c0duxYBQIBlZeXR3ZJ2Ww2vfrqq/rZz36mlpYWFRUVadGiRbr//vsH6tcAAACShqfbZUu2yt8eVI3bG5myilcWwzASoyVhFHk8HmVmZsrtdsvlcsW6HAAAEsK8H72uj0626MlvzNacicMH/ef35+93QvbBAQAAg29U56hNIqzDIeAAAIA+CS80ToReOAQcAADQJ+GFxomwVZyAAwAA+iQ3I9RTrqE1EONKzo+AAwAA+sRpS5IktfnbY1zJ+RFwAABAnzg6A06rvyPGlZwfAQcAAPSJM6VzBCdAwAEAACbhiExREXAAAIBJRNbgMIIDAADMwmELXeGJNTgAAMA0HJ1rcPztQXUE4/tKTwQcAADQJ+EpKin+p6kIOAAAoE/syVZZLKHHrXHeC4eAAwAA+sRisUSmqbz+YIyr6R0BBwAA9Fl4mqo1wAgOAAAwidSUxOhmTMABAAB9Fh7B8RJwAACAWTgYwQEAAGYTueAm28QBAIBZODu7GTNFBQAATOPMFBW7qAAAgElErigeoA8OAAAwicgVxRnBAQAAZsEuKgAAYDpnpqgIOAAAwCTCIzhtjOAAAACziFyLioADAADMwtHZB4cpKgAAYBpMUQEAANNxssgYAACYTSqdjAEAgNmcafTHCA4AADAJpqgAAIDppNLJGAAAmE14BMfXHlQwaMS4mp4RcAAAQJ85O/vgSPE9TUXAAQAAfWZPPhMd4nmaioADAAD6zGq1RJr9eRnBAQAAZuFIgOtREXAAAEC/OBKg2R8BBwAA9Esi9MIh4AAAgH5xJEA3YwIOAADol8gVxRnBAQAAZuFkkTEAADAbpqgAAIDpOFJC3YyZogIAAKbhsIXiA1NUAADANMLXo6KTMQAAMA0a/QEAANPhUg0AAMB0wtvEmaICAACmkZrCCA4AADAZGv0BAADTYYoKAACYDlNUAADAdMJ9cLhUAwAAMI3wFBWXagAAAKYx5Bv9rVu3TnPmzJHT6VRWVtY577///vu6+eabVVRUJIfDoalTp+qRRx457/c2NDRoyZIlcrlcysrK0u23367m5uYB+A0AAMDHOSKLjIMKBo0YV9O95IH8cr/fr8WLF6ukpESPP/74Oe+XlpYqLy9PmzdvVlFRkbZv365ly5YpKSlJK1as6PF7lyxZourqar3yyisKBAL6+te/rmXLlunJJ58cyF8HAADozAiOJHnbOyJrcuKJxTCMAY9eGzdu1L333qvGxsbzHrt8+XLt27dPW7Zs6fb9ffv26ZJLLtHbb7+tWbNmSZL++te/6nOf+5yOHz+uwsLC8/4Mj8ejzMxMud1uuVyufv0uAAAMdcGgofHfeVmS9M79CzQ83T4oP7c/f7/jbg2O2+1WTk5Oj+/v2LFDWVlZkXAjSQsWLJDVatXOnTu7/YzP55PH4+lyAwAAF8ZqtSg1JRQh4nUnVVwFnO3bt+vpp5/WsmXLejympqZGeXl5XV5LTk5WTk6Oampquv3M+vXrlZmZGbkVFRVFtW4AAIaa8DRVvO6k6nfAWb16tSwWS6+3/fv397uQPXv2aOHChXrwwQd13XXX9fvzvVmzZo3cbnfkVllZGdXvBwBgqIn3Xjj9XhW0atUqLV26tNdjxo8f36/v/PDDDzV//nwtW7ZM999/f6/HFhQUqK6urstr7e3tamhoUEFBQbefsdvtstsHZ34QAIChIDxFFa/djPsdcHJzc5Wbmxu1Avbu3at58+bp1ltv1bp16857fElJiRobG1VaWqqZM2dKkrZs2aJgMKjZs2dHrS4AANCzyAhOID574QzoGpyKigqVlZWpoqJCHR0dKisrU1lZWaRnzZ49e3Tttdfquuuu08qVK1VTU6OamhrV19dHvmPXrl0qLi7WiRMnJElTp07VDTfcoDvuuEO7du3Sm2++qRUrVugrX/lKn3ZQAQCAixfuhdPmD8a4ku4N6Mb1tWvXatOmTZHnM2bMkCRt3bpVc+fO1XPPPaf6+npt3rxZmzdvjhw3ZswYHT16VJLU2tqq8vJyBQKByPtPPPGEVqxYofnz58tqtWrRokXasGHDQP4qAADgLPHezXhQ+uDEG/rgAABwce7aXKq/7KnR9xZeqq+VjB2Un5nQfXAAAED8C09RxesiYwIOAADotzNTVAQcAABgEs7IBTcJOAAAwCQYwQEAAKbj6OyDQ8ABAACmwRQVAAAwnXjvg0PAAQAA/RbpZMwIDgAAMAtn5FINBBwAAGAS7KICAACmwxQVAAAwHQdTVAAAwGycKfTBAQAAJnP2FJVhGDGu5lwEHAAA0G/hgCNJ3kAwhpV0j4ADAAD6LbyLSorPhcYEHAAA0G9JVotsyaEYEY/djAk4AADggsRzsz8CDgAAuCDOlPjthUPAAQAAFyTVFr/djAk4AADggjjjuJsxAQcAAFyQcLM/1uAAAADTYIoKAACYDouMAQCA6Zy54CZ9cAAAgEk4mKICAABmwxQVAAAwHQedjAEAgNkQcAAAgOmEp6hamaICAABmwQgOAAAwHYeNTsYAAMBkHExRAQAAswlfbNPLCA4AADCLSKO/AJ2MAQCASYSnqFiDAwAATMPJLioAAGA2Zy8yNgwjxtV0RcABAAAXJLwGxzAkX3swxtV0RcABAAAXJDyCI8XfNBUBBwAAXJDkJKtsSaEoEW+9cAg4AADggsXr5RoIOAAA4ILF604qAg4AALhgkZ1U/vhq9kfAAQAAFywyRcUaHAAAYBbx2s2YgAMAAC5Y5HpUBBwAAGAWTqaoAACA2TBFBQAATMdhS5bECA4AADCRM9vECTgAAMAkzjT6ow8OAAAwCfrgAAAA02GKCgAAmE54isrLCA4AADALGv0BAADTGZJTVOvWrdOcOXPkdDqVlZV1zvvvv/++br75ZhUVFcnhcGjq1Kl65JFHzvu9Y8eOlcVi6XJ7+OGHB+A3AAAAvXF29sGJtymq5IH8cr/fr8WLF6ukpESPP/74Oe+XlpYqLy9PmzdvVlFRkbZv365ly5YpKSlJK1as6PW7//Vf/1V33HFH5HlGRkbU6wcAAL1z2EJjJfE2gjOgAeehhx6SJG3cuLHb92+77bYuz8ePH68dO3bo+eefP2/AycjIUEFBQVTqBAAAF8aREooS8RZw4m4NjtvtVk5OznmPe/jhhzVs2DDNmDFDP/zhD9Xe3nODIZ/PJ4/H0+UGAAAuXrzuohrQEZz+2r59u55++mn9+c9/7vW4e+65R5/4xCeUk5Oj7du3a82aNaqurtZPfvKTbo9fv359ZDQJAABEz5ldVO0yDEMWiyXGFYX0ewRn9erV5yzw/fht//79/S5kz549WrhwoR588EFdd911vR67cuVKzZ07V9OmTdOdd96pH//4x3r00Ufl8/m6PX7NmjVyu92RW2VlZb/rAwAA5woHnKAh+TuCMa7mjH6P4KxatUpLly7t9Zjx48f36zs//PBDzZ8/X8uWLdP999/f35I0e/Zstbe36+jRo5oyZco579vtdtnt9n5/LwAA6F14m7gktfk7ZE9O6uXowdPvgJObm6vc3NyoFbB3717NmzdPt956q9atW3dB31FWViar1aq8vLyo1QUAAM4vJcmqlCSLAh2GWv0dynLGuqKQAV2DU1FRoYaGBlVUVKijo0NlZWWSpIkTJyo9PV179uzRvHnzdP3112vlypWqqamRJCUlJUVC1K5du3TLLbfotdde08iRI7Vjxw7t3LlT1157rTIyMrRjxw7dd999+upXv6rs7OyB/HUAAEA3HClJCnS0x9UFNwc04Kxdu1abNm2KPJ8xY4YkaevWrZo7d66ee+451dfXa/Pmzdq8eXPkuDFjxujo0aOSpNbWVpWXlysQCEgKTTc99dRT+u53vyufz6dx48bpvvvu08qVKwfyVwEAAD1w2JLk8barLY62ilsMwzBiXcRg83g8yszMlNvtlsvlinU5AAAktGt/9LqOnGzRM98s0VXjzt/q5UL15+933PXBAQAAiSW80DiepqgIOAAA4KKEt4q3+XtuujvYCDgAAOCihLsZM4IDAABMIzUl3M2YgAMAAEwiMoJDwAEAAGZBwAEAAKYTmaJiDQ4AADALRnAAAIDpOG2hCyMQcAAAgGkwRQUAAEyHKSoAAGA6Zy7VQCdjAABgEuFLNdDoDwAAmAZTVAAAwHS4mjgAADAdByM4AADAbOiDAwAATMdxVh8cwzBiXE0IAQcAAFyU8BRVR9BQoIOAAwAATCA8giPFzzQVAQcAAFwUW7JVyVaLpPjZSUXAAQAAF+1Ms7/46GZMwAEAABctstCYKSoAAGAW4W7GXqaoAACAWaQyggMAAMzGGWcX3CTgAACAixbuZswUFQAAMA2mqAAAgOmEp6jogwMAAEwjEnDogwMAAMyCKSoAAGA6TFEBAADTCXcy5mKbAADANBz0wQEAAGYT7oPDFBUAADANhy0UKZiiAgAApuFIYQQHAACYDGtwAACA6dDoDwAAmE5kmzhTVAAAwCyYogIAAKYTnqLyMoIDAADMwtm5iyrQYSjQEYxxNQQcAAAQBam2M5EiHqapCDgAAOCi2ZKsSrJaJMXHNBUBBwAAXDSLxRLZScUIDgAAMI0zO6li3wuHgAMAAKIinnZSEXAAAEBUMEUFAABMxxG5XAMBBwAAmETkelRMUQEAALNgigoAAJiOwxbqZswUFQAAMA1HSihWMEUFAABMw9k5gkMfHAAAYBpndlFxsU0AAGAS4UXGbQFGcAAAgEk4h0ofnHXr1mnOnDlyOp3Kyso65/1Tp07phhtuUGFhoex2u4qKirRixQp5PJ5ev7ehoUFLliyRy+VSVlaWbr/9djU3Nw/QbwEAAPoidahsE/f7/Vq8eLHuuuuu7n+41aqFCxfqpZde0oEDB7Rx40a9+uqruvPOO3v93iVLlmjv3r165ZVX9Kc//UlvvPGGli1bNhC/AgAA6KN4avSXPJBf/tBDD0mSNm7c2O372dnZXcLPmDFjdPfdd+uHP/xhj9+5b98+/fWvf9Xbb7+tWbNmSZIeffRRfe5zn9OPfvQjFRYWRu8XAAAAfTZkpqj6q6qqSs8//7yuueaaHo/ZsWOHsrKyIuFGkhYsWCCr1aqdO3d2+xmfzyePx9PlBgAAomvITFH11c033yyn06mRI0fK5XLpd7/7XY/H1tTUKC8vr8trycnJysnJUU1NTbefWb9+vTIzMyO3oqKiqNYPAADO9MHxxsEUVb8DzurVq2WxWHq97d+/v1/f+dOf/lTvvvuuXnzxRR0+fFgrV67sb1m9WrNmjdxud+RWWVkZ1e8HAABnpqjiYQSn32twVq1apaVLl/Z6zPjx4/v1nQUFBSooKFBxcbFycnJ09dVX64EHHtCIESO6Pbaurq7La+3t7WpoaFBBQUG332+322W32/tVEwAA6J8zU1Sx74PT74CTm5ur3NzcgahFkhQMhrof+ny+bt8vKSlRY2OjSktLNXPmTEnSli1bFAwGNXv27AGrCwAA9C48guMNxL6T8YDuoqqoqFBDQ4MqKirU0dGhsrIySdLEiROVnp6ul19+WbW1tbryyiuVnp6uvXv36p//+Z/1qU99SmPHjpUk7dq1S7fccotee+01jRw5UlOnTtUNN9ygO+64Q4899pgCgYBWrFihr3zlK+ygAgAghsKdjP0dQbV3BJWcFLulvgMacNauXatNmzZFns+YMUOStHXrVs2dO1cOh0O//e1vdd9998nn86moqEg33nijVq9eHflMa2urysvLFQgEIq898cQTWrFihebPny+r1apFixZpw4YNA/mrAACA8whfi0qSWgMdcsUw4FgMwzBi9tNjxOPxKDMzU263Wy6XK9blAABgCoZhaMJ3XlbQkHZ9Z77yXKlR/f7+/P2Oi23iAAAg8Vkslsg0Vax3UhFwAABA1Dg6e+HE+nINBBwAABA1DlsoWjCCAwAATMOZ0jmCQ8ABAABm4YiTK4oTcAAAQNQ44qSbMQEHAABEzZluxozgAAAAk3DEyQU3CTgAACBq6IMDAABMhykqAABgOqlMUQEAALMJ98Eh4AAAANNgigoAAJjOmSkq+uAAAACTcKaEOxkHY1pHckx/OgAAMJXiERm6a+4ETchNj2kdBBwAABA1lxZm6tLCzFiXwRQVAAAwHwIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwHQIOAAAwnSF5NXHDMCRJHo8nxpUAAIC+Cv/dDv8d782QDDhNTU2SpKKiohhXAgAA+qupqUmZmZm9HmMx+hKDTCYYDKqqqkoZGRmyWCxR/W6Px6OioiJVVlbK5XJF9btxBud5cHCeBwfneXBwngfPQJ1rwzDU1NSkwsJCWa29r7IZkiM4VqtVo0aNGtCf4XK5+A9oEHCeBwfneXBwngcH53nwDMS5Pt/ITRiLjAEAgOkQcAAAgOkQcKLMbrfrwQcflN1uj3UppsZ5Hhyc58HBeR4cnOfBEw/nekguMgYAAObGCA4AADAdAg4AADAdAg4AADAdAg4AADAdAk4U/eIXv9DYsWOVmpqq2bNna9euXbEuKeG98cYb+sIXvqDCwkJZLBb993//d5f3DcPQ2rVrNWLECDkcDi1YsEAHDx6MTbEJav369bryyiuVkZGhvLw8/cM//IPKy8u7HOP1erV8+XINGzZM6enpWrRokWpra2NUceL61a9+pWnTpkWan5WUlOgvf/lL5H3Oc/Q9/PDDslgsuvfeeyOvcZ6j47vf/a4sFkuXW3FxceT9WJ9nAk6UPP3001q5cqUefPBBvfvuu5o+fbquv/561dXVxbq0hNbS0qLp06frF7/4Rbfv/+AHP9CGDRv02GOPaefOnUpLS9P1118vr9c7yJUmrm3btmn58uV666239MorrygQCOi6665TS0tL5Jj77rtPf/zjH/Xss89q27Ztqqqq0o033hjDqhPTqFGj9PDDD6u0tFTvvPOO5s2bp4ULF2rv3r2SOM/R9vbbb+vXv/61pk2b1uV1znP0XHrppaquro7c/va3v0Xei/l5NhAVV111lbF8+fLI846ODqOwsNBYv359DKsyF0nGCy+8EHkeDAaNgoIC44c//GHktcbGRsNutxv/+Z//GYMKzaGurs6QZGzbts0wjNA5TUlJMZ599tnIMfv27TMkGTt27IhVmaaRnZ1t/O53v+M8R1lTU5MxadIk45VXXjGuueYa49vf/rZhGPx7jqYHH3zQmD59erfvxcN5ZgQnCvx+v0pLS7VgwYLIa1arVQsWLNCOHTtiWJm5HTlyRDU1NV3Oe2ZmpmbPns15vwhut1uSlJOTI0kqLS1VIBDocp6Li4s1evRozvNF6Ojo0FNPPaWWlhaVlJRwnqNs+fLl+vznP9/lfEr8e462gwcPqrCwUOPHj9eSJUtUUVEhKT7O85C82Ga0nTx5Uh0dHcrPz+/yen5+vvbv3x+jqsyvpqZGkro97+H30D/BYFD33nuvPvWpT+myyy6TFDrPNptNWVlZXY7lPF+YDz74QCUlJfJ6vUpPT9cLL7ygSy65RGVlZZznKHnqqaf07rvv6u233z7nPf49R8/s2bO1ceNGTZkyRdXV1XrooYd09dVXa8+ePXFxngk4ACKWL1+uPXv2dJlHR3RNmTJFZWVlcrvdeu6553Trrbdq27ZtsS7LNCorK/Xtb39br7zyilJTU2Ndjql99rOfjTyeNm2aZs+erTFjxuiZZ56Rw+GIYWUhTFFFwfDhw5WUlHTO6vDa2loVFBTEqCrzC59bznt0rFixQn/605+0detWjRo1KvJ6QUGB/H6/GhsbuxzPeb4wNptNEydO1MyZM7V+/XpNnz5djzzyCOc5SkpLS1VXV6dPfOITSk5OVnJysrZt26YNGzYoOTlZ+fn5nOcBkpWVpcmTJ+vQoUNx8e+ZgBMFNptNM2fO1GuvvRZ5LRgM6rXXXlNJSUkMKzO3cePGqaCgoMt593g82rlzJ+e9HwzD0IoVK/TCCy9oy5YtGjduXJf3Z86cqZSUlC7nuby8XBUVFZznKAgGg/L5fJznKJk/f74++OADlZWVRW6zZs3SkiVLIo85zwOjublZhw8f1ogRI+Lj3/OgLGUeAp566inDbrcbGzduND788ENj2bJlRlZWllFTUxPr0hJaU1OT8d577xnvvfeeIcn4yU9+Yrz33nvGsWPHDMMwjIcfftjIysoyXnzxRWP37t3GwoULjXHjxhltbW0xrjxx3HXXXUZmZqbx+uuvG9XV1ZFba2tr5Jg777zTGD16tLFlyxbjnXfeMUpKSoySkpIYVp2YVq9ebWzbts04cuSIsXv3bmP16tWGxWIx/vd//9cwDM7zQDl7F5VhcJ6jZdWqVcbrr79uHDlyxHjzzTeNBQsWGMOHDzfq6uoMw4j9eSbgRNGjjz5qjB492rDZbMZVV11lvPXWW7EuKeFt3brVkHTO7dZbbzUMI7RV/IEHHjDy8/MNu91uzJ8/3ygvL49t0Qmmu/Mryfj9738fOaatrc24++67jezsbMPpdBpf+tKXjOrq6tgVnaBuu+02Y8yYMYbNZjNyc3ON+fPnR8KNYXCeB8rHAw7nOTpuuukmY8SIEYbNZjNGjhxp3HTTTcahQ4ci78f6PFsMwzAGZ6wIAABgcLAGBwAAmA4BBwAAmA4BBwAAmA4BBwAAmA4BBwAAmA4BBwAAmA4BBwAAmA4BBwAAmA4BBwAAmA4BBwAAmA4BBwAAmA4BBwAAmM7/D1APYlMWTxvvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pos_n['bias'].squeeze()[0, -1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 51, 51])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_n['bias'].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 51, 51])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_og['bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]], device='cuda:0')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_og['bias'].squeeze() == pos_n['bias'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "posso = loadit('posoo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pos', 'positional_indices', 'positional_grid'])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 7, 6])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [-4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
       "        [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
       "        [-6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5],\n",
       "        [-7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3,  4],\n",
       "        [-8, -7, -6, -5, -4, -3, -2, -1,  0,  1,  2,  3]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss['pos'][poss['positional_indices']].squeeze()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 7, 6])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  2,  1,  0, -1, -2, -3, -4, -5, -6, -7, -8],\n",
       "        [ 4,  3,  2,  1,  0, -1, -2, -3, -4, -5, -6, -7],\n",
       "        [ 5,  4,  3,  2,  1,  0, -1, -2, -3, -4, -5, -6],\n",
       "        [ 6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4, -5],\n",
       "        [ 7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4],\n",
       "        [ 8,  7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posso['pos'][posso['positional_indices']].squeeze()[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0],\n",
       "        [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],\n",
       "        [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],\n",
       "        [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],\n",
       "        [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],\n",
       "        [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss['positional_indices'].squeeze()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0],\n",
       "        [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1],\n",
       "        [13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],\n",
       "        [14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3],\n",
       "        [15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4],\n",
       "        [16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posso['positional_indices'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11, -12],\n",
       "        [  0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10, -11],\n",
       "        [  1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9, -10],\n",
       "        [  2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8,  -9],\n",
       "        [  3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7,  -8],\n",
       "        [  4,   3,   2,   1,   0,  -1,  -2,  -3,  -4,  -5,  -6,  -7]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posso['pos'][posso['positional_indices']].squeeze()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_arange = torch.arange(10)\n",
    "context_arange = torch.arange(10)\n",
    "indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "indices += (10 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0],\n",
       "        [10,  9,  8,  7,  6,  5,  4,  3,  2,  1],\n",
       "        [11, 10,  9,  8,  7,  6,  5,  4,  3,  2],\n",
       "        [12, 11, 10,  9,  8,  7,  6,  5,  4,  3],\n",
       "        [13, 12, 11, 10,  9,  8,  7,  6,  5,  4],\n",
       "        [14, 13, 12, 11, 10,  9,  8,  7,  6,  5],\n",
       "        [15, 14, 13, 12, 11, 10,  9,  8,  7,  6],\n",
       "        [16, 15, 14, 13, 12, 11, 10,  9,  8,  7],\n",
       "        [17, 16, 15, 14, 13, 12, 11, 10,  9,  8],\n",
       "        [18, 17, 16, 15, 14, 13, 12, 11, 10,  9]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_len = b1_lengths\n",
    "x_len = b2_lengths\n",
    "total_len = x_len + cache_len\n",
    "cache= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = repeat(torch.arange(total_len.max()), 'i -> b r i', b=len(total_len), r=x_len.max())\n",
    "cache_offset = cache_len[:,None,None] if exists(cache) else cache_len\n",
    "diagonal_offset = torch.arange(x_len.max())[None,:,None]\n",
    "##\n",
    "## positional stuff ##\n",
    "positional_grid = (causal_mask - cache_offset - diagonal_offset)*-1\n",
    "pos = torch.arange(positional_grid.min(), positional_grid.max()+1)[:,None]\n",
    "min_cache_len = 0 if cache_len.__class__ == int else cache_len.min()\n",
    "positional_indices = ((positional_grid) + (total_len.max() - min_cache_len - 1)) # shift so zero is the smallest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3,  2,  1,  0, -1, -2, -3, -4, -5, -6, -7, -8],\n",
       "         [ 4,  3,  2,  1,  0, -1, -2, -3, -4, -5, -6, -7],\n",
       "         [ 5,  4,  3,  2,  1,  0, -1, -2, -3, -4, -5, -6],\n",
       "         [ 6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4, -5],\n",
       "         [ 7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4],\n",
       "         [ 8,  7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3]],\n",
       "\n",
       "        [[ 7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4],\n",
       "         [ 8,  7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3],\n",
       "         [ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0, -1, -2],\n",
       "         [10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0, -1],\n",
       "         [11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0],\n",
       "         [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1]],\n",
       "\n",
       "        [[ 6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4, -5],\n",
       "         [ 7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4],\n",
       "         [ 8,  7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3],\n",
       "         [ 9,  8,  7,  6,  5,  4,  3,  2,  1,  0, -1, -2],\n",
       "         [10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0, -1],\n",
       "         [11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0]]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  2,  1,  0, -1, -2, -3, -4, -5, -6, -7, -8],\n",
       "        [ 4,  3,  2,  1,  0, -1, -2, -3, -4, -5, -6, -7],\n",
       "        [ 5,  4,  3,  2,  1,  0, -1, -2, -3, -4, -5, -6],\n",
       "        [ 6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4, -5],\n",
       "        [ 7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3, -4],\n",
       "        [ 8,  7,  6,  5,  4,  3,  2,  1,  0, -1, -2, -3]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[positional_indices].squeeze()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
