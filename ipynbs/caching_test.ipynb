{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  7,  7, 15])\n"
     ]
    }
   ],
   "source": [
    "cache_len = torch.tensor([2,5,2,8])\n",
    "cur_lens = torch.tensor([5,2,5,7])\n",
    "total_len = cache_lens + cur_lens\n",
    "print(total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(causal_mask.shape[0])[None].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cache_lens[:,None,None] + torch.arange(cur_lens.max())[None,:,None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 15])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(total_len.max())[None,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  7,  7, 15]) tensor([5, 2, 5, 7]) tensor([2, 5, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "print(total_len, cur_lens, cache_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 7, 15]) torch.Size([4, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = repeat(torch.arange(total_len.max()), 'i -> b r i', b=len(total_len), r=cur_lens.max())\n",
    "causal_mask = causal_mask >= (cache_lens[:,None,None] + torch.arange(cur_lens.max())[None,:,None] + 1)\n",
    "causal_mask[-3,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.arange(total_len.max()).expand(len(total_len), -1)\n",
    "causal_mask = causal_mask > cache_len[:,None]\n",
    "causal_mask = causal_mask < torch.arange(causal_mask.shape[0])[None].T + cache_len[:,None] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2],\n",
       "        [ 6],\n",
       "        [ 4],\n",
       "        [11]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(causal_mask.shape[0])[None].T + cache_len[:,None] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "from torch.utils.checkpoint import checkpoint # # gradient/activation checkpointing\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# token shifting\n",
    "# lucidrains implementation: https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py\n",
    "# BlinkDL idea from RWKV-LM https://github.com/BlinkDL/RWKV-LM\n",
    "def shift(t, amount, mask = None):\n",
    "    if amount == 0:\n",
    "        return t\n",
    "    else:\n",
    "        amount = min(amount, t.shape[1])\n",
    "\n",
    "    if exists(mask):\n",
    "        t = t.masked_fill(~mask[..., None], 0.)\n",
    "\n",
    "    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n",
    "\n",
    "class ShiftTokens(nn.Module):\n",
    "    def __init__(self, shifts, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.shifts = tuple(shifts)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        mask = kwargs.get('mask', None)\n",
    "        shifts = self.shifts\n",
    "        segments = len(shifts)\n",
    "        feats_per_shift = x.shape[-1] // segments\n",
    "        splitted = x.split(feats_per_shift, dim = -1)\n",
    "        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n",
    "        segments_to_shift = list(map(lambda args: shift(*args, mask = mask), zip(segments_to_shift, shifts)))\n",
    "        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    '''Adapted From Phil Wang's x-transformers library to handle non-square matrices'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            activation()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                activation()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, i, j, device, dtype):\n",
    "        # get the (i x j) matrix of distances\n",
    "        assert i >= 1 and j >= 1 and i <= j, 'I should be in the range [1, j] and j >= 1'\n",
    "        seq_arange = torch.arange(i, device = device)\n",
    "        context_arange = torch.arange(j, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (j-1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-i + 1, (j+i), device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "     \n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos) \n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1,))\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device = device).type_as(self.inv_freq)\n",
    "        sinu = einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n",
    "        return emb * self.scale\n",
    "\n",
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(t, groups = 1, dim = -1):\n",
    "    if groups == 1:\n",
    "        return F.normalize(t, p = 2, dim = dim)\n",
    "    t = rearrange(t, '... (g d) -> ... g d', g = groups)\n",
    "    t = F.normalize(t, p = 2, dim = dim)\n",
    "    return rearrange(t, '... g d -> ... (g d)')\n",
    "\n",
    "class CosineAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.shared_kv = kwargs.get('shared_kv', False)\n",
    "        self.talking_heads = kwargs.get('talking_heads', False)\n",
    "\n",
    "        self.n_feats, self.head_dim, self.n_heads = n_feats, head_dim, n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        self.causal = causal\n",
    "\n",
    "        if self.talking_heads:\n",
    "            self._head_proj = nn.Conv2d(n_heads, n_heads, (1, 1))\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        if not self.shared_kv:\n",
    "            self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "            self.qkv = lambda x: rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=n_heads, d=head_dim)\n",
    "        else:\n",
    "            self.q_proj, self.kv_proj = [nn.Linear(n_feats, el, bias=bias) for el in [n_heads * head_dim, 2 * head_dim]]\n",
    "            map_q, map_kv = lambda q: rearrange(q, 'b n (h d) -> b h n d', h=n_heads), lambda kv: rearrange(kv, 'b n (kv d) -> kv b () n d', kv=2, d=head_dim)\n",
    "            self.qkv = lambda x: (map_q(self.q_proj(x)), *map_kv(self.kv_proj(x)))\n",
    "\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "    \n",
    "    def head_proj(self, dots):\n",
    "        if not self.talking_heads:\n",
    "            return dots\n",
    "        dots = self._head_proj(dots)\n",
    "        return dots      \n",
    "\n",
    "    def attend(self, query, key, value, attn_mask, pos_bias):\n",
    "        query, key = map(l2norm, (query, key))\n",
    "        \n",
    "        dots = einsum('bhid,bhjd->bhij', query, key) * self.temperature\n",
    "        dots = self.head_proj(dots)\n",
    "\n",
    "        #dots += pos_bias\n",
    "     \n",
    "\n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "\n",
    "        attn = self.activation(dots)\n",
    "     \n",
    "        attn = self.dropout(attn)\n",
    "        return einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "\n",
    "\n",
    "    def attach_cache(self, kv, cache, cache_indices):\n",
    "        kv = torch.stack(kv, dim=0)\n",
    "        if cache is None:\n",
    "            return kv\n",
    "        zero_vector = torch.zeros_like(kv[:, :, :, :1, :])\n",
    "        kv_w_cache = torch.cat([cache, kv, zero_vector], dim=-2)\n",
    "        print(kv_w_cache.shape, 'pre_gather')\n",
    "        kv_w_cache = torch.gather(kv_w_cache, dim=-2, index=cache_indices) # we do this to remove unnecessary padding\n",
    "        return kv_w_cache\n",
    "\n",
    "    def forward(self, x, pos_bias, mask, cache=None, cache_indices=None):\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "    \n",
    "        q, k, v  = self.qkv(x)\n",
    "        kv = self.attach_cache([k, v], cache, cache_indices)\n",
    "        k, v = kv\n",
    "\n",
    "        out = self.attend(q, k, v, mask, pos_bias)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out, kv\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(self.norm(x), *args, **kwargs)\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim = -1)\n",
    "        return x * self.act(gate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim, \n",
    "            depth, \n",
    "            heads, \n",
    "            dim_head, \n",
    "            causal=True,\n",
    "            temperature=15.5,\n",
    "            shared_temperture=False,\n",
    "            intermediate_loss=True,\n",
    "            dropout = 0.1,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            intermediate_loss = False\n",
    "\n",
    "        ff_mult = kwargs.get('ff_mult', 4)\n",
    "        self.checkpoint_every_n = kwargs.get('checkpoint_every_n', 0)\n",
    "        self.token_shift = kwargs.get('token_shift', False)\n",
    "        self.causal = causal\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature), requires_grad=True) if shared_temperture else temperature\n",
    "    \n",
    "\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.depth = depth\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = dim // 4,\n",
    "            heads = heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.token_shifter = lambda x: x\n",
    "        if self.token_shift:\n",
    "            self.token_shifter = ShiftTokens(range(0, 2), nn.Identity())\n",
    "        self.token_shift = lambda x: self.token_shifter(x)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, CosineAttention(\n",
    "                    dim, \n",
    "                    n_heads=heads, \n",
    "                    head_dim=dim_head, \n",
    "                    causal=causal,\n",
    "                    temperature=self.temperature,\n",
    "                    dropout=dropout,\n",
    "                    **kwargs\n",
    "                )),\n",
    "                PreNorm(dim, self.ff(dim, mult=ff_mult))\n",
    "            ]))\n",
    "\n",
    "    @staticmethod\n",
    "    def ff(dim, mult=4, dropout=0.1):\n",
    "        return nn.Sequential(\n",
    "            GLU(dim, dim * mult, nn.SiLU()),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_custom_forward(module):\n",
    "        def custom_forward(*args, **kwargs):\n",
    "            return module(*args, **kwargs)\n",
    "        return custom_forward\n",
    "\n",
    "    def checkpoint(self, layer, module, *args, **kwargs):\n",
    "        condition = self.training and self.checkpoint_every_n != 0 and layer < self.depth - 1 and layer % self.checkpoint_every_n == 0\n",
    "        return checkpoint(self.create_custom_forward(module), *args, **kwargs) if condition else module(*args, **kwargs)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cache(cache, layer):\n",
    "        if cache is None:\n",
    "            return None\n",
    "        return cache['cache'][layer]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cache_indices(x_lens, cache_lens, cache_kv, x):  \n",
    "        # used later w/ gather to remove padding when cache is concatenated with current input to remove padding\n",
    "        max_new_len = (x_lens + cache_lens).max()\n",
    "\n",
    "        B, H, N, D = x.shape[0], 1, (x.shape[1] + cache_kv.shape[-2]), cache_kv.shape[-1]\n",
    "        indices = []\n",
    "        for i in range(B): # stinky for loop to sort out indices for gather \n",
    "            cache_indices = torch.arange(cache_lens[i], device='cpu')\n",
    "            total_length = cache_lens[i] + x_lens[i]\n",
    "            diff_from_max_len = max_new_len - total_length\n",
    "            x_indices = torch.arange(x_lens[i]+diff_from_max_len, device='cpu') + cache_kv.shape[-2]\n",
    "            if diff_from_max_len > 0:\n",
    "                x_indices[-diff_from_max_len:] = N # last index will be used for padding\n",
    "            new_indices = torch.cat([cache_indices, x_indices])\n",
    "            indices.append(new_indices)\n",
    "\n",
    "        indices = torch.stack(indices, dim=0)\n",
    "        \n",
    "        indices = rearrange(indices, 'b n -> () b () n ()').expand(2, B, H,-1, D) # 2 for key and value\n",
    "        return indices.to(x.device)\n",
    "\n",
    "\n",
    "    def create_masks(self, x, length, cache): # could clean this up ):\n",
    "        x_len = length if length is not None else torch.tensor(x.shape[-2]).expand(x.shape[0])\n",
    "        cache_len = cache['cache_lengths'] if exists(cache) else 0\n",
    "        total_len = x_len + cache_len\n",
    "        kv_mask = torch.arange(total_len.max(), device=x.device).expand(len(total_len), -1) >= total_len.unsqueeze(-1)\n",
    "        q_mask = torch.arange(x_len.max(), device=x.device).expand(len(x_len), -1) >= x_len.unsqueeze(-1)\n",
    "        attn_mask = ~(rearrange(~q_mask, \"b n -> b () n ()\") * rearrange(~kv_mask, \"b n -> b () () n\"))\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = repeat(torch.arange(total_len.max()), 'i -> b r i', b=len(total_len), r=x_len.max())\n",
    "            causal_mask = causal_mask >= ((cache_len[:,None,None] if exists(cache) else cache_len) + torch.arange(x_len.max())[None,:,None] + 1)\n",
    "            print(attn_mask.shape, causal_mask[:,None].shape)\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask[:,None])\n",
    "            \n",
    "        return q_mask, attn_mask, total_len, x_len, cache_len\n",
    "\n",
    "    def forward(self, x, length=None, self_condtioning=None, cache=None):\n",
    "        intermediate_logits = []\n",
    "        cached_kvs = []\n",
    "    \n",
    "        mask, attn_mask, total_lens, x_len, cache_len = self.create_masks(x, length, cache)\n",
    "\n",
    "        cache_indices = self.get_cache_indices(x_len, cache_len, cache['cache'], x) if exists(cache) else None\n",
    "      \n",
    "        pos_bias = self.positional_bias(i = attn_mask.shape[-2], j = attn_mask.shape[-1], device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for i, (attn, ff) in enumerate(self.layers):\n",
    "\n",
    "            x = self.token_shift(x)\n",
    "            a_out, kv = self.checkpoint(i, attn, x, pos_bias, attn_mask, self.get_cache(cache, layer=i), cache_indices)\n",
    "            x = a_out + x\n",
    "            cached_kvs.append(kv)\n",
    "            x = self.checkpoint(i, ff, x) + x   \n",
    "\n",
    "            if i < self.depth - 1 and self_condtioning is not None:\n",
    "                x, logits = self_condtioning(x)\n",
    "                intermediate_logits.append(logits)\n",
    "\n",
    "        if len(intermediate_logits) > 0: # stack intermediate logits\n",
    "            intermediate_logits = torch.stack(intermediate_logits, dim=0) # D x B x N x L\n",
    "\n",
    "        cached_kvs = torch.stack(cached_kvs, dim=0) if len(cached_kvs) > 0 else None\n",
    "        cached_kvs = {'cache_lengths': total_lens, 'cache': cached_kvs} if exists(cached_kvs) else None\n",
    "\n",
    "\n",
    "        return x, intermediate_logits, cached_kvs\n",
    "\n",
    "class shared_embedding_output_layer(nn.Module):\n",
    "    '''Pass a embedding layer and then use this module as the output layer'''\n",
    "    def __init__(self, embedding_layer, bias=False):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.use_bias = bias\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(embedding_layer.weight.shape[0]))#\n",
    "            nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, weight=self.embedding_layer.weight, bias=self.bias if self.use_bias else None)\n",
    "\n",
    "\n",
    "class transformer_lm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        causal=True,\n",
    "        temperature=15.5,\n",
    "        dropout=0.,\n",
    "        shared_temperture=True,\n",
    "        self_conditioning=False,\n",
    "        intermediate_loss=True,\n",
    "        use_abs_pos=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self_conditioning == False\n",
    "\n",
    "        self.self_conditioning = True if self_conditioning else None\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.use_abs_pos = use_abs_pos\n",
    "        if self.use_abs_pos:\n",
    "            self.abs_pos_fn = ScaledSinuEmbedding(dim=dim)\n",
    "        self.abs_pos = lambda x: x + self.abs_pos_fn(x) if self.use_abs_pos else x\n",
    "\n",
    "        if self_conditioning:\n",
    "            self.reprojection_layer = nn.Linear(vocab_size, dim)\n",
    "\n",
    "\n",
    "        self.layers = transformer(\n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = heads, \n",
    "            dim_head = dim_head, \n",
    "            causal = causal, \n",
    "            dropout = dropout,\n",
    "            temperature = temperature,\n",
    "            shared_temperture = shared_temperture,\n",
    "            intermediate_loss = intermediate_loss,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.tie_embedding = kwargs.get('tie_embedding', False)\n",
    "        print('Tie embedding:', self.tie_embedding) if self.tie_embedding else None\n",
    " \n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "\n",
    "        self.to_logits = shared_embedding_output_layer(self.embedding) if self.tie_embedding else nn.Linear(dim, vocab_size)\n",
    "        \n",
    "\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "\n",
    "    def self_condition_fn(self):\n",
    "        def self_condition(x):\n",
    "            logits = self.to_logits(self.post_norm(x))\n",
    "            if self.self_conditioning: # not effective for LMs (intermediate loss is tho)\n",
    "                z = F.softmax(logits, dim=-1)\n",
    "                z = self.reprojection_layer(z)\n",
    "                x = z + x\n",
    "            return x, logits\n",
    "        return self_condition if (self.self_conditioning or self.intermediate_loss) and self.training else None\n",
    "\n",
    "\n",
    "    def forward(self, x, length=None, cache:Dict=None):\n",
    "        '''\n",
    "        x: [B, N] (embedding indices)\n",
    "        length: [B] (length of each sequence)\n",
    "        cache: {cache_lengths: [B, N], cache: [L, KV, B, H, N, D]} KV: key and value (2)\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        x = self.abs_pos(x) \n",
    "  \n",
    "        x, interim_logits, cached_kvs = self.layers(x, length, self_condtioning=self.self_condition_fn(), cache=cache)\n",
    "        x = self.post_norm(x)\n",
    "        x = self.to_logits(x)\n",
    "\n",
    "        return  x, interim_logits, cached_kvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "class CharacterTokenizer(): # only for testing\n",
    "    def __init__(self):\n",
    "        self.vocab = ['#', '/'] + list(string.ascii_lowercase) + [' '] # bos/eos -> /, pad -> #\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.tokenize(text)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.token_to_id[token] for token in text]\n",
    "\n",
    "tokenizer = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = transformer_lm(\n",
    "    dim = 256,\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    depth = 10,\n",
    "    heads = 1,\n",
    "    dim_head = 32,\n",
    "    dropout=0.0,\n",
    "    causal = True,\n",
    "    shared_kv = True,\n",
    ")\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(tensors:List[torch.Tensor], pad_token:int):\n",
    "    max_len = max([t.shape[0] for t in tensors])\n",
    "    lengths = torch.tensor([t.shape[0] for t in tensors])\n",
    "    padded_tensors = [torch.cat([t, torch.full((max_len - t.shape[0],), pad_token, dtype=t.dtype)], dim=0) for t in tensors]\n",
    "    return torch.stack(padded_tensors, dim=0), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 13, 10]) tensor([6, 7, 6]) tensor([7, 6, 4])\n",
      "tensor([13, 13, 10])\n"
     ]
    }
   ],
   "source": [
    "print(b1_lengths + b2_lengths, b1_lengths, b2_lengths)\n",
    "print(fb_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THE LENGTHS TO MAKE THE CAUSAL MASK RATHER THAN THE OTHER THINGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 7, 7]) torch.Size([3, 1, 7, 7])\n",
      "second\n",
      "torch.Size([3, 1, 7, 13]) torch.Size([3, 1, 7, 13])\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "third\n",
      "torch.Size([3, 1, 13, 13]) torch.Size([3, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "s1_b1, s2_b1, s3_b1 = torch.tensor(tokenizer('/hello')), torch.tensor(tokenizer('/buenos')), torch.tensor(tokenizer('/whats'))\n",
    "s1_b2, s2_b2, s3_b2 = torch.tensor(tokenizer(' world/')), torch.tensor(tokenizer(' dias/')), torch.tensor(tokenizer(' up/'))\n",
    "b1, b1_lengths = collate_fn([s1_b1, s2_b1, s3_b1], pad_token=tokenizer.token_to_id['#'])\n",
    "b2, b2_lengths = collate_fn([s1_b2, s2_b2, s3_b2], pad_token=tokenizer.token_to_id['#'])\n",
    "# comparsion set\n",
    "f_1, f_2, f_3 = torch.tensor(tokenizer('/hello world/')), torch.tensor(tokenizer('/buenos dias/')), torch.tensor(tokenizer('/whats up/'))\n",
    "fb, fb_lengths = collate_fn([f_1, f_2, f_3], pad_token=tokenizer.token_to_id['#'])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_s1, interim_logits, cached_kvs = model(b1, length=b1_lengths)\n",
    "    #print(cached_kvs['cache'].shape, 'cache', b2.shape)\n",
    "    print('second')\n",
    "    logits_s2, interim_logits, cached_kvs_s2 = model(b2, length=b2_lengths, cache=cached_kvs)\n",
    "    print('third')\n",
    "    logits_fs, interim_logits, cached_kvs_fs = model(fb, length=fb_lengths)\n",
    "\n",
    "#logits_s1.masked_fill_(b1_lengths_mask.unsqueeze(-1), 0)\n",
    "\n",
    "#print('logits_s2:', logits_s2.shape, 'logits_fs:', logits_fs.shape)\n",
    "D, B = 1, 1\n",
    "#print(logits_s2[B, :, D], logits_fs[B, :, D])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6397,  0.7431, -0.8576, -0.5721,  0.2620, -0.6212, -0.3207, -0.0375,\n",
      "        -0.7380, -0.3104,  0.1822, -0.6504, -0.6416,  0.2827,  0.6039,  0.6327,\n",
      "         0.2097, -0.0468, -0.2050, -0.2130, -0.9321,  0.1500,  0.7238, -0.2692,\n",
      "        -0.0446,  0.3955,  0.0053,  0.3986, -1.2466,  0.5881,  1.2487, -0.2498])\n"
     ]
    }
   ],
   "source": [
    "print(cached_kvs['cache'][L,0,1,0,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 13, 10])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_kvs_fs['cache_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:  torch.Size([10, 2, 3, 1, 13, 32]) torch.Size([10, 2, 3, 1, 13, 32])\n",
      "tensor([-0.1548,  1.1113, -0.1597, -0.4324,  0.2550, -0.1549, -0.1684,  0.3414,\n",
      "         0.1699, -0.1366,  0.1756, -0.9724, -0.9767,  0.4447, -1.7030,  1.8319,\n",
      "         0.0057, -0.2066,  0.2175, -0.2070, -0.8515,  0.0823,  0.7015, -0.0918,\n",
      "        -0.1286,  0.0922,  0.9617,  0.2686, -0.9577,  0.5732,  1.2578,  0.1263])\n",
      "\n",
      "tensor([-0.1548,  1.1113, -0.1597, -0.4324,  0.2550, -0.1549, -0.1684,  0.3414,\n",
      "         0.1699, -0.1366,  0.1756, -0.9724, -0.9767,  0.4447, -1.7030,  1.8319,\n",
      "         0.0057, -0.2066,  0.2175, -0.2070, -0.8515,  0.0823,  0.7015, -0.0918,\n",
      "        -0.1286,  0.0922,  0.9617,  0.2686, -0.9577,  0.5732,  1.2578,  0.1263])\n"
     ]
    }
   ],
   "source": [
    "print('shapes: ', cached_kvs_fs['cache'].shape, cached_kvs_s2['cache'].shape)\n",
    "N = 8\n",
    "L = -1\n",
    "I = 0\n",
    "print(cached_kvs_fs['cache'][L,0,I,0,N])\n",
    "print()\n",
    "print(cached_kvs_s2['cache'][L,0,I,0,N])\n",
    "assert torch.allclose(cached_kvs_fs['cache'][L,:,I,:,N], cached_kvs_s2['cache'][L,:,I,:,N], rtol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 3, 1, 13, 32])\n",
      "torch.Size([10, 2, 3, 1, 13, 32])\n"
     ]
    }
   ],
   "source": [
    "print(cached_kvs_fs['cache'].shape)\n",
    "print(cached_kvs_s2['cache'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9226,  0.3684,  0.1545,  0.1768, -0.6461,  0.1462,  0.3829]) tensor([ 0.9226,  0.3684,  0.1545,  0.1768, -0.6460,  0.1462,  0.3829])\n"
     ]
    }
   ],
   "source": [
    "print(logits_s1[B, :, D], logits_fs[B, :logits_s1.shape[1], D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11, 32])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "tensor([ 0.9778,  0.3346,  0.4991, -0.7083, -0.3909]) tensor([ 0.9778,  0.3346,  0.4991, -0.7083, -0.3909])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x_og, i_logits, cache_kv = model(torch.tensor(tokenizer('/hello bro/')).unsqueeze(0))\n",
    "\n",
    "    x, i_logits, cache_kv = model(torch.tensor(tokenizer('/hello')).unsqueeze(0))\n",
    "    x_c, i_logits, cache_kv = model(torch.tensor(tokenizer(' bro/')).unsqueeze(0), cache=cache_kv)\n",
    "        #print(cache_kv.shape)\n",
    "\n",
    "    #print(x_og.shape, x_c.shape)\n",
    "print(x_og[0,6:, 0], x_c[0,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  9,  6, 13, 13, 16, 28,  3, 19, 16,  1])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor(tokenizer('/hello')), torch.tensor(tokenizer(' bro/'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 29]) torch.Size([1, 5, 29])\n",
      "tensor([ 0.4475, -0.1402, -0.5746,  0.9537, -1.0230]) tensor([ 0.3507, -0.1993, -0.5755,  0.7726, -1.1193])\n"
     ]
    }
   ],
   "source": [
    "print(x_og.shape, x_c.shape)\n",
    "print(x_og[0,6:, 0], x_c[0,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(x, length, cache):\n",
    "    x_len = length if length is not None else torch.tensor(x.shape[-2]).expand(x.shape[0])\n",
    "    cache_len = 0 if cache is None else cache['length']\n",
    "    total_len = x_len + cache_len\n",
    "    kv_mask = torch.arange(total_len.max(), device=x.device).expand(len(total_len), -1) >= total_len.unsqueeze(-1)\n",
    "    q_mask = torch.arange(x_len.max(), device=x.device).expand(len(x_len), -1) >= x_len.unsqueeze(-1)\n",
    "    attn_mask = ~(rearrange(~q_mask, \"b n -> b () n ()\") * rearrange(~kv_mask, \"b n -> b () () n\"))\n",
    "\n",
    "    if 1==1: #causal\n",
    "        causal_mask = torch.ones(attn_mask.shape[-2], attn_mask.shape[-1], device=x.device).triu(1 + attn_mask.shape[-2] - attn_mask.shape[-1]).bool()\n",
    "        attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "    return q_mask, attn_mask, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False, False, False, False, False, False, False, False, False, False]]),\n",
       " tensor([[[[False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False]]]]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masks(x=torch.rand(1, 10,3), length=None, cache={'length': torch.tensor([5])})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
