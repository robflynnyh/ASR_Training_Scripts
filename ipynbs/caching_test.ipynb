{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "from torch.utils.checkpoint import checkpoint # # gradient/activation checkpointing\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# token shifting\n",
    "# lucidrains implementation: https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py\n",
    "# BlinkDL idea from RWKV-LM https://github.com/BlinkDL/RWKV-LM\n",
    "def shift(t, amount, mask = None):\n",
    "    if amount == 0:\n",
    "        return t\n",
    "    else:\n",
    "        amount = min(amount, t.shape[1])\n",
    "\n",
    "    if exists(mask):\n",
    "        t = t.masked_fill(~mask[..., None], 0.)\n",
    "\n",
    "    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n",
    "\n",
    "class ShiftTokens(nn.Module):\n",
    "    def __init__(self, shifts, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.shifts = tuple(shifts)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        mask = kwargs.get('mask', None)\n",
    "        shifts = self.shifts\n",
    "        segments = len(shifts)\n",
    "        feats_per_shift = x.shape[-1] // segments\n",
    "        splitted = x.split(feats_per_shift, dim = -1)\n",
    "        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n",
    "        segments_to_shift = list(map(lambda args: shift(*args, mask = mask), zip(segments_to_shift, shifts)))\n",
    "        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    '''Adapted From Phil Wang's x-transformers library to handle non-square matrices'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            activation()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                activation()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, i, j, device, dtype):\n",
    "        # get the (i x j) matrix of distances\n",
    "        assert i >= 1 and j >= 1 and i <= j, 'I should be in the range [1, j] and j >= 1'\n",
    "        seq_arange = torch.arange(i, device = device)\n",
    "        context_arange = torch.arange(j, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (j-1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-i + 1, (j+i), device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "     \n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos) \n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1,))\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device = device).type_as(self.inv_freq)\n",
    "        sinu = einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n",
    "        return emb * self.scale\n",
    "\n",
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(t, groups = 1, dim = -1):\n",
    "    if groups == 1:\n",
    "        return F.normalize(t, p = 2, dim = dim)\n",
    "    t = rearrange(t, '... (g d) -> ... g d', g = groups)\n",
    "    t = F.normalize(t, p = 2, dim = dim)\n",
    "    return rearrange(t, '... g d -> ... (g d)')\n",
    "\n",
    "class CosineAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.shared_kv = kwargs.get('shared_kv', False)\n",
    "        self.talking_heads = kwargs.get('talking_heads', False)\n",
    "\n",
    "        self.n_feats, self.head_dim, self.n_heads = n_feats, head_dim, n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        self.causal = causal\n",
    "\n",
    "        if self.talking_heads:\n",
    "            self._head_proj = nn.Conv2d(n_heads, n_heads, (1, 1))\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        if not self.shared_kv:\n",
    "            self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "            self.qkv = lambda x: rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=n_heads, d=head_dim)\n",
    "        else:\n",
    "            self.q_proj, self.kv_proj = [nn.Linear(n_feats, el, bias=bias) for el in [n_heads * head_dim, 2 * head_dim]]\n",
    "            map_q, map_kv = lambda q: rearrange(q, 'b n (h d) -> b h n d', h=n_heads), lambda kv: rearrange(kv, 'b n (kv d) -> kv b () n d', kv=2, d=head_dim)\n",
    "            self.qkv = lambda x: (map_q(self.q_proj(x)), *map_kv(self.kv_proj(x)))\n",
    "\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "    \n",
    "    def head_proj(self, dots):\n",
    "        if not self.talking_heads:\n",
    "            return dots\n",
    "        dots = self._head_proj(dots)\n",
    "        return dots      \n",
    "\n",
    "    def attend(self, query, key, value, attn_mask, pos_bias):\n",
    "        query, key = map(l2norm, (query, key))\n",
    "        \n",
    "        dots = einsum('bhid,bhjd->bhij', query, key) * self.temperature\n",
    "        dots = self.head_proj(dots)\n",
    "\n",
    "        #dots += pos_bias\n",
    "        if attn_mask.shape[-1] != attn_mask.shape[-2]:\n",
    "            print(attn_mask.shape, 'mask') #[3, 1, 7, 13\n",
    "            print(attn_mask[0, 0, 0])\n",
    "            raise MemoryError('ohno')\n",
    "\n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "\n",
    "        attn = self.activation(dots)\n",
    "     \n",
    "        attn = self.dropout(attn)\n",
    "        return einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "\n",
    "\n",
    "    def attach_cache(self, kv, cache, cache_indices):\n",
    "        kv = torch.stack(kv, dim=0)\n",
    "        if cache is None:\n",
    "            return kv\n",
    "        zero_vector = torch.zeros_like(kv[:, :, :, :1, :])\n",
    "        kv_w_cache = torch.cat([cache, kv, zero_vector], dim=-2)\n",
    "        print(kv_w_cache.shape, 'pre_gather')\n",
    "        kv_w_cache = torch.gather(kv_w_cache, dim=-2, index=cache_indices) # we do this to remove unnecessary padding\n",
    "        return kv_w_cache\n",
    "\n",
    "    def forward(self, x, pos_bias, mask, cache=None, cache_indices=None):\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "    \n",
    "        q, k, v  = self.qkv(x)\n",
    "        kv = self.attach_cache([k, v], cache, cache_indices)\n",
    "        k, v = kv\n",
    "\n",
    "        out = self.attend(q, k, v, mask, pos_bias)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out, kv\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(self.norm(x), *args, **kwargs)\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim = -1)\n",
    "        return x * self.act(gate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim, \n",
    "            depth, \n",
    "            heads, \n",
    "            dim_head, \n",
    "            causal=True,\n",
    "            temperature=15.5,\n",
    "            shared_temperture=False,\n",
    "            intermediate_loss=True,\n",
    "            dropout = 0.1,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            intermediate_loss = False\n",
    "\n",
    "        ff_mult = kwargs.get('ff_mult', 4)\n",
    "        self.checkpoint_every_n = kwargs.get('checkpoint_every_n', 0)\n",
    "        self.token_shift = kwargs.get('token_shift', False)\n",
    "        self.causal = causal\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature), requires_grad=True) if shared_temperture else temperature\n",
    "    \n",
    "\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.depth = depth\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = dim // 4,\n",
    "            heads = heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.token_shifter = lambda x: x\n",
    "        if self.token_shift:\n",
    "            self.token_shifter = ShiftTokens(range(0, 2), nn.Identity())\n",
    "        self.token_shift = lambda x: self.token_shifter(x)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, CosineAttention(\n",
    "                    dim, \n",
    "                    n_heads=heads, \n",
    "                    head_dim=dim_head, \n",
    "                    causal=causal,\n",
    "                    temperature=self.temperature,\n",
    "                    dropout=dropout,\n",
    "                    **kwargs\n",
    "                )),\n",
    "                PreNorm(dim, self.ff(dim, mult=ff_mult))\n",
    "            ]))\n",
    "\n",
    "    @staticmethod\n",
    "    def ff(dim, mult=4, dropout=0.1):\n",
    "        return nn.Sequential(\n",
    "            GLU(dim, dim * mult, nn.SiLU()),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_custom_forward(module):\n",
    "        def custom_forward(*args, **kwargs):\n",
    "            return module(*args, **kwargs)\n",
    "        return custom_forward\n",
    "\n",
    "    def checkpoint(self, layer, module, *args, **kwargs):\n",
    "        condition = self.training and self.checkpoint_every_n != 0 and layer < self.depth - 1 and layer % self.checkpoint_every_n == 0\n",
    "        return checkpoint(self.create_custom_forward(module), *args, **kwargs) if condition else module(*args, **kwargs)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cache(cache, layer):\n",
    "        if cache is None:\n",
    "            return None\n",
    "        return cache['cache'][layer]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cache_indices(x_lens, cache_lens, cache_kv, x):  \n",
    "        # used later w/ gather to remove padding when cache is concatenated with current input to remove padding\n",
    "        max_new_len = (x_lens + cache_lens).max()\n",
    "\n",
    "        B, H, N, D = x.shape[0], 1, (x.shape[1] + cache_kv.shape[-2]), cache_kv.shape[-1]\n",
    "        indices = []\n",
    "        for i in range(B): # stinky for loop to sort out indices for gather \n",
    "            cache_indices = torch.arange(cache_lens[i], device='cpu')\n",
    "            total_length = cache_lens[i] + x_lens[i]\n",
    "            diff_from_max_len = max_new_len - total_length\n",
    "            x_indices = torch.arange(x_lens[i]+diff_from_max_len, device='cpu') + cache_kv.shape[-2]\n",
    "            if diff_from_max_len > 0:\n",
    "                x_indices[-diff_from_max_len:] = N # last index will be used for padding\n",
    "            new_indices = torch.cat([cache_indices, x_indices])\n",
    "            indices.append(new_indices)\n",
    "\n",
    "        indices = torch.stack(indices, dim=0)\n",
    "        \n",
    "        indices = rearrange(indices, 'b n -> () b () n ()').expand(2, B, H,-1, D) # 2 for key and value\n",
    "        return indices.to(x.device)\n",
    "\n",
    "\n",
    "    def create_masks(self, x, length, cache):\n",
    "        x_len = length if length is not None else torch.tensor(x.shape[-2]).expand(x.shape[0])\n",
    "        cache_len = 0 if cache is None else cache['cache_lengths']\n",
    "        total_len = x_len + cache_len\n",
    "        kv_mask = torch.arange(total_len.max(), device=x.device).expand(len(total_len), -1) >= total_len.unsqueeze(-1)\n",
    "        q_mask = torch.arange(x_len.max(), device=x.device).expand(len(x_len), -1) >= x_len.unsqueeze(-1)\n",
    "        attn_mask = ~(rearrange(~q_mask, \"b n -> b () n ()\") * rearrange(~kv_mask, \"b n -> b () () n\"))\n",
    "\n",
    "        if self.causal:\n",
    "            offset = attn_mask.shape[-1] - attn_mask.shape[-2] + 1 if attn_mask.shape[-1] != attn_mask.shape[-2] else 0\n",
    "            causal_mask = torch.ones(attn_mask.shape[-2], attn_mask.shape[-1], device=x.device).triu(1 + offset).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "            \n",
    "        return q_mask, attn_mask, total_len, x_len, cache_len\n",
    "\n",
    "    def forward(self, x, length=None, self_condtioning=None, cache=None):\n",
    "        intermediate_logits = []\n",
    "        cached_kvs = []\n",
    "    \n",
    "        mask, attn_mask, total_lens, x_len, cache_len = self.create_masks(x, length, cache)\n",
    "\n",
    "        cache_indices = self.get_cache_indices(x_len, cache_len, cache['cache'], x) if exists(cache) else None\n",
    "        if exists(cache): # torch.Size([2, 11, 32])\n",
    "            print(cache_indices.shape)\n",
    "            #torch.Size([2, 3, 1, 13, 32])\n",
    "            print(cache_indices[0, 1, 0, :, 0])\n",
    "        pos_bias = self.positional_bias(i = attn_mask.shape[-2], j = attn_mask.shape[-1], device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for i, (attn, ff) in enumerate(self.layers):\n",
    "\n",
    "            x = self.token_shift(x)\n",
    "            a_out, kv = self.checkpoint(i, attn, x, pos_bias, attn_mask, self.get_cache(cache, layer=i), cache_indices)\n",
    "            x = a_out + x\n",
    "            cached_kvs.append(kv)\n",
    "            x = self.checkpoint(i, ff, x) + x   \n",
    "\n",
    "            if i < self.depth - 1 and self_condtioning is not None:\n",
    "                x, logits = self_condtioning(x)\n",
    "                intermediate_logits.append(logits)\n",
    "\n",
    "        if len(intermediate_logits) > 0: # stack intermediate logits\n",
    "            intermediate_logits = torch.stack(intermediate_logits, dim=0) # D x B x N x L\n",
    "\n",
    "        cached_kvs = torch.stack(cached_kvs, dim=0) if len(cached_kvs) > 0 else None\n",
    "        cached_kvs = {'cache_lengths': total_lens, 'cache': cached_kvs} if exists(cached_kvs) else None\n",
    "\n",
    "\n",
    "        return x, intermediate_logits, cached_kvs\n",
    "\n",
    "class shared_embedding_output_layer(nn.Module):\n",
    "    '''Pass a embedding layer and then use this module as the output layer'''\n",
    "    def __init__(self, embedding_layer, bias=False):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.use_bias = bias\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(embedding_layer.weight.shape[0]))#\n",
    "            nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, weight=self.embedding_layer.weight, bias=self.bias if self.use_bias else None)\n",
    "\n",
    "\n",
    "class transformer_lm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        causal=True,\n",
    "        temperature=15.5,\n",
    "        dropout=0.,\n",
    "        shared_temperture=True,\n",
    "        self_conditioning=False,\n",
    "        intermediate_loss=True,\n",
    "        use_abs_pos=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self_conditioning == False\n",
    "\n",
    "        self.self_conditioning = True if self_conditioning else None\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.use_abs_pos = use_abs_pos\n",
    "        if self.use_abs_pos:\n",
    "            self.abs_pos_fn = ScaledSinuEmbedding(dim=dim)\n",
    "        self.abs_pos = lambda x: x + self.abs_pos_fn(x) if self.use_abs_pos else x\n",
    "\n",
    "        if self_conditioning:\n",
    "            self.reprojection_layer = nn.Linear(vocab_size, dim)\n",
    "\n",
    "\n",
    "        self.layers = transformer(\n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = heads, \n",
    "            dim_head = dim_head, \n",
    "            causal = causal, \n",
    "            dropout = dropout,\n",
    "            temperature = temperature,\n",
    "            shared_temperture = shared_temperture,\n",
    "            intermediate_loss = intermediate_loss,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.tie_embedding = kwargs.get('tie_embedding', False)\n",
    "        print('Tie embedding:', self.tie_embedding) if self.tie_embedding else None\n",
    " \n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "\n",
    "        self.to_logits = shared_embedding_output_layer(self.embedding) if self.tie_embedding else nn.Linear(dim, vocab_size)\n",
    "        \n",
    "\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "\n",
    "    def self_condition_fn(self):\n",
    "        def self_condition(x):\n",
    "            logits = self.to_logits(self.post_norm(x))\n",
    "            if self.self_conditioning: # not effective for LMs (intermediate loss is tho)\n",
    "                z = F.softmax(logits, dim=-1)\n",
    "                z = self.reprojection_layer(z)\n",
    "                x = z + x\n",
    "            return x, logits\n",
    "        return self_condition if (self.self_conditioning or self.intermediate_loss) and self.training else None\n",
    "\n",
    "\n",
    "    def forward(self, x, length=None, cache:Dict=None):\n",
    "        '''\n",
    "        x: [B, N] (embedding indices)\n",
    "        length: [B] (length of each sequence)\n",
    "        cache: {cache_lengths: [B, N], cache: [L, KV, B, H, N, D]} KV: key and value (2)\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        x = self.abs_pos(x) \n",
    "  \n",
    "        x, interim_logits, cached_kvs = self.layers(x, length, self_condtioning=self.self_condition_fn(), cache=cache)\n",
    "        x = self.post_norm(x)\n",
    "        x = self.to_logits(x)\n",
    "\n",
    "        return  x, interim_logits, cached_kvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "class CharacterTokenizer(): # only for testing\n",
    "    def __init__(self):\n",
    "        self.vocab = ['#', '/'] + list(string.ascii_lowercase) + [' '] # bos/eos -> /, pad -> #\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.tokenize(text)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.token_to_id[token] for token in text]\n",
    "\n",
    "tokenizer = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = transformer_lm(\n",
    "    dim = 256,\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    depth = 10,\n",
    "    heads = 1,\n",
    "    dim_head = 32,\n",
    "    dropout=0.0,\n",
    "    causal = True,\n",
    "    shared_kv = True,\n",
    ")\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(tensors:List[torch.Tensor], pad_token:int):\n",
    "    max_len = max([t.shape[0] for t in tensors])\n",
    "    lengths = torch.tensor([t.shape[0] for t in tensors])\n",
    "    padded_tensors = [torch.cat([t, torch.full((max_len - t.shape[0],), pad_token, dtype=t.dtype)], dim=0) for t in tensors]\n",
    "    return torch.stack(padded_tensors, dim=0), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 13, 10]) tensor([6, 7, 6]) tensor([7, 6, 4])\n",
      "tensor([13, 13, 10])\n"
     ]
    }
   ],
   "source": [
    "print(b1_lengths + b2_lengths, b1_lengths, b2_lengths)\n",
    "print(fb_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1, 13, 32])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "torch.Size([2, 3, 1, 15, 32]) pre_gather\n",
      "torch.Size([3, 1, 7, 13]) mask\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True])\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "ohno",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [126], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     logits_s1, interim_logits, cached_kvs \u001b[38;5;241m=\u001b[39m model(b1, length\u001b[38;5;241m=\u001b[39mb1_lengths)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#print(cached_kvs['cache'].shape, 'cache', b2.shape)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     logits_s2, interim_logits, cached_kvs_s2 \u001b[38;5;241m=\u001b[39m model(b2, length\u001b[38;5;241m=\u001b[39mb2_lengths, cache\u001b[38;5;241m=\u001b[39mcached_kvs)\n\u001b[1;32m     14\u001b[0m     logits_fs, interim_logits, cached_kvs_fs \u001b[38;5;241m=\u001b[39m model(fb, length\u001b[38;5;241m=\u001b[39mfb_lengths)\n\u001b[1;32m     15\u001b[0m fb_lengths_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(fb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mfb\u001b[38;5;241m.\u001b[39mdevice)[\u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m fb_lengths[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [121], line 480\u001b[0m, in \u001b[0;36mtransformer_lm.forward\u001b[0;34m(self, x, length, cache)\u001b[0m\n\u001b[1;32m    477\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m    478\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabs_pos(x) \n\u001b[0;32m--> 480\u001b[0m x, interim_logits, cached_kvs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_condtioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_condition_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_norm(x)\n\u001b[1;32m    482\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_logits(x)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [121], line 371\u001b[0m, in \u001b[0;36mtransformer.forward\u001b[0;34m(self, x, length, self_condtioning, cache)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (attn, ff) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    370\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_shift(x)\n\u001b[0;32m--> 371\u001b[0m     a_out, kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     x \u001b[38;5;241m=\u001b[39m a_out \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    373\u001b[0m     cached_kvs\u001b[38;5;241m.\u001b[39mappend(kv)\n",
      "Cell \u001b[0;32mIn [121], line 308\u001b[0m, in \u001b[0;36mtransformer.checkpoint\u001b[0;34m(self, layer, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer, module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    307\u001b[0m     condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_every_n \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m layer \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m layer \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_every_n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m checkpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_custom_forward(module), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [121], line 218\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [121], line 205\u001b[0m, in \u001b[0;36mCosineAttention.forward\u001b[0;34m(self, x, pos_bias, mask, cache, cache_indices)\u001b[0m\n\u001b[1;32m    202\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattach_cache([k, v], cache, cache_indices)\n\u001b[1;32m    203\u001b[0m k, v \u001b[38;5;241m=\u001b[39m kv\n\u001b[0;32m--> 205\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb h n d -> b n (h d)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(out)\n",
      "Cell \u001b[0;32mIn [121], line 178\u001b[0m, in \u001b[0;36mCosineAttention.attend\u001b[0;34m(self, query, key, value, attn_mask, pos_bias)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(attn_mask\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#[3, 1, 7, 13\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(attn_mask[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mMemoryError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mohno\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    180\u001b[0m dots\u001b[38;5;241m.\u001b[39mmasked_fill_(attn_mask, \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfinfo(dots\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m    182\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(dots)\n",
      "\u001b[0;31mMemoryError\u001b[0m: ohno"
     ]
    }
   ],
   "source": [
    "s1_b1, s2_b1, s3_b1 = torch.tensor(tokenizer('/hello')), torch.tensor(tokenizer('/buenos')), torch.tensor(tokenizer('/whats'))\n",
    "s1_b2, s2_b2, s3_b2 = torch.tensor(tokenizer(' world/')), torch.tensor(tokenizer(' dias/')), torch.tensor(tokenizer(' up/'))\n",
    "b1, b1_lengths = collate_fn([s1_b1, s2_b1, s3_b1], pad_token=tokenizer.token_to_id['#'])\n",
    "b2, b2_lengths = collate_fn([s1_b2, s2_b2, s3_b2], pad_token=tokenizer.token_to_id['#'])\n",
    "# comparsion set\n",
    "f_1, f_2, f_3 = torch.tensor(tokenizer('/hello world/')), torch.tensor(tokenizer('/buenos dias/')), torch.tensor(tokenizer('/whats up/'))\n",
    "fb, fb_lengths = collate_fn([f_1, f_2, f_3], pad_token=tokenizer.token_to_id['#'])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_s1, interim_logits, cached_kvs = model(b1, length=b1_lengths)\n",
    "    #print(cached_kvs['cache'].shape, 'cache', b2.shape)\n",
    "    logits_s2, interim_logits, cached_kvs_s2 = model(b2, length=b2_lengths, cache=cached_kvs)\n",
    "    logits_fs, interim_logits, cached_kvs_fs = model(fb, length=fb_lengths)\n",
    "fb_lengths_mask = torch.arange(fb.shape[1], device=fb.device)[None, :] >= fb_lengths[:, None]\n",
    "b2_lengths_mask = torch.arange(b2.shape[1], device=b2.device)[None, :] >= b2_lengths[:, None]\n",
    "\n",
    "#logits_s1.masked_fill_(b1_lengths_mask.unsqueeze(-1), 0)\n",
    "logits_s2.masked_fill_(b2_lengths_mask.unsqueeze(-1), 0)\n",
    "logits_fs.masked_fill_(fb_lengths_mask.unsqueeze(-1), 0)\n",
    "#print('logits_s2:', logits_s2.shape, 'logits_fs:', logits_fs.shape)\n",
    "D, B = 1, 1\n",
    "#print(logits_s2[B, :, D], logits_fs[B, :, D])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4633, -0.1938, -0.6470, -0.2119, -0.6190,  0.0410,  1.0812, -0.8186,\n",
      "        -0.2605,  0.5479,  0.9541, -0.5564, -0.5637,  1.2437,  0.0752,  0.3840,\n",
      "         0.9438, -0.8844,  0.5153, -0.0246, -0.2578, -0.3141,  0.2165, -0.2250,\n",
      "         1.3045,  0.3957, -0.4365,  0.9814,  1.0865,  0.2685, -0.0134,  0.2742])\n"
     ]
    }
   ],
   "source": [
    "print(cached_kvs['cache'][L,0,1,0,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:  torch.Size([10, 2, 3, 1, 13, 32]) torch.Size([10, 2, 3, 1, 13, 32])\n",
      "tensor([ 0.4633, -0.1938, -0.6470, -0.2119, -0.6190,  0.0410,  1.0812, -0.8186,\n",
      "        -0.2605,  0.5479,  0.9541, -0.5564, -0.5637,  1.2437,  0.0752,  0.3840,\n",
      "         0.9438, -0.8844,  0.5153, -0.0246, -0.2578, -0.3141,  0.2165, -0.2250,\n",
      "         1.3045,  0.3957, -0.4365,  0.9814,  1.0865,  0.2685, -0.0134,  0.2742])\n",
      "\n",
      "tensor([ 0.4633, -0.1938, -0.6470, -0.2119, -0.6190,  0.0410,  1.0812, -0.8186,\n",
      "        -0.2605,  0.5479,  0.9541, -0.5564, -0.5637,  1.2437,  0.0752,  0.3840,\n",
      "         0.9438, -0.8844,  0.5153, -0.0246, -0.2578, -0.3141,  0.2165, -0.2250,\n",
      "         1.3045,  0.3957, -0.4365,  0.9814,  1.0865,  0.2685, -0.0134,  0.2742])\n"
     ]
    }
   ],
   "source": [
    "print('shapes: ', cached_kvs_fs['cache'].shape, cached_kvs_s2['cache'].shape)\n",
    "N = 6\n",
    "L = 1\n",
    "print(cached_kvs_fs['cache'][L,0,1,0,N])\n",
    "print()\n",
    "print(cached_kvs_s2['cache'][L,0,1,0,N])\n",
    "assert torch.allclose(cached_kvs_fs['cache'][L,:,:,:,N], cached_kvs_s2['cache'][L,:,:,:,N], rtol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 3, 1, 13, 32])\n",
      "torch.Size([10, 2, 3, 1, 13, 32])\n"
     ]
    }
   ],
   "source": [
    "print(cached_kvs_fs['cache'].shape)\n",
    "print(cached_kvs_s2['cache'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.9226,  0.3684,  0.1545,  0.1768, -0.6461,  0.1462,  0.3829]) tensor([ 0.9226,  0.3684,  0.1545,  0.1768, -0.6460,  0.1462,  0.3829])\n"
     ]
    }
   ],
   "source": [
    "print(logits_s1[B, :, D], logits_fs[B, :logits_s1.shape[1], D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11, 32])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "tensor([ 0.9778,  0.3346,  0.4991, -0.7083, -0.3909]) tensor([ 0.9778,  0.3346,  0.4991, -0.7083, -0.3909])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x_og, i_logits, cache_kv = model(torch.tensor(tokenizer('/hello bro/')).unsqueeze(0))\n",
    "\n",
    "    x, i_logits, cache_kv = model(torch.tensor(tokenizer('/hello')).unsqueeze(0))\n",
    "    x_c, i_logits, cache_kv = model(torch.tensor(tokenizer(' bro/')).unsqueeze(0), cache=cache_kv)\n",
    "        #print(cache_kv.shape)\n",
    "\n",
    "    #print(x_og.shape, x_c.shape)\n",
    "print(x_og[0,6:, 0], x_c[0,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  9,  6, 13, 13, 16, 28,  3, 19, 16,  1])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor(tokenizer('/hello')), torch.tensor(tokenizer(' bro/'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 29]) torch.Size([1, 5, 29])\n",
      "tensor([ 0.4475, -0.1402, -0.5746,  0.9537, -1.0230]) tensor([ 0.3507, -0.1993, -0.5755,  0.7726, -1.1193])\n"
     ]
    }
   ],
   "source": [
    "print(x_og.shape, x_c.shape)\n",
    "print(x_og[0,6:, 0], x_c[0,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(x, length, cache):\n",
    "    x_len = length if length is not None else torch.tensor(x.shape[-2]).expand(x.shape[0])\n",
    "    cache_len = 0 if cache is None else cache['length']\n",
    "    total_len = x_len + cache_len\n",
    "    kv_mask = torch.arange(total_len.max(), device=x.device).expand(len(total_len), -1) >= total_len.unsqueeze(-1)\n",
    "    q_mask = torch.arange(x_len.max(), device=x.device).expand(len(x_len), -1) >= x_len.unsqueeze(-1)\n",
    "    attn_mask = ~(rearrange(~q_mask, \"b n -> b () n ()\") * rearrange(~kv_mask, \"b n -> b () () n\"))\n",
    "\n",
    "    if 1==1: #causal\n",
    "        causal_mask = torch.ones(attn_mask.shape[-2], attn_mask.shape[-1], device=x.device).triu(1 + attn_mask.shape[-2] - attn_mask.shape[-1]).bool()\n",
    "        attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "    return q_mask, attn_mask, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False, False, False, False, False, False, False, False, False, False]]),\n",
       " tensor([[[[False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False],\n",
       "           [False, False, False, False, False, False, False, False, False, False,\n",
       "            False, False, False, False, False]]]]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masks(x=torch.rand(1, 10,3), length=None, cache={'length': torch.tensor([5])})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
