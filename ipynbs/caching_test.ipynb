{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleit(obj, path):\n",
    "    import pickle as pkl\n",
    "    with open(path, 'wb') as f:\n",
    "        pkl.dump(obj, f)\n",
    "\n",
    "def loadit(path):\n",
    "    import pickle as pkl\n",
    "    with open(path, 'rb') as f:\n",
    "        return pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "from torch.utils.checkpoint import checkpoint # # gradient/activation checkpointing\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# token shifting\n",
    "# lucidrains implementation: https://github.com/lucidrains/x-transformers/blob/main/x_transformers/x_transformers.py\n",
    "# BlinkDL idea from RWKV-LM https://github.com/BlinkDL/RWKV-LM\n",
    "def shift(t, amount, mask = None):\n",
    "    if amount == 0:\n",
    "        return t\n",
    "    else:\n",
    "        amount = min(amount, t.shape[1])\n",
    "\n",
    "    if exists(mask):\n",
    "        t = t.masked_fill(~mask[..., None], 0.)\n",
    "\n",
    "    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n",
    "\n",
    "class ShiftTokens(nn.Module):\n",
    "    def __init__(self, shifts, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.shifts = tuple(shifts)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        mask = kwargs.get('mask', None)\n",
    "        shifts = self.shifts\n",
    "        segments = len(shifts)\n",
    "        feats_per_shift = x.shape[-1] // segments\n",
    "        splitted = x.split(feats_per_shift, dim = -1)\n",
    "        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n",
    "        segments_to_shift = list(map(lambda args: shift(*args, mask = mask), zip(segments_to_shift, shifts)))\n",
    "        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            activation()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                activation()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "\n",
    "    def forward(self, pos, indices, device, dtype):\n",
    "        pos = pos.to(device=device, dtype=dtype)\n",
    "        \n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos) \n",
    "      \n",
    "        bias = pos[indices]\n",
    "        #print(bias.shape)\n",
    "        bias = rearrange(bias, 'b i j h -> b h i j')\n",
    "        return bias\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1,))\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device = device).type_as(self.inv_freq)\n",
    "        sinu = einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n",
    "        return emb * self.scale\n",
    "\n",
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(t, groups = 1, dim = -1):\n",
    "    if groups == 1:\n",
    "        return F.normalize(t, p = 2, dim = dim)\n",
    "    t = rearrange(t, '... (g d) -> ... g d', g = groups)\n",
    "    t = F.normalize(t, p = 2, dim = dim)\n",
    "    return rearrange(t, '... g d -> ... (g d)')\n",
    "\n",
    "class CosineAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.shared_kv = kwargs.get('shared_kv', False)\n",
    "        self.talking_heads = kwargs.get('talking_heads', False)\n",
    "\n",
    "        self.n_feats, self.head_dim, self.n_heads = n_feats, head_dim, n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "        self.causal = causal\n",
    "\n",
    "        if self.talking_heads:\n",
    "            self._head_proj = nn.Conv2d(n_heads, n_heads, (1, 1))\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        if not self.shared_kv:\n",
    "            self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "            self.qkv = lambda x: rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=n_heads, d=head_dim)\n",
    "        else:\n",
    "            self.q_proj, self.kv_proj = [nn.Linear(n_feats, el, bias=bias) for el in [n_heads * head_dim, 2 * head_dim]]\n",
    "            map_q, map_kv = lambda q: rearrange(q, 'b n (h d) -> b h n d', h=n_heads), lambda kv: rearrange(kv, 'b n (kv d) -> kv b () n d', kv=2, d=head_dim)\n",
    "            self.qkv = lambda x: (map_q(self.q_proj(x)), *map_kv(self.kv_proj(x)))\n",
    "\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "    \n",
    "    def head_proj(self, dots):\n",
    "        if not self.talking_heads:\n",
    "            return dots\n",
    "        dots = self._head_proj(dots)\n",
    "        return dots      \n",
    "\n",
    "    def attend(self, query, key, value, attn_mask, pos_bias):\n",
    "        query, key = map(l2norm, (query, key))\n",
    "        \n",
    "        dots = einsum('bhid,bhjd->bhij', query, key) * self.temperature\n",
    "        dots = self.head_proj(dots)\n",
    "        #print(dots.shape, pos_bias.shape)\n",
    "        #dots += pos_bias  \n",
    "\n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "\n",
    "        attn = self.activation(dots)\n",
    "     \n",
    "        attn = self.dropout(attn)\n",
    "        return einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "\n",
    "\n",
    "    def attach_cache(self, kv, cache, cache_indices):\n",
    "        kv = torch.stack(kv, dim=0)\n",
    "        if cache is None:\n",
    "            return kv\n",
    "        zero_vector = torch.zeros_like(kv[:, :, :, :1, :])\n",
    "        kv_w_cache = torch.cat([cache, kv, zero_vector], dim=-2)\n",
    "        #print(kv_w_cache.shape)\n",
    "        kv_w_cache = torch.gather(kv_w_cache, dim=-2, index=cache_indices) # we do this to remove unnecessary padding\n",
    "        return kv_w_cache\n",
    "\n",
    "    def forward(self, x, pos_bias, mask, cache=None, cache_indices=None):\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "    \n",
    "        q, k, v  = self.qkv(x)\n",
    "        kv = self.attach_cache([k, v], cache, cache_indices)\n",
    "        k, v = kv\n",
    "\n",
    "        out = self.attend(q, k, v, mask, pos_bias)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out, kv\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(self.norm(x), *args, **kwargs)\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim = -1)\n",
    "        return x * self.act(gate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim, \n",
    "            depth, \n",
    "            heads, \n",
    "            dim_head, \n",
    "            causal=True,\n",
    "            temperature=15.5,\n",
    "            shared_temperture=False,\n",
    "            intermediate_loss=True,\n",
    "            dropout = 0.1,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            intermediate_loss = False\n",
    "\n",
    "        ff_mult = kwargs.get('ff_mult', 4)\n",
    "        self.checkpoint_every_n = kwargs.get('checkpoint_every_n', 0)\n",
    "        self.token_shift = kwargs.get('token_shift', False)\n",
    "        self.causal = causal\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature), requires_grad=True) if shared_temperture else temperature\n",
    "    \n",
    "\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.depth = depth\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = dim // 4,\n",
    "            heads = heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.token_shifter = lambda x: x\n",
    "        if self.token_shift:\n",
    "            self.token_shifter = ShiftTokens(range(0, 2), nn.Identity())\n",
    "        self.token_shift = lambda x: self.token_shifter(x)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, CosineAttention(\n",
    "                    dim, \n",
    "                    n_heads=heads, \n",
    "                    head_dim=dim_head, \n",
    "                    causal=causal,\n",
    "                    temperature=self.temperature,\n",
    "                    dropout=dropout,\n",
    "                    **kwargs\n",
    "                )),\n",
    "                PreNorm(dim, self.ff(dim, mult=ff_mult))\n",
    "            ]))\n",
    "\n",
    "    @staticmethod\n",
    "    def ff(dim, mult=4, dropout=0.1):\n",
    "        return nn.Sequential(\n",
    "            GLU(dim, dim * mult, nn.SiLU()),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_custom_forward(module):\n",
    "        def custom_forward(*args, **kwargs):\n",
    "            return module(*args, **kwargs)\n",
    "        return custom_forward\n",
    "\n",
    "    def checkpoint(self, layer, module, *args, **kwargs):\n",
    "        condition = self.training and self.checkpoint_every_n != 0 and layer < self.depth - 1 and layer % self.checkpoint_every_n == 0\n",
    "        return checkpoint(self.create_custom_forward(module), *args, **kwargs) if condition else module(*args, **kwargs)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cache(cache, layer):\n",
    "        if cache is None:\n",
    "            return None\n",
    "        return cache['cache'][layer]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cache_indices(x_lens, cache_lens, cache_kv, x):  \n",
    "        # used later w/ gather to remove padding when cache is concatenated with current input to remove padding\n",
    "        max_new_len = (x_lens + cache_lens).max()\n",
    "\n",
    "        B, H, N, D = x.shape[0], 1, (x.shape[1] + cache_kv.shape[-2]), cache_kv.shape[-1]\n",
    "        indices = []\n",
    "        for i in range(B): # stinky for loop to sort out indices for gather \n",
    "            cache_indices = torch.arange(cache_lens[i], device='cpu')\n",
    "            total_length = cache_lens[i] + x_lens[i]\n",
    "            diff_from_max_len = max_new_len - total_length\n",
    "            x_indices = torch.arange(x_lens[i]+diff_from_max_len, device='cpu') + cache_kv.shape[-2]\n",
    "            if diff_from_max_len > 0:\n",
    "                x_indices[-diff_from_max_len:] = N # last index will be used for padding\n",
    "            new_indices = torch.cat([cache_indices, x_indices])\n",
    "            indices.append(new_indices)\n",
    "\n",
    "        indices = torch.stack(indices, dim=0)\n",
    "        \n",
    "        indices = rearrange(indices, 'b n -> () b () n ()').expand(2, B, H,-1, D) # 2 for key and value\n",
    "        return indices.to(x.device)\n",
    "\n",
    "\n",
    "    def create_masks_and_positions(self, x, length, cache): # could clean this up ):\n",
    "        x_len = length if length is not None else torch.tensor(x.shape[-2]).expand(x.shape[0])\n",
    "        cache_len = cache['cache_lengths'] if exists(cache) else 0\n",
    "        total_len = x_len + cache_len\n",
    "        kv_mask = torch.arange(total_len.max(), device=x.device).expand(len(total_len), -1) >= total_len.unsqueeze(-1)\n",
    "        q_mask = torch.arange(x_len.max(), device=x.device).expand(len(x_len), -1) >= x_len.unsqueeze(-1)\n",
    "        attn_mask = ~(rearrange(~q_mask, \"b n -> b () n ()\") * rearrange(~kv_mask, \"b n -> b () () n\"))\n",
    "\n",
    "        causal_mask = repeat(torch.arange(total_len.max()), 'i -> b r i', b=len(total_len), r=x_len.max())\n",
    "        cache_offset = cache_len[:,None,None] if exists(cache) else cache_len\n",
    "        diagonal_offset = torch.arange(x_len.max())[None,:,None]\n",
    "\n",
    "        ## positional stuff ##\n",
    "        positional_grid = causal_mask - cache_offset - diagonal_offset \n",
    "        pos = torch.arange(positional_grid.min(), positional_grid.max()+1).flip(0)[:,None]\n",
    "        min_cache_len = 0 if cache_len.__class__ == int else cache_len.min()\n",
    "        positional_indices = ((positional_grid*-1) + (total_len.max() - min_cache_len - 1)) # shift so zero is the smallest number\n",
    "        #print(pos.shape, positional_indices.shape)\n",
    "        pos_bias = self.positional_bias(pos=pos, indices=positional_indices, dtype=x.dtype, device=x.device)\n",
    "        ## positional stuff ##\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = causal_mask >= (cache_offset + diagonal_offset + 1)\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask[:,None])\n",
    "            \n",
    "        return q_mask, attn_mask, total_len, x_len, cache_len, pos_bias\n",
    "\n",
    "    def forward(self, x, length=None, self_condtioning=None, cache=None):\n",
    "        intermediate_logits = []\n",
    "        cached_kvs = []\n",
    "    \n",
    "        mask, attn_mask, total_lens, x_len, cache_len, pos_bias = self.create_masks_and_positions(x, length, cache)\n",
    "    \n",
    "        #print(cache_len if exists(cache) else None)\n",
    "        cache_indices = self.get_cache_indices(x_len, cache_len, cache['cache'], x) if exists(cache) else None\n",
    "\n",
    "    \n",
    "        for i, (attn, ff) in enumerate(self.layers):\n",
    "\n",
    "            x = self.token_shift(x)\n",
    "            a_out, kv = self.checkpoint(i, attn, x, pos_bias, attn_mask, self.get_cache(cache, layer=i), cache_indices)\n",
    "            x = a_out + x\n",
    "            cached_kvs.append(kv)\n",
    "            x = self.checkpoint(i, ff, x) + x   \n",
    "\n",
    "            if i < self.depth - 1 and self_condtioning is not None:\n",
    "                x, logits = self_condtioning(x)\n",
    "                intermediate_logits.append(logits)\n",
    "\n",
    "        if len(intermediate_logits) > 0: # stack intermediate logits\n",
    "            intermediate_logits = torch.stack(intermediate_logits, dim=0) # D x B x N x L\n",
    "\n",
    "        cached_kvs = torch.stack(cached_kvs, dim=0) if len(cached_kvs) > 0 else None\n",
    "        cached_kvs = {'cache_lengths': total_lens, 'cache': cached_kvs} if exists(cached_kvs) else None\n",
    "\n",
    "\n",
    "        return x, intermediate_logits, cached_kvs\n",
    "\n",
    "class shared_embedding_output_layer(nn.Module):\n",
    "    '''Pass a embedding layer and then use this module as the output layer'''\n",
    "    def __init__(self, embedding_layer, bias=False):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.use_bias = bias\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(embedding_layer.weight.shape[0]))#\n",
    "            nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, weight=self.embedding_layer.weight, bias=self.bias if self.use_bias else None)\n",
    "\n",
    "\n",
    "class transformer_lm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        causal=True,\n",
    "        temperature=15.5,\n",
    "        dropout=0.,\n",
    "        shared_temperture=True,\n",
    "        self_conditioning=False,\n",
    "        intermediate_loss=True,\n",
    "        use_abs_pos=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self_conditioning == False\n",
    "\n",
    "        self.self_conditioning = True if self_conditioning else None\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.use_abs_pos = use_abs_pos\n",
    "        if self.use_abs_pos:\n",
    "            self.abs_pos_fn = ScaledSinuEmbedding(dim=dim)\n",
    "        self.abs_pos = lambda x: x + self.abs_pos_fn(x) if self.use_abs_pos else x\n",
    "\n",
    "        if self_conditioning:\n",
    "            self.reprojection_layer = nn.Linear(vocab_size, dim)\n",
    "\n",
    "\n",
    "        self.layers = transformer(\n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = heads, \n",
    "            dim_head = dim_head, \n",
    "            causal = causal, \n",
    "            dropout = dropout,\n",
    "            temperature = temperature,\n",
    "            shared_temperture = shared_temperture,\n",
    "            intermediate_loss = intermediate_loss,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.tie_embedding = kwargs.get('tie_embedding', False)\n",
    "        print('Tie embedding:', self.tie_embedding) if self.tie_embedding else None\n",
    " \n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "\n",
    "        self.to_logits = shared_embedding_output_layer(self.embedding) if self.tie_embedding else nn.Linear(dim, vocab_size)\n",
    "        \n",
    "\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "\n",
    "    def self_condition_fn(self):\n",
    "        def self_condition(x):\n",
    "            logits = self.to_logits(self.post_norm(x))\n",
    "            if self.self_conditioning: # not effective for LMs (intermediate loss is tho)\n",
    "                z = F.softmax(logits, dim=-1)\n",
    "                z = self.reprojection_layer(z)\n",
    "                x = z + x\n",
    "            return x, logits\n",
    "        return self_condition if (self.self_conditioning or self.intermediate_loss) and self.training else None\n",
    "\n",
    "\n",
    "    def forward(self, x, length=None, cache:Dict=None):\n",
    "        '''\n",
    "        x: [B, N] (embedding indices)\n",
    "        length: [B] (length of each sequence)\n",
    "        cache: {cache_lengths: [B, N], cache: [L, KV, B, H, N, D]} KV: key and value (2)\n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        x = self.abs_pos(x) \n",
    "  \n",
    "        x, interim_logits, cached_kvs = self.layers(x, length, self_condtioning=self.self_condition_fn(), cache=cache)\n",
    "        x = self.post_norm(x)\n",
    "        x = self.to_logits(x)\n",
    "\n",
    "        return  x, interim_logits, cached_kvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def collate_fn(tensors:List[torch.Tensor], pad_token:int):\n",
    "    max_len = max([t.shape[0] for t in tensors])\n",
    "    lengths = torch.tensor([t.shape[0] for t in tensors])\n",
    "    padded_tensors = [torch.cat([t, torch.full((max_len - t.shape[0],), pad_token, dtype=t.dtype)], dim=0) for t in tensors]\n",
    "    return torch.stack(padded_tensors, dim=0), lengths\n",
    "    \n",
    "class CharacterTokenizer(): # only for testing\n",
    "    def __init__(self):\n",
    "        self.vocab = ['#', '/'] + list(string.ascii_lowercase) + [' '] # bos/eos -> /, pad -> #\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.tokenize(text)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.token_to_id[token] for token in text]\n",
    "\n",
    "tokenizer = CharacterTokenizer()\n",
    "model = transformer_lm(\n",
    "    dim = 256,\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    depth = 10,\n",
    "    heads = 1,\n",
    "    dim_head = 32,\n",
    "    dropout=0.0,\n",
    "    causal = True,\n",
    "    shared_kv = True,\n",
    ")\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9, 12,  9]) tensor([3, 7, 6]) tensor([6, 5, 3])\n",
      "tensor([16, 21, 17])\n"
     ]
    }
   ],
   "source": [
    "print(b1_lengths + b2_lengths, b1_lengths, b2_lengths)\n",
    "print(fb_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THE LENGTHS TO MAKE THE CAUSAL MASK RATHER THAN THE OTHER THINGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 12,  9])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_kvs_s2['cache_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_b1, s2_b1, s3_b1 = torch.tensor(tokenizer('/hi')), torch.tensor(tokenizer('/buenos')), torch.tensor(tokenizer('/whats'))\n",
    "s1_b2, s2_b2, s3_b2 = torch.tensor(tokenizer(' there')), torch.tensor(tokenizer(' dias')), torch.tensor(tokenizer(' up'))\n",
    "s1_b3, s2_b3, s3_b3 = torch.tensor(tokenizer(' fella/')), torch.tensor(tokenizer(' captain/')), torch.tensor(tokenizer(' donkey/'))\n",
    "b1, b1_lengths = collate_fn([s1_b1, s2_b1, s3_b1], pad_token=tokenizer.token_to_id['#'])\n",
    "b2, b2_lengths = collate_fn([s1_b2, s2_b2, s3_b2], pad_token=tokenizer.token_to_id['#'])\n",
    "b3, b3_lengths = collate_fn([s1_b3, s2_b3, s3_b3], pad_token=tokenizer.token_to_id['#'])\n",
    "# comparsion set\n",
    "f_1, f_2, f_3 = torch.tensor(tokenizer('/hi there fella/')), torch.tensor(tokenizer('/buenos dias captain/')), torch.tensor(tokenizer('/whats up donkey/'))\n",
    "fb, fb_lengths = collate_fn([f_1, f_2, f_3], pad_token=tokenizer.token_to_id['#'])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_s1, interim_logits, cached_kvs = model(b1, length=b1_lengths)\n",
    "    logits_s2, interim_logits, cached_kvs_s2 = model(b2, length=b2_lengths, cache=cached_kvs)\n",
    "    logits_s3, interim_logits, cached_kvs_s3 = model(b3, length=b3_lengths, cache=cached_kvs_s2)\n",
    "    #print('third')\n",
    "    logits_fs, interim_logits, cached_kvs_fs = model(fb, length=fb_lengths)\n",
    "\n",
    "#logits_s1.masked_fill_(b1_lengths_mask.unsqueeze(-1), 0)\n",
    "\n",
    "#print('logits_s2:', logits_s2.shape, 'logits_fs:', logits_fs.shape)\n",
    "D, B = 1, 1\n",
    "#print(logits_s2[B, :, D], logits_fs[B, :, D])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 7, 6]) tensor([6, 5, 3]) tensor([7, 9, 8]) tensor([16, 21, 17])\n",
      "cache_len:  tensor([ 9, 12,  9]) tensor([ 9, 12,  9]) tensor([7, 9, 8]) tensor([16, 21, 17])\n"
     ]
    }
   ],
   "source": [
    "print(b1_lengths, b2_lengths, b3_lengths, fb_lengths)\n",
    "print('cache_len: ', b1_lengths+b2_lengths, cached_kvs_s2['cache_lengths'], b3_lengths, fb_lengths) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes:  torch.Size([10, 2, 3, 1, 21, 32]) torch.Size([10, 2, 3, 1, 12, 32])\n",
      "tensor([-0.0358,  0.3998, -0.1813, -0.6897,  0.4125, -0.0046,  0.3270,  0.4562,\n",
      "        -1.2174,  0.3249,  0.5883,  1.2496,  0.6172, -0.4362,  0.9562, -0.7017,\n",
      "         1.3587,  0.6092, -0.1841,  0.3618, -0.3768,  0.0638,  1.1105, -0.5938,\n",
      "         0.7717, -0.8628,  0.4942,  0.0017,  0.2373, -0.0265,  0.4116, -0.1227])\n",
      "\n",
      "tensor([-0.0358,  0.3998, -0.1813, -0.6897,  0.4125, -0.0046,  0.3270,  0.4562,\n",
      "        -1.2174,  0.3249,  0.5883,  1.2496,  0.6172, -0.4362,  0.9562, -0.7017,\n",
      "         1.3587,  0.6092, -0.1841,  0.3618, -0.3768,  0.0638,  1.1105, -0.5938,\n",
      "         0.7717, -0.8628,  0.4942,  0.0017,  0.2373, -0.0265,  0.4116, -0.1227])\n"
     ]
    }
   ],
   "source": [
    "print('shapes: ', cached_kvs_fs['cache'].shape, cached_kvs_s2['cache'].shape)\n",
    "c_lens = cached_kvs_fs['cache_lengths']\n",
    "mask = torch.arange(c_lens.max())[:,None] < c_lens[None,:]\n",
    "mask = ~mask.T\n",
    "mask = rearrange(mask, 'b i -> () () b () i ()')\n",
    "fs_cache =  cached_kvs_fs['cache'].masked_fill(mask, 0)\n",
    "\n",
    "N = 15\n",
    "L = 3\n",
    "I = 2\n",
    "kv = 0\n",
    "print(fs_cache[L,kv,I,0,N])\n",
    "print()\n",
    "print(cached_kvs_s3['cache'][L,kv,I,0,N])\n",
    "torch.allclose(fs_cache[L,kv,I,0,N], cached_kvs_s3['cache'][L,kv,I,0,N], rtol=0.0000001), 'failed custom check'\n",
    "assert torch.allclose(fs_cache, cached_kvs_s3['cache'], atol=0.000001), 'failed full check'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 15 is out of bounds for dimension 4 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cached_kvs_fs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m][L,kv,I,\u001b[38;5;241m0\u001b[39m,N][\u001b[38;5;241m0\u001b[39m], cached_kvs_s2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m][L,kv,I,\u001b[38;5;241m0\u001b[39m,N][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 15 is out of bounds for dimension 4 with size 12"
     ]
    }
   ],
   "source": [
    "cached_kvs_fs['cache'][L,kv,I,0,N][0], cached_kvs_s2['cache'][L,kv,I,0,N][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 3, 1, 21, 32])\n",
      "torch.Size([10, 2, 3, 1, 12, 32])\n"
     ]
    }
   ],
   "source": [
    "print(cached_kvs_fs['cache'].shape)\n",
    "print(cached_kvs_s2['cache'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7635, -0.1934,  0.6801, -0.2448, -0.5811,  0.6995, -0.2468]) tensor([ 0.7635, -0.1934,  0.6801, -0.2448, -0.5811,  0.6995, -0.2468])\n"
     ]
    }
   ],
   "source": [
    "print(logits_s1[B, :, D], logits_fs[B, :logits_s1.shape[1], D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0880,  1.2568,  0.6429, -0.5517, -0.0935]) tensor([-0.0880,  1.2568,  0.6429, -0.5517, -0.0935])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x_og, i_logits, cache_kv = model(torch.tensor(tokenizer('/hello bro/')).unsqueeze(0))\n",
    "\n",
    "    x, i_logits, cache_kv = model(torch.tensor(tokenizer('/hello')).unsqueeze(0))\n",
    "    x_c, i_logits, cache_kv = model(torch.tensor(tokenizer(' bro/')).unsqueeze(0), cache=cache_kv)\n",
    "        #print(cache_kv.shape)\n",
    "\n",
    "    #print(x_og.shape, x_c.shape)\n",
    "print(x_og[0,6:, 0], x_c[0,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  9,  6, 13, 13, 16, 28,  3, 19, 16,  1])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor(tokenizer('/hello')), torch.tensor(tokenizer(' bro/'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 29]) torch.Size([1, 5, 29])\n",
      "tensor([-0.0880,  1.2568,  0.6429, -0.5517, -0.0935]) tensor([-0.0880,  1.2568,  0.6429, -0.5517, -0.0935])\n"
     ]
    }
   ],
   "source": [
    "print(x_og.shape, x_c.shape)\n",
    "print(x_og[0,6:, 0], x_c[0,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(x, length, cache):\n",
    "    x_len = length if length is not None else torch.tensor(x.shape[-2]).expand(x.shape[0])\n",
    "    cache_len = 0 if cache is None else cache['length']\n",
    "    total_len = x_len + cache_len\n",
    "    kv_mask = torch.arange(total_len.max(), device=x.device).expand(len(total_len), -1) >= total_len.unsqueeze(-1)\n",
    "    q_mask = torch.arange(x_len.max(), device=x.device).expand(len(x_len), -1) >= x_len.unsqueeze(-1)\n",
    "    attn_mask = ~(rearrange(~q_mask, \"b n -> b () n ()\") * rearrange(~kv_mask, \"b n -> b () () n\"))\n",
    "\n",
    "    if 1==1: #causal\n",
    "        causal_mask = torch.ones(attn_mask.shape[-2], attn_mask.shape[-1], device=x.device).triu(1 + attn_mask.shape[-2] - attn_mask.shape[-1]).bool()\n",
    "        attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "    return q_mask, attn_mask, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_masks(x=torch.rand(1, 10,3), length=None, cache={'length': torch.tensor([5])})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
