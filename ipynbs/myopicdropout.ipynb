{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([5, 9, 94, 32, 64]) dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = torch.randn(5, 9, 94, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 93, 93, 93],\n",
       "        [ 1,  1,  1,  ..., 92, 92, 92],\n",
       "        [ 2,  2,  2,  ..., 91, 91, 91],\n",
       "        ...,\n",
       "        [91, 91, 91,  ...,  2,  2,  2],\n",
       "        [92, 92, 92,  ...,  1,  1,  1],\n",
       "        [93, 93, 93,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChunkGrid(3008, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    #chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1))\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3847)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
       "          12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,   23,\n",
       "          24,   25,   26,   27,   28,   29,   30,   31,   32,   33,   34,   35,\n",
       "          36,   37,   38,   39,   40,   41,   42,   43,   44,   45,   46,   47,\n",
       "          48,   49,   50,   51,   52,   53,   54,   55,   56,   57,   58,   59,\n",
       "          61,   63,   64,   65,   67,   68,   69,   70,   71,   74,   75,   76,\n",
       "          77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n",
       "          89,   90,   92,   93,   95,   99,  101,  103,  105,  106,  109,  111,\n",
       "         128,  129,  131,  132,  135,  137,  139,  140,  143,  146,  147,  148,\n",
       "         149,  151,  152,  156,  161,  163,  165,  171,  176,  177,  180,  181,\n",
       "         183,  186,  188,  189,  197,  198,  201,  214,  216,  222,  228,  231,\n",
       "         232,  240,  243,  244,  247,  253,  254,  268,  269,  275,  276,  279,\n",
       "         290,  291,  308,  317,  331,  348,  349,  350,  356,  371,  372,  382,\n",
       "         387,  392,  396,  401,  438,  496,  497,  502,  504,  510,  530,  550,\n",
       "         554,  555,  566,  574,  613,  626,  656,  687,  698,  713,  725,  738,\n",
       "         740,  768,  770,  795,  817,  838,  847,  918,  936,  998, 1321, 1630,\n",
       "        1686, 1691, 2571, 2652, 2998, 3171, 3417, 3847])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkgrid = ChunkGrid(4800, 48)\n",
    "pareto = torch.distributions.pareto.Pareto(torch.tensor(3.0), torch.tensor(2.0)).sample(chunkgrid.shape)\n",
    "chunkgrid = chunkgrid - pareto\n",
    "\n",
    "column = 0\n",
    "print(chunkgrid[column].topk(384, largest=False).indices.max())\n",
    "chunkgrid[column].topk(200, largest=False).indices.sort(-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4800])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkgrid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0097,  3.0432,  3.0915,  3.1019,  3.1028,  3.1398,  3.1784,  3.1840,\n",
      "         3.1873,  3.2184,  3.2200,  3.2216,  3.2373,  3.2410,  3.2466,  3.2763,\n",
      "         3.2913,  3.2934,  3.3168,  3.3351,  3.3378,  3.3578,  3.4119,  3.4313,\n",
      "         3.4581,  3.4597,  3.4775,  3.4794,  3.4946,  3.5376,  3.5586,  3.5749,\n",
      "         3.6269,  3.6638,  3.7310,  3.8028,  3.8267,  3.8380,  3.8780,  3.8874,\n",
      "         3.8957,  3.9028,  3.9122,  3.9925,  4.1036,  4.1047,  4.1234,  4.1829,\n",
      "         4.2224,  4.2514,  4.3246,  4.4109,  4.4347,  4.4368,  4.5100,  4.5780,\n",
      "         4.6048,  4.6141,  4.6461,  4.6887,  4.7531,  4.8639,  4.9481,  5.0410,\n",
      "         5.0525,  5.1421,  5.2515,  5.2779,  5.2822,  5.2838,  5.3425,  5.5370,\n",
      "         5.5552,  5.8407,  5.8487,  5.8869,  5.9556,  6.1609,  6.2159,  6.4114,\n",
      "         6.8111,  6.9793,  7.0003,  7.3768,  7.3914,  7.5698,  7.5836,  7.9328,\n",
      "         8.1207,  8.3174,  8.5855,  8.7896,  8.8310,  9.0010,  9.4844,  9.7659,\n",
      "        11.1205, 12.4118, 12.7426, 18.7468])\n"
     ]
    }
   ],
   "source": [
    "dist = torch.distributions.pareto.Pareto(3, 2).sample(torch.tensor([100]))\n",
    "plt.hist(dist)\n",
    "plt.show()\n",
    "print(dist.sort(-1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3847)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  12,   25,   28,   30,   57,   59,  132,  156,  161,  163,  198,  222,\n",
       "         228,  243,  268,  279,  348,  349,  350,  497,  530,  550,  554,  555,\n",
       "         656,  698,  725,  738,  795,  817,  838,  936, 1321, 1630, 1691, 2571,\n",
       "        2652, 2998, 3417, 3847])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chunkgrid[0].topk(40, largest=False).indices.max())\n",
    "chunkgrid[0].topk(40, largest=False).indices.sort(-1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 3).triu(1).bool().repeat_interleave(3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class MyopicAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        max_keep_keys=50,\n",
    "        chunk_window=3,\n",
    "        bias=True,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.max_keep_keys = max_keep_keys\n",
    "        self.W = chunk_window\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def pad_to_window_size(self, x, window_size, axis=3, mask=None):\n",
    "        \"\"\"\n",
    "        Pad the input on two sides to be divisible by `window_size`\n",
    "        \"\"\"\n",
    "        QKV, batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "        padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "        padding = torch.zeros(QKV, batch_size, heads, padding_length, hidden_size,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        mask = F.pad(mask, (0, padding_length), value=True) \n",
    "        return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "    def unpad(self, x, padding_length):\n",
    "        \"\"\"\n",
    "        Undo padding.\n",
    "        \"\"\"\n",
    "        if padding_length > 0:\n",
    "            return x[:, :-padding_length]\n",
    "        return x\n",
    "\n",
    "    def ChunkGrid(self, Total_Size, Block_Size):\n",
    "        Psize = Total_Size // Block_Size\n",
    "        chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "        #chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1)) # don't normalize cus it'll stretch the distribution by sequence length\n",
    "        return chunk_grid    \n",
    "\n",
    "    def causal_windowed_mask(self, window_number, window_size, device):\n",
    "        '''\n",
    "        Create a block diagonal causal mask, to prevent selecting future tokens in the topk key selection\n",
    "        '''\n",
    "        return torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "\n",
    "    def standard_forward(self, qkv, mask):\n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        positions = self.positional_bias(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        dots += positions\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal:\n",
    "            # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, value)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, return_attention=False):\n",
    "        assert mask is not None, 'pls wear a mask'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        tokeep = min(self.max_keep_keys, N) if self.max_keep_keys != -1 else N # number of keys to keep\n",
    "        W = min(self.W, N) if self.W != -1 else N # window size\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "\n",
    "        qkv, pad_n, mask = self.pad_to_window_size(qkv, W, axis=3, mask=mask) # add padding so it's divisible by W\n",
    "        q, kv = qkv[0], qkv[1:] # separate q and kv, we keep kv together for now as we apply the same operations to both\n",
    "        \n",
    "        q = rearrange(q, \"b h (n w) d -> b h n w d\", w=W)# split q into windows/chunks of size W\n",
    "      \n",
    "        q_mask = repeat(rearrange(mask, \"b (n w) -> b n w\", w=W), \"b n w -> b h n w\", h=H) # do the same for the mask\n",
    "            \n",
    "        kv = repeat(kv, \"kv b h n d -> kv b h nw n d\", nw=q.shape[2]) # duplicate k and v for total number of windows\n",
    "        #print(q.shape, kv.shape)\n",
    "        KV, B, H, NW, N, D = kv.shape\n",
    "\n",
    "        chunkgrid = self.ChunkGrid(Total_Size=N, Block_Size=W).to(q.device)\n",
    "        chunkgrid = repeat(chunkgrid, \"w n -> b h w n\", b=B, h=H).contiguous()\n",
    "\n",
    "        SCALE = torch.tensor(3.0, device=q.device, dtype=q.dtype)\n",
    "        ALPHA = torch.tensor(2.0, device=q.device, dtype=q.dtype)\n",
    "        pareto_dist = torch.distributions.pareto.Pareto(SCALE, ALPHA).sample(chunkgrid.shape).to(q.device)\n",
    "        chunkgrid = chunkgrid - pareto_dist\n",
    "\n",
    "        chunkgrid = repeat(chunkgrid, \"b h w n -> kv b h w n\", kv=2)\n",
    "\n",
    "        cmask = repeat(mask, 'b n -> kv b h nw n', kv=2, h=H, nw=NW)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = self.causal_windowed_mask(window_number=NW, window_size=W, device=q.device)\n",
    "            cmask = torch.logical_or(cmask, causal_mask)\n",
    "        \n",
    "        chunkgrid = chunkgrid.masked_fill(cmask, torch.finfo(q.dtype).max) # max cus we topk in reverse order \n",
    "\n",
    "        keep_indices = chunkgrid.topk(k=tokeep, dim=-1, sorted=False, largest=False).indices.sort(dim=-1).values\n",
    "        KV, B, H, NW, N, D = kv.shape \n",
    "        kv = kv.gather(-2, repeat(keep_indices, \"kv b h w n -> kv b h w n d\", d=D))\n",
    "\n",
    "        kv_mask = repeat(mask, \"b n -> b h nw n\", h=H, nw=NW)\n",
    "     \n",
    "        kv_mask = kv_mask.gather(-1, keep_indices[0])\n",
    "\n",
    "        k, v = kv\n",
    "        # nw (number of windows) = p (in the einsum below)\n",
    "        dots = einsum(\"b h n p d, b h n z d -> b h n p z \", q, k) * self.scale # Z is number of chunks in Q, N is max sequence length after dropping\n",
    "        print(dots.shape)\n",
    "        ## positional stuff\n",
    "        pos_bias = self.positional_bias(N, device=dots.device, dtype=dots.dtype)\n",
    "        pos_bias = repeat(pos_bias, 'h i j -> b h i j', b = B)\n",
    "        pos_bias = rearrange(pos_bias, 'b h (n w) j -> b h n w j', w = W)\n",
    "\n",
    "        keep_indices = repeat(keep_indices, \"kv b h nw n -> kv b h nw w n\", w=W)[0] \n",
    "        pos_bias = pos_bias.gather(-1, keep_indices)\n",
    "        \n",
    "        dots = dots + pos_bias\n",
    "\n",
    "        mask_val = -torch.finfo(dots.dtype).max\n",
    "        \n",
    "        qk_mask = rearrange(q_mask, \"b h n w -> b h n w ()\") * rearrange(kv_mask, \"b h w n -> b h w () n\")\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = keep_indices > rearrange(torch.arange(0, N, device=q.device), \"(nw w) -> w nw ()\", w=NW, nw=W)\n",
    "            qk_mask = torch.logical_or(qk_mask, causal_mask)\n",
    "    \n",
    "        dots.masked_fill_(qk_mask, mask_val)\n",
    "      \n",
    "        #print(dots.shape)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        print(attn.shape)\n",
    "\n",
    "        normal_attn = self.standard_forward(qkv=qkv, mask=mask)\n",
    "        normal_attn = rearrange(normal_attn, \"b h n d -> b n (h d)\")\n",
    "     \n",
    "\n",
    "        out = einsum(\"b h n w z, b h n z d -> b h n w d\", attn, v) \n",
    "        print(attn.shape, v.shape, out.shape)\n",
    "\n",
    "        out = rearrange(out, \"b h n w d -> b (n w) (h d)\")\n",
    "        return out, normal_attn\n",
    "        \n",
    "        out = self.unpad(out, pad_n)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "     \n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 12, 10, 100, 1000])\n",
      "torch.Size([10, 12, 10, 100, 1000])\n",
      "torch.Size([10, 12, 10, 100, 1000]) torch.Size([10, 12, 10, 1000, 24]) torch.Size([10, 12, 10, 100, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = MyopicAttention(n_feats=216, head_dim=24, n_heads=12, max_keep_keys=-1, chunk_window=100, causal=True)\n",
    "\n",
    "x = torch.ones(10, 1000, 216) + torch.randn(10, 1000, 216) * 0.01\n",
    "mask = torch.zeros(10, 1000).bool()\n",
    "mask[0, 0:10] = True\n",
    "mask[2, 23:45] = True\n",
    "\n",
    "attn = attention(x, mask)\n",
    "torch.allclose(attn[0], attn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "slice() cannot be applied to a 0-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [612], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m num2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mallclose(attn[\u001b[38;5;241m1\u001b[39m][num1,num2][:\u001b[38;5;241m99\u001b[39m],attn[\u001b[38;5;241m0\u001b[39m][num1,num2][:\u001b[38;5;241m99\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: slice() cannot be applied to a 0-dim tensor."
     ]
    }
   ],
   "source": [
    "num1 = 0\n",
    "num2 = 100\n",
    "torch.allclose(attn[1][num1,num2][:99],attn[0][num1,num2][:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "slice() cannot be applied to a 0-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [613], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attn[\u001b[38;5;241m0\u001b[39m][num1,num2][:\u001b[38;5;241m99\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: slice() cannot be applied to a 0-dim tensor."
     ]
    }
   ],
   "source": [
    "attn[0][num1,num2][:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9017,  1.2341,  0.1318, -0.3404,  0.2027,  0.1517,  0.9933, -1.0023,\n",
       "         0.4215,  0.4376, -1.6437,  0.7163,  0.6535, -0.7211,  0.8963, -0.0382,\n",
       "        -0.0511, -0.6504,  0.7377, -0.6373,  0.3636,  1.0683,  0.5238, -0.4440,\n",
       "        -0.0612, -0.1117, -1.1908,  0.2908, -0.0528, -0.9548, -0.3116,  0.4130,\n",
       "        -0.6204,  0.5859, -0.3136,  0.1222, -0.4089, -0.6751, -0.7990, -1.0568,\n",
       "         0.5428, -1.2991, -0.0865, -0.1419,  0.3109, -0.1384,  0.0252,  0.3017,\n",
       "         0.2061, -0.5738,  0.6518,  0.2316,  0.9323,  0.1007,  0.1936, -0.3836,\n",
       "        -1.2462,  1.0776, -1.1836,  0.9172,  0.5507,  0.1429, -0.4009,  0.4086,\n",
       "         0.0516, -0.1978,  0.4702,  0.2351, -0.0570, -0.8962, -0.0337, -0.0521,\n",
       "         0.8836, -0.7612,  0.3654,  0.0920, -0.0477, -0.5163, -0.3073,  0.4050,\n",
       "        -0.1547, -0.6694,  0.6186,  0.0226, -0.1409,  0.0384,  0.7714,  0.1069,\n",
       "        -0.3933,  0.0884,  0.9205, -0.8535, -0.3086, -0.1124,  0.8949, -0.2679,\n",
       "         0.1135, -0.0592, -1.0804], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[1][num1,num2][:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attn[0], attn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0,1,2,3,4,5,6]).topk(k=3, sorted=False, largest=False).indices.sort(dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,\n",
       "         24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,\n",
       "         38,  39,  40,  41,  42,  43,  44,  45,  46,  47, 524, 525, 526, 527,\n",
       "        528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541,\n",
       "        542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555,\n",
       "        556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569,\n",
       "        570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583,\n",
       "        584, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626,\n",
       "        627, 628, 629, 630, 631, 632, 633, 634, 636, 637, 638, 639, 640, 641,\n",
       "        642, 643, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656,\n",
       "        657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670,\n",
       "        671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684,\n",
       "        685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698,\n",
       "        699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712,\n",
       "        713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726,\n",
       "        727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
       "        741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754,\n",
       "        755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 785])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(attn[1], 'b h nw w k -> b h (nw w) k')[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 1000, 216)\n",
    "mask = torch.zeros(10, 1000).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD CHUNKGRID SHIT\n",
    "def ChunkGrid(self, Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1))\n",
    "    return chunk_grid    \n",
    "\n",
    "chunkgrid = repeat(chunkgrid, \"w n -> b h w n\", b=B, h=H).contiguous()\n",
    "MEAN = torch.tensor(0, device=q.device, dtype=q.dtype)\n",
    "STD = torch.tensor(0.125, device=q.device, dtype=q.dtype)\n",
    "uniform_dist = torch.distributions.normal.Normal(MEAN, STD).sample(chunkgrid.shape).to(q.device)\n",
    "chunkgrid += uniform_dist\n",
    "chunkgrid = repeat(chunkgrid, \"b h w n -> kv b h w n\", kv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_windowed_mask(window_number, window_size, device):\n",
    "    mask = torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_windowed_mask(3, 4, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.indices.sort().values[0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(2).unsqueeze(-1).repeat(2).expand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km[0, 0, :, 0, :100].sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.arange(0, 3008).repeat(3008,1) - torch.arange(0, 3008).repeat(3008,1).T).reshape(32, -1, 3008).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- duplicated across KV\n",
    "- each batch, head and Window have a different view of the keys\n",
    "- 94 is the number of windows i.e 94*32(win size) = 3008 (sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv[km].reshape(2, 5, 8, 3, -1, 24).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat(cg, \"W N -> KV B H W N\", B=5, H=8, KV=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(N_BLOCKS, BLOCK_SIZE):\n",
    "    chunk_grid = (torch.arange(0, N_BLOCKS).repeat(BLOCK_SIZE,1) - torch.arange(0, BLOCK_SIZE).repeat(N_BLOCKS,1).T).repeat_interleave(BLOCK_SIZE, dim=1).abs()\n",
    "    chunk_grid = chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1)\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ChunkGrid(41, 3)\n",
    "uniform_dist = torch.distributions.uniform.Uniform(0, 1).sample(cg.shape)\n",
    "cg += uniform_dist\n",
    "keep_indices = cg.topk(9, dim=-1).indices\n",
    "keep_mask = torch.zeros_like(cg).scatter_(1, keep_indices, 1).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
