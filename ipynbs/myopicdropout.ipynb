{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([5, 9, 94, 32, 64]) dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = torch.randn(5, 9, 94, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 93, 93, 93],\n",
       "        [ 1,  1,  1,  ..., 92, 92, 92],\n",
       "        [ 2,  2,  2,  ..., 91, 91, 91],\n",
       "        ...,\n",
       "        [91, 91, 91,  ...,  2,  2,  2],\n",
       "        [92, 92, 92,  ...,  1,  1,  1],\n",
       "        [93, 93, 93,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChunkGrid(3008, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    #chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1))\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2976)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
       "          12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,   23,\n",
       "          24,   25,   26,   27,   28,   29,   30,   31,   32,   33,   34,   35,\n",
       "          36,   37,   38,   39,   40,   41,   42,   43,   44,   45,   46,   47,\n",
       "          48,   49,   50,   51,   53,   54,   55,   56,   57,   58,   59,   60,\n",
       "          61,   62,   63,   64,   65,   66,   68,   70,   72,   73,   74,   75,\n",
       "          76,   77,   78,   79,   80,   81,   82,   83,   84,   85,   86,   87,\n",
       "          88,   90,   91,   92,   93,   94,   95,   96,  100,  105,  106,  108,\n",
       "         111,  112,  116,  118,  119,  121,  122,  123,  128,  129,  130,  132,\n",
       "         133,  139,  140,  141,  142,  143,  144,  153,  159,  160,  164,  165,\n",
       "         171,  179,  181,  182,  185,  194,  195,  201,  203,  204,  207,  209,\n",
       "         211,  212,  213,  218,  220,  222,  223,  228,  240,  241,  244,  246,\n",
       "         249,  262,  268,  270,  271,  273,  281,  283,  287,  290,  294,  301,\n",
       "         308,  315,  326,  348,  352,  358,  367,  371,  418,  430,  453,  460,\n",
       "         470,  501,  504,  509,  530,  559,  620,  632,  650,  684,  700,  705,\n",
       "         773,  812,  828,  865,  885,  926,  957,  992, 1075, 1116, 1208, 1230,\n",
       "        1337, 1461, 1570, 1673, 2391, 2443, 2538, 2976])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkgrid = ChunkGrid(4800, 48)\n",
    "pareto = torch.distributions.pareto.Pareto(torch.tensor(3.0), torch.tensor(2.0)).sample(chunkgrid.shape)\n",
    "chunkgrid = chunkgrid - pareto\n",
    "\n",
    "column = 0\n",
    "print(chunkgrid[column].topk(384, largest=False).indices.max())\n",
    "chunkgrid[column].topk(200, largest=False).indices.sort(-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4800])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkgrid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0027,  3.0276,  3.0841,  3.0949,  3.1100,  3.1404,  3.1466,  3.1557,\n",
      "         3.1773,  3.1794,  3.1864,  3.1913,  3.2046,  3.2129,  3.2437,  3.2550,\n",
      "         3.2893,  3.2930,  3.3105,  3.3153,  3.3707,  3.3853,  3.3963,  3.4286,\n",
      "         3.4443,  3.4512,  3.4530,  3.4659,  3.4859,  3.5483,  3.5705,  3.6011,\n",
      "         3.6134,  3.6304,  3.6466,  3.6544,  3.7388,  3.7417,  3.8170,  3.8332,\n",
      "         3.8828,  3.9243,  4.0115,  4.0353,  4.0695,  4.1411,  4.2320,  4.2701,\n",
      "         4.3045,  4.3887,  4.4338,  4.4428,  4.4532,  4.5289,  4.8246,  4.9139,\n",
      "         5.0659,  5.1310,  5.2852,  5.4057,  5.4943,  5.6852,  5.7500,  5.8086,\n",
      "         6.3482,  6.4263,  6.6473,  6.7006,  6.7580,  6.7968,  6.8142,  6.8534,\n",
      "         6.9199,  6.9989,  7.4794,  7.7167,  7.7324,  7.9597,  8.0943,  8.1085,\n",
      "         8.1113,  8.1790,  8.2046,  8.5093,  8.6637,  8.6658,  9.5260,  9.5810,\n",
      "        10.8826, 13.5426, 14.4840, 14.7551, 15.0706, 19.0224, 21.5217, 24.3253,\n",
      "        27.8317, 30.5397, 34.8064, 49.2122])\n"
     ]
    }
   ],
   "source": [
    "dist = torch.distributions.pareto.Pareto(3, 2).sample(torch.tensor([100]))\n",
    "plt.hist(dist)\n",
    "plt.show()\n",
    "print(dist.sort(-1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2976)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   2,   21,   31,   38,   40,   68,   77,   80,   93,  100,  119,  121,\n",
       "         129,  142,  182,  201,  220,  240,  249,  273,  281,  308,  348,  352,\n",
       "         367,  501,  509,  559,  700,  705,  865,  926,  957,  992, 1116, 1230,\n",
       "        1461, 1673, 2538, 2976])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chunkgrid[0].topk(40, largest=False).indices.max())\n",
    "chunkgrid[0].topk(40, largest=False).indices.sort(-1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 3).triu(1).bool().repeat_interleave(3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSigmoid():\n",
    "    \"\"\"\n",
    "    TAKEN FROM: https://github.com/yandexdataschool/gumbel_lstm/blob/master/gumbel_sigmoid.py\n",
    "    A gumbel-sigmoid nonlinearity with gumbel(0,1) noize\n",
    "    In short, it's a function that mimics #[a>0] indicator where a is the logit\n",
    "    \n",
    "    Explaination and motivation: https://arxiv.org/abs/1611.01144\n",
    "    \n",
    "    Math:\n",
    "    Sigmoid is a softmax of two logits: a and 0\n",
    "    e^a / (e^a + e^0) = 1 / (1 + e^(0 - a)) = sigm(a)\n",
    "    \n",
    "    Gumbel-sigmoid is a gumbel-softmax for same logits:\n",
    "    gumbel_sigm(a) = e^([a+gumbel1]/t) / [ e^([a+gumbel1]/t) + e^(gumbel2/t)]\n",
    "    where t is temperature, gumbel1 and gumbel2 are two samples from gumbel noize: -log(-log(uniform(0,1)))\n",
    "    gumbel_sigm(a) = 1 / ( 1 +  e^(gumbel2/t - [a+gumbel1]/t) = 1 / ( 1+ e^(-[a + gumbel1 - gumbel2]/t)\n",
    "    gumbel_sigm(a) = sigm([a+gumbel1-gumbel2]/t)\n",
    "    \n",
    "    For computation reasons:\n",
    "    gumbel1-gumbel2 = -log(-log(uniform1(0,1)) +log(-log(uniform2(0,1)) = -log( log(uniform2(0,1)) / log(uniform1(0,1)) )\n",
    "    gumbel_sigm(a) = sigm([a-log(log(uniform2(0,1))/log(uniform1(0,1))]/t)\n",
    "    \n",
    "    \n",
    "    :param t: temperature of sampling. Lower means more spike-like sampling. Can be symbolic.\n",
    "    :param eps: a small number used for numerical stability\n",
    "    :returns: a callable that can (and should) be used as a nonlinearity\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, t=0.1, eps=1e-20):\n",
    "        self.temperature=t\n",
    "        self.eps=eps\n",
    "         \n",
    "    def __call__(self,logits):\n",
    "        \"\"\"computes a gumbel sigmoid sample\"\"\"\n",
    "                \n",
    "        #sample from Gumbel(0, 1)\n",
    "        uniform1 = torch.rand(logits.shape)\n",
    "        uniform2 = torch.rand(logits.shape)\n",
    "        \n",
    "        noise = -torch.log(torch.log(uniform2 + self.eps)/torch.log(uniform1 + self.eps) +self.eps)\n",
    "        \n",
    "        #draw a sample from the Gumbel-Sigmoid distribution\n",
    "        return ((logits + noise) / self.temperature).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSigmoid():\n",
    "    \"\"\"\n",
    "    adapted from: https://github.com/yandexdataschool/gumbel_lstm/blob/master/gumbel_sigmoid.py\n",
    "    and https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, t=0.1, eps=None):\n",
    "        self.temperature=t\n",
    "         \n",
    "    def __call__(self,logits):\n",
    "\n",
    "        \"\"\"computes a gumbel sigmoid sample\"\"\"\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / self.temperature\n",
    "        gumbels = gumbels.sigmoid()\n",
    "        return gumbels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GumbelSigmoid at 0x7f099bfd63d0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GumbelSigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class MyopicAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        max_keep_keys=50,\n",
    "        chunk_window=3,\n",
    "        bias=True,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.max_keep_keys = max_keep_keys\n",
    "        self.W = chunk_window\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def pad_to_window_size(self, x, window_size, axis=3, mask=None):\n",
    "        \"\"\"\n",
    "        Pad the input on two sides to be divisible by `window_size`\n",
    "        \"\"\"\n",
    "        QKV, batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "        padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "        padding = torch.zeros(QKV, batch_size, heads, padding_length, hidden_size,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        mask = F.pad(mask, (0, padding_length), value=True) \n",
    "        return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "    def unpad(self, x, padding_length):\n",
    "        \"\"\"\n",
    "        Undo padding.\n",
    "        \"\"\"\n",
    "        if padding_length > 0:\n",
    "            return x[:, :-padding_length]\n",
    "        return x\n",
    "\n",
    "    def ChunkGrid(self, Total_Size, Block_Size):\n",
    "        Psize = Total_Size // Block_Size\n",
    "        chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "        #chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1)) # don't normalize cus it'll stretch the distribution by sequence length\n",
    "        return chunk_grid    \n",
    "\n",
    "    def causal_windowed_mask(self, window_number, window_size, device):\n",
    "        '''\n",
    "        Create a block diagonal causal mask, to prevent selecting future tokens in the topk key selection\n",
    "        '''\n",
    "        return torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "\n",
    "    def standard_forward(self, qkv, mask):\n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        positions = self.positional_bias(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        dots += positions\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal:\n",
    "            # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, value)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, return_attention=False):\n",
    "        assert mask is not None, 'pls wear a mask'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        tokeep = min(self.max_keep_keys, N) if self.max_keep_keys != -1 else N # number of keys to keep\n",
    "        W = min(self.W, N) if self.W != -1 else N # window size\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "\n",
    "        qkv, pad_n, mask = self.pad_to_window_size(qkv, W, axis=3, mask=mask) # add padding so it's divisible by W\n",
    "        q, kv = qkv[0], qkv[1:] # separate q and kv, we keep kv together for now as we apply the same operations to both\n",
    "        \n",
    "        q = rearrange(q, \"b h (n w) d -> b h n w d\", w=W)# split q into windows/chunks of size W\n",
    "      \n",
    "        q_mask = repeat(rearrange(mask, \"b (n w) -> b n w\", w=W), \"b n w -> b h n w\", h=H) # do the same for the mask\n",
    "            \n",
    "        kv = repeat(kv, \"kv b h n d -> kv b h nw n d\", nw=q.shape[2]) # duplicate k and v for total number of windows\n",
    "        #print(q.shape, kv.shape)\n",
    "        KV, B, H, NW, N, D = kv.shape\n",
    "\n",
    "        chunkgrid = self.ChunkGrid(Total_Size=N, Block_Size=W).to(q.device)\n",
    "        chunkgrid = repeat(chunkgrid, \"w n -> b h w n\", b=B, h=H).contiguous()\n",
    "\n",
    "        SCALE = torch.tensor(3.0, device=q.device, dtype=q.dtype)\n",
    "        ALPHA = torch.tensor(2.0, device=q.device, dtype=q.dtype)\n",
    "        pareto_dist = torch.distributions.pareto.Pareto(SCALE, ALPHA).sample(chunkgrid.shape).to(q.device)\n",
    "        chunkgrid = chunkgrid - pareto_dist\n",
    "\n",
    "        chunkgrid = repeat(chunkgrid, \"b h w n -> kv b h w n\", kv=2)\n",
    "\n",
    "        cmask = repeat(mask, 'b n -> kv b h nw n', kv=2, h=H, nw=NW)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = self.causal_windowed_mask(window_number=NW, window_size=W, device=q.device)\n",
    "            cmask = torch.logical_or(cmask, causal_mask)\n",
    "        \n",
    "        chunkgrid = chunkgrid.masked_fill(cmask, torch.finfo(q.dtype).max) # max cus we topk in reverse order \n",
    "\n",
    "        keep_indices = chunkgrid.topk(k=tokeep, dim=-1, sorted=False, largest=False).indices.sort(dim=-1).values\n",
    "        KV, B, H, NW, N, D = kv.shape \n",
    "        kv = kv.gather(-2, repeat(keep_indices, \"kv b h w n -> kv b h w n d\", d=D))\n",
    "\n",
    "        kv_mask = repeat(mask, \"b n -> b h nw n\", h=H, nw=NW)\n",
    "     \n",
    "        kv_mask = kv_mask.gather(-1, keep_indices[0])\n",
    "\n",
    "        k, v = kv\n",
    "        # nw (number of windows) = p (in the einsum below)\n",
    "        dots = einsum(\"b h n p d, b h n z d -> b h n p z \", q, k) * self.scale # Z is number of chunks in Q, N is max sequence length after dropping\n",
    "       \n",
    "        ## positional stuff\n",
    "        pos_bias = self.positional_bias(N, device=dots.device, dtype=dots.dtype)\n",
    "        pos_bias = repeat(pos_bias, 'h i j -> b h i j', b = B)\n",
    "        pos_bias = rearrange(pos_bias, 'b h (n w) j -> b h n w j', w = W)\n",
    "\n",
    "        keep_indices = repeat(keep_indices, \"kv b h nw n -> kv b h nw w n\", w=W)[0] \n",
    "        pos_bias = pos_bias.gather(-1, keep_indices)\n",
    "        \n",
    "        dots = dots + pos_bias\n",
    "\n",
    "        mask_val = -torch.finfo(dots.dtype).max\n",
    "        \n",
    "        qk_mask = rearrange(q_mask, \"b h n w -> b h n w ()\") * rearrange(kv_mask, \"b h w n -> b h w () n\")\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = keep_indices > rearrange(torch.arange(0, N, device=q.device), \"(nw w) -> w nw ()\", w=NW, nw=W)\n",
    "            qk_mask = torch.logical_or(qk_mask, causal_mask)\n",
    "    \n",
    "        dots.masked_fill_(qk_mask, mask_val)\n",
    "      \n",
    "        #print(dots.shape)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "      \n",
    "\n",
    "        normal_attn = self.standard_forward(qkv=qkv, mask=mask)\n",
    "        normal_attn = rearrange(normal_attn, \"b h n d -> b n (h d)\")\n",
    "     \n",
    "\n",
    "        out = einsum(\"b h n w z, b h n z d -> b h n w d\", attn, v) \n",
    "\n",
    "        out = rearrange(out, \"b h n w d -> b (n w) (h d)\")\n",
    "   \n",
    "        \n",
    "        out = self.unpad(out, pad_n)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "     \n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class MyopicAttention3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        max_keep_keys=50,\n",
    "        chunk_window=3,\n",
    "        bias=True,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.max_keep_keys = max_keep_keys\n",
    "        self.W = chunk_window\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.grid_pos_projection = nn.Linear(1, head_dim*n_heads)\n",
    "        self.grid_k_projection = nn.Linear(head_dim, head_dim)\n",
    "        self.grid_activation = nn.ReLU()\n",
    "        self.grid_scaler_projection = nn.Linear(head_dim, 1)\n",
    "        self.gumbel_sigmoid = GumbelSigmoid(t=0.1, eps=1e-20)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def pad_to_window_size(self, x, window_size, axis=3, mask=None):\n",
    "        \"\"\"\n",
    "        Pad the input on two sides to be divisible by `window_size`\n",
    "        \"\"\"\n",
    "        QKV, batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "        padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "        padding = torch.zeros(QKV, batch_size, heads, padding_length, hidden_size,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        mask = F.pad(mask, (0, padding_length), value=True) \n",
    "        return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "    def unpad(self, x, padding_length):\n",
    "        \"\"\"\n",
    "        Undo padding.\n",
    "        \"\"\"\n",
    "        if padding_length > 0:\n",
    "            return x[:, :-padding_length]\n",
    "        return x\n",
    "\n",
    "    def valuegrid(self, total_size, block_size, k):\n",
    "        n = total_size // block_size\n",
    "        device, dtype = k.device, k.dtype\n",
    "        # get time\n",
    "        t = time.time()\n",
    "        indices = (rearrange(torch.arange(n, device = device), 'i -> i 1') - rearrange(torch.arange(n, device = device), 'j -> 1 j')) + (n - 1)\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = torch.float32)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "        pos = self.grid_pos_projection(pos)[indices]\n",
    "    \n",
    "        pos = pos.repeat_interleave(block_size, dim=1)\n",
    "        pos = rearrange(pos, \"p n (h d) -> () h p n d\", h = self.n_heads, d = self.head_dim)\n",
    "        print(\"pos\", time.time() - t)\n",
    "    \n",
    "        # get time\n",
    "        t = time.time()\n",
    "        k_voting = self.grid_k_projection(k)\n",
    "        print(\"k_voting k projection\", time.time() - t)\n",
    "        k_voting = k_voting + pos\n",
    "        t = time.time()\n",
    "        k_voting = self.grid_activation(k_voting)\n",
    "        print(\"k_voting activation\", time.time() - t)\n",
    "        t = time.time()\n",
    "        k_voting = self.grid_scaler_projection(k_voting).squeeze(-1)\n",
    "        print(\"k_voting scaler projection\", time.time() - t)\n",
    "        t = time.time()\n",
    "        k_voting = k_voting / k_voting.sum(dim=-1, keepdim=True)\n",
    "        print(\"normalization\", time.time() - t)\n",
    "        t = time.time()\n",
    "        k_voting = self.gumbel_sigmoid(k_voting)\n",
    "        print(\"gumbel sigmoid\", time.time() - t)\n",
    "        return k_voting    \n",
    "\n",
    "    def causal_windowed_mask(self, window_number, window_size, device):\n",
    "        '''Create a block diagonal causal mask, to prevent selecting future tokens in the topk key selection'''\n",
    "        return torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "\n",
    "    def standard_forward(self, qkv, mask):\n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        positions = self.positional_bias(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        dots += positions\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal:\n",
    "            # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, value)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, return_attention=False):\n",
    "        assert mask is not None, 'pls wear a mask'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        tokeep = min(self.max_keep_keys, N) if self.max_keep_keys != -1 else N # number of keys to keep\n",
    "        W = min(self.W, N) if self.W != -1 else N # window size\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "\n",
    "        qkv, pad_n, mask = self.pad_to_window_size(qkv, W, axis=3, mask=mask) # add padding so it's divisible by W\n",
    "        q, kv = qkv[0], qkv[1:] # separate q and kv, we keep kv together for now as we apply the same operations to both\n",
    "        \n",
    "        q = rearrange(q, \"b h (n w) d -> b h n w d\", w=W)# split q into windows/chunks of size W\n",
    "      \n",
    "        q_mask = repeat(rearrange(mask, \"b (n w) -> b n w\", w=W), \"b n w -> b h n w\", h=H) # do the same for the mask\n",
    "            \n",
    "        kv = repeat(kv, \"kv b h n d -> kv b h nw n d\", nw=q.shape[2]) # duplicate k and v for total number of windows\n",
    "        #print(q.shape, kv.shape)\n",
    "        KV, B, H, NW, N, D = kv.shape\n",
    "\n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        valuegrid = self.valuegrid(total_size=N, block_size=W, k=kv[0]).to(kv.device)\n",
    "        valuegrid = repeat(valuegrid, \"b h w n -> kv b h w n\", kv=2)\n",
    "        kv = kv * valuegrid.unsqueeze(-1) # maybe just do this for the keys? idk\n",
    "        print(f'value gridding done in: {time.time() - t:.2f}s')\n",
    "\n",
    "        cmask = repeat(mask, 'b n -> kv b h nw n', kv=2, h=H, nw=NW)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = self.causal_windowed_mask(window_number=NW, window_size=W, device=q.device)\n",
    "            cmask = torch.logical_or(cmask, causal_mask)\n",
    "        \n",
    "        valuegrid = valuegrid.masked_fill(cmask, -torch.finfo(q.dtype).max) \n",
    "\n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        keep_indices = valuegrid.topk(k=tokeep, dim=-1, sorted=False, largest=True).indices.sort(dim=-1).values\n",
    "        print(f'topk done in: {time.time() - t:.2f}s')\n",
    "        KV, B, H, NW, N, D = kv.shape \n",
    "       \n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        kv = kv.gather(-2, repeat(keep_indices, \"kv b h w n -> kv b h w n d\", d=D))\n",
    "        print(f'gather done in: {time.time() - t:.2f}s')\n",
    "\n",
    "        kv_mask = repeat(mask, \"b n -> b h nw n\", h=H, nw=NW)\n",
    "     \n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        kv_mask = kv_mask.gather(-1, keep_indices[0])\n",
    "        print(f'kv mask done in: {time.time() - t:.2f}s')\n",
    "\n",
    "        k, v = kv\n",
    "        # nw (number of windows) = p (in the einsum below)\n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        dots = einsum(\"b h n p d, b h n z d -> b h n p z \", q, k) * self.scale # Z is number of chunks in Q, N is max sequence length after dropping\n",
    "        print(f'dots done in: {time.time() - t:.2f}s')\n",
    "\n",
    "\n",
    "        ## positional stuff\n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        pos_bias = self.positional_bias(N, device=dots.device, dtype=dots.dtype)\n",
    "        pos_bias = repeat(pos_bias, 'h i j -> b h i j', b = B)\n",
    "        pos_bias = rearrange(pos_bias, 'b h (n w) j -> b h n w j', w = W)\n",
    "\n",
    "        keep_indices = repeat(keep_indices, \"kv b h nw n -> kv b h nw w n\", w=W)[0] \n",
    "        pos_bias = pos_bias.gather(-1, keep_indices)\n",
    "        \n",
    "        dots = dots + pos_bias\n",
    "        print(f'pos bias done in: {time.time() - t:.2f}s')\n",
    "\n",
    "        mask_val = -torch.finfo(dots.dtype).max\n",
    "        \n",
    "        qk_mask = rearrange(q_mask, \"b h n w -> b h n w ()\") * rearrange(kv_mask, \"b h w n -> b h w () n\")\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = keep_indices > rearrange(torch.arange(0, N, device=q.device), \"(nw w) -> w nw ()\", w=NW, nw=W)\n",
    "            qk_mask = torch.logical_or(qk_mask, causal_mask)\n",
    "    \n",
    "        dots.masked_fill_(qk_mask, mask_val)\n",
    "      \n",
    "        #print(dots.shape)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "      \n",
    "\n",
    "        #normal_attn = self.standard_forward(qkv=qkv, mask=mask)\n",
    "        #normal_attn = rearrange(normal_attn, \"b h n d -> b n (h d)\")\n",
    "     \n",
    "\n",
    "        out = einsum(\"b h n w z, b h n z d -> b h n w d\", attn, v) \n",
    "\n",
    "        out = rearrange(out, \"b h n w d -> b (n w) (h d)\")\n",
    "   \n",
    "        \n",
    "        out = self.unpad(out, pad_n)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "     \n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 0.37087130546569824\n",
      "k_voting k projection 1.5005145072937012\n",
      "k_voting activation 0.903881311416626\n",
      "k_voting scaler projection 0.1942126750946045\n",
      "normalization 0.025990724563598633\n",
      "gumbel sigmoid 1.5739057064056396\n",
      "value gridding done in: 8.22s\n",
      "topk done in: 0.47s\n",
      "gather done in: 0.08s\n",
      "kv mask done in: 0.01s\n",
      "dots done in: 0.22s\n",
      "pos bias done in: 0.74s\n"
     ]
    }
   ],
   "source": [
    "attention = MyopicAttention3(n_feats=216, head_dim=24, n_heads=12, max_keep_keys=256, chunk_window=48, causal=True)\n",
    "\n",
    "x = torch.ones(10, 5000, 216) + torch.randn(10, 5000, 216) * 0.01\n",
    "mask = torch.zeros(10, 5000).bool()\n",
    "mask[0, 0:10] = True\n",
    "mask[2, 23:45] = True\n",
    "\n",
    "attn= attention(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1762,  0.0375, -0.0757,  ..., -0.4059,  0.0893,  0.2033],\n",
       "         [ 0.1762,  0.0375, -0.0757,  ..., -0.4059,  0.0893,  0.2033],\n",
       "         [ 0.2048,  0.0727, -0.0292,  ..., -0.4337,  0.1257,  0.2507],\n",
       "         ...,\n",
       "         [ 0.3993,  0.0694, -0.1596,  ..., -0.7297,  0.1368,  0.3698],\n",
       "         [ 0.4000,  0.0698, -0.1586,  ..., -0.7304,  0.1366,  0.3703],\n",
       "         [ 0.3991,  0.0694, -0.1587,  ..., -0.7312,  0.1353,  0.3711]],\n",
       "\n",
       "        [[ 0.2733, -0.0692,  0.0803,  ..., -0.4667,  0.2940,  0.2760],\n",
       "         [ 0.1897,  0.0028, -0.0534,  ..., -0.4747,  0.1475,  0.1296],\n",
       "         [ 0.1232,  0.0283, -0.0190,  ..., -0.3945,  0.1204,  0.2007],\n",
       "         ...,\n",
       "         [ 0.4019,  0.0690, -0.1576,  ..., -0.7312,  0.1378,  0.3730],\n",
       "         [ 0.4021,  0.0692, -0.1567,  ..., -0.7305,  0.1368,  0.3733],\n",
       "         [ 0.4029,  0.0689, -0.1560,  ..., -0.7296,  0.1371,  0.3733]],\n",
       "\n",
       "        [[ 0.1604, -0.0411,  0.2058,  ..., -0.2483,  0.2510,  0.2636],\n",
       "         [ 0.1253,  0.0116, -0.0204,  ..., -0.2253,  0.1403,  0.2904],\n",
       "         [ 0.1359,  0.0186, -0.0633,  ..., -0.3076,  0.1638,  0.1821],\n",
       "         ...,\n",
       "         [ 0.4012,  0.0695, -0.1598,  ..., -0.7305,  0.1363,  0.3702],\n",
       "         [ 0.4012,  0.0687, -0.1590,  ..., -0.7310,  0.1372,  0.3715],\n",
       "         [ 0.4009,  0.0683, -0.1601,  ..., -0.7312,  0.1369,  0.3713]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2140,  0.1152, -0.1405,  ..., -0.3341,  0.1541,  0.3900],\n",
       "         [ 0.1379,  0.0692, -0.0622,  ..., -0.3783,  0.0578,  0.2077],\n",
       "         [ 0.1812,  0.0203, -0.0517,  ..., -0.3819,  0.1133,  0.1854],\n",
       "         ...,\n",
       "         [ 0.4011,  0.0697, -0.1596,  ..., -0.7293,  0.1367,  0.3739],\n",
       "         [ 0.4027,  0.0690, -0.1588,  ..., -0.7296,  0.1367,  0.3744],\n",
       "         [ 0.4027,  0.0699, -0.1578,  ..., -0.7303,  0.1366,  0.3722]],\n",
       "\n",
       "        [[ 0.0107,  0.0433,  0.1180,  ..., -0.3569,  0.0532,  0.1916],\n",
       "         [ 0.1740, -0.0282, -0.0087,  ..., -0.3478,  0.0866,  0.2267],\n",
       "         [ 0.1721, -0.0900,  0.0139,  ..., -0.3651,  0.1484,  0.1780],\n",
       "         ...,\n",
       "         [ 0.4013,  0.0705, -0.1578,  ..., -0.7317,  0.1352,  0.3723],\n",
       "         [ 0.4005,  0.0691, -0.1580,  ..., -0.7321,  0.1346,  0.3732],\n",
       "         [ 0.3996,  0.0681, -0.1572,  ..., -0.7313,  0.1340,  0.3727]],\n",
       "\n",
       "        [[ 0.2289,  0.2586, -0.1206,  ..., -0.5004,  0.0305,  0.3359],\n",
       "         [ 0.1955,  0.1729, -0.1682,  ..., -0.4833,  0.0391,  0.2559],\n",
       "         [ 0.1757,  0.0873, -0.1025,  ..., -0.4519,  0.0818,  0.2425],\n",
       "         ...,\n",
       "         [ 0.4027,  0.0689, -0.1587,  ..., -0.7321,  0.1362,  0.3725],\n",
       "         [ 0.4009,  0.0693, -0.1583,  ..., -0.7312,  0.1363,  0.3724],\n",
       "         [ 0.4012,  0.0704, -0.1586,  ..., -0.7313,  0.1356,  0.3732]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 8, 1024, 24])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7., -6., -5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.nn.Linear(1, 10)\n",
    "b = torch.arange(-7.0,7.0)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter\n",
    "\n",
    "class MyopicAttention2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        max_keep_keys=50,\n",
    "        chunk_window=3,\n",
    "        bias=True,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.max_keep_keys = max_keep_keys\n",
    "        self.W = chunk_window\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.half_precision_mode = kwargs.get(\"half_precision_mode\", 'float16') # 'float16' or 'bfloat16' or float32\n",
    "\n",
    "        self.scale = nn.Parameter(torch.tensor(kwargs.get('scale', 3.0), requires_grad=True))\n",
    "        self.alpha = nn.Parameter(torch.tensor(kwargs.get('alpha', 2.0), requires_grad=True))\n",
    "        self.distance_multiplier = nn.Parameter(torch.tensor([kwargs.get('distance_multiplier_prior', 1.0)]*n_heads, requires_grad=True))\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def pad_to_window_size(self, x, window_size, axis=3, mask=None):\n",
    "        \"\"\"\n",
    "        Pad the input on two sides to be divisible by `window_size`\n",
    "        \"\"\"\n",
    "        QKV, batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "        padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "        padding = torch.zeros(QKV, batch_size, heads, padding_length, hidden_size,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        mask = F.pad(mask, (0, padding_length), value=True) \n",
    "        return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "    def unpad(self, x, padding_length):\n",
    "        \"\"\"\n",
    "        Undo padding.\n",
    "        \"\"\"\n",
    "        if padding_length > 0:\n",
    "            return x[:, :-padding_length]\n",
    "        return x\n",
    "\n",
    "    def ChunkGrid(self, Total_Size, Block_Size):\n",
    "        Psize = Total_Size // Block_Size\n",
    "        chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "        return chunk_grid #* self.distance_multiplier.to(chunk_grid.device)\n",
    "\n",
    "    def causal_windowed_mask(self, window_number, window_size, device):\n",
    "        '''\n",
    "        Create a block diagonal causal mask, to prevent selecting future tokens in the topk key selection\n",
    "        '''\n",
    "        return torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "\n",
    "\n",
    "    def half_precision_if_on_cuda(self, x, is_cuda):\n",
    "        if not is_cuda:\n",
    "            return x\n",
    "        elif self.half_precision_mode == 'float16':\n",
    "            return x.half()\n",
    "        elif self.half_precision_mode == 'bfloat16':\n",
    "            return x.bfloat16()\n",
    "        else:\n",
    "            return x\n",
    "      \n",
    "\n",
    "    def standard_forward(self, qkv, mask):\n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        positions = self.positional_bias(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        dots += positions\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal:\n",
    "            # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, value)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, return_attention=False):\n",
    "        assert mask is not None, 'pls wear a mask'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        tokeep = min(self.max_keep_keys, N) if self.max_keep_keys != -1 else N # number of keys to keep\n",
    "        W = min(self.W, N) if self.W != -1 else N # window size\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "\n",
    "        qkv, pad_n, mask = self.pad_to_window_size(qkv, W, axis=3, mask=mask) # add padding so it's divisible by W\n",
    "        q, kv = qkv[0], qkv[1:] # separate q and kv, we keep kv together for now as we apply the same operations to both\n",
    "        \n",
    "        q = rearrange(q, \"b h (n w) d -> b h n w d\", w=W) # split q into windows/chunks of size W\n",
    "      \n",
    "        q_mask = repeat(rearrange(mask, \"b (n w) -> b n w\", w=W), \"b n w -> b h n w\", h=H) # do the same for the mask\n",
    "        kv = repeat(kv, \"kv b h n d -> kv b h nw n d\", nw=q.shape[2]) # duplicate k and v for total number of windows\n",
    "        \n",
    "        KV, B, H, NW, N, D = kv.shape\n",
    "\n",
    "        chunkgrid = self.ChunkGrid(Total_Size=N, Block_Size=W).to(q.device)\n",
    "        chunkgrid = self.half_precision_if_on_cuda(chunkgrid, q.is_cuda)\n",
    "        chunkgrid = repeat(chunkgrid, \"w n -> b h w n\", b=B, h=H).contiguous() \n",
    "        distance_multiplier = self.half_precision_if_on_cuda(self.distance_multiplier, q.is_cuda).to(q.device)\n",
    "        distance_multiplier = repeat(self.distance_multiplier, \"h -> b h w n\", b=B, w=NW, n=N)\n",
    "        chunkgrid = chunkgrid * distance_multiplier\n",
    "        \n",
    "\n",
    "        SCALE = self.scale.to(q.device).to(chunkgrid.dtype)\n",
    "        ALPHA = self.alpha.to(q.device).to(chunkgrid.dtype)\n",
    "        pareto_dist = torch.distributions.pareto.Pareto(SCALE, ALPHA).rsample(chunkgrid.shape).to(chunkgrid.device) # rsample so we can backprop\n",
    "        chunkgrid = chunkgrid - pareto_dist\n",
    "\n",
    "        chunkgrid = repeat(chunkgrid, \"b h w n -> kv b h w n\", kv=2)\n",
    "\n",
    "        cmask = repeat(mask, 'b n -> kv b h nw n', kv=2, h=H, nw=NW)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = self.causal_windowed_mask(window_number=NW, window_size=W, device=q.device)\n",
    "            cmask = torch.logical_or(cmask, causal_mask)\n",
    "        \n",
    "        chunkgrid = chunkgrid.masked_fill(cmask, torch.finfo(chunkgrid.dtype).max) # max cus we topk in reverse order \n",
    "\n",
    "        keep_indices = chunkgrid.topk(k=tokeep, dim=-1, sorted=False, largest=False)\n",
    "        '''\n",
    "         we want to take half of the keep indices (the ones with the largest vals) and apply a softmax to the values\n",
    "         then scatter the values with a multiply reduction to the k tensor\n",
    "         this allows the model to learn which keys to keep using the parametized pareto distribution\n",
    "         so kinda works like a relu (but not really)\n",
    "        '''\n",
    "        sorted_vals_with_indices = keep_indices.values.sort(-1)\n",
    "        num_to_scatter = tokeep // 4 # number of keys to scatter to kv\n",
    "        scatter_indices = sorted_vals_with_indices.indices[..., -num_to_scatter:].long() # indices of the keys to scatter\n",
    "        scatter_indices = repeat(scatter_indices[0], 'b h w n -> b h w n d', d=D)\n",
    "        scatter_vals = sorted_vals_with_indices.values[..., -num_to_scatter:] # values of the keys to scatter\n",
    "        scatter_vals = scatter_vals[0].softmax(-1) #* -1 + 1 # softmax but we want the smallest values to be the largest\n",
    "        scatter_vals = repeat(scatter_vals, 'b h w n -> b h w n d', d=D)\n",
    "        scatter_vals = scatter_vals * torch.randn(D, device=scatter_vals.device, dtype=scatter_vals.dtype) # add some noise to the values\n",
    "\n",
    "        print(scatter_vals.shape, 'scatter_vals')\n",
    "        print(kv.shape, 'kv')\n",
    "\n",
    "        kv = kv.contiguous() # we need kv to be contigous so we can scatter properly\n",
    "        kv[0] = scatter(\n",
    "            src = scatter_vals,\n",
    "            index = scatter_indices,\n",
    "            dim = -2,\n",
    "            out = kv[0].clone(),\n",
    "            reduce = 'mul'\n",
    "        )\n",
    "\n",
    "        ###\n",
    "        keep_indices = keep_indices.indices.sort(dim=-1).values\n",
    "        KV, B, H, NW, N, D = kv.shape \n",
    "        kv = kv.gather(-2, repeat(keep_indices, \"kv b h w n -> kv b h w n d\", d=D))\n",
    "\n",
    "        kv_mask = repeat(mask, \"b n -> b h nw n\", h=H, nw=NW)\n",
    "     \n",
    "        kv_mask = kv_mask.gather(-1, keep_indices[0])\n",
    "\n",
    "        k, v = kv\n",
    "        # nw (number of windows) = p (in the einsum below)\n",
    "        dots = einsum(\"b h n p d, b h n z d -> b h n p z \", q, k) * self.scale # Z is number of chunks in Q, N is max sequence length after dropping\n",
    "       \n",
    "        ## positional stuff\n",
    "        pos_bias = self.positional_bias(N, device=dots.device, dtype=dots.dtype)\n",
    "        pos_bias = repeat(pos_bias, 'h i j -> b h i j', b = B)\n",
    "        pos_bias = rearrange(pos_bias, 'b h (n w) j -> b h n w j', w = W)\n",
    "\n",
    "        keep_indices = repeat(keep_indices, \"kv b h nw n -> kv b h nw w n\", w=W)[0] \n",
    "        pos_bias = pos_bias.gather(-1, keep_indices)\n",
    "        \n",
    "        dots = dots + pos_bias\n",
    "\n",
    "        mask_val = -torch.finfo(dots.dtype).max\n",
    "        \n",
    "        qk_mask = rearrange(q_mask, \"b h n w -> b h n w ()\") * rearrange(kv_mask, \"b h w n -> b h w () n\")\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = keep_indices > rearrange(torch.arange(0, N, device=q.device), \"(nw w) -> w nw ()\", w=NW, nw=W)\n",
    "            qk_mask = torch.logical_or(qk_mask, causal_mask)\n",
    "    \n",
    "        dots.masked_fill_(qk_mask, mask_val)\n",
    "      \n",
    "        #print(dots.shape)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "      \n",
    "\n",
    "        normal_attn = self.standard_forward(qkv=qkv, mask=mask)\n",
    "        normal_attn = rearrange(normal_attn, \"b h n d -> b n (h d)\")\n",
    "     \n",
    "\n",
    "        out = einsum(\"b h n w z, b h n z d -> b h n w d\", attn, v) \n",
    "\n",
    "        out = rearrange(out, \"b h n w d -> b (n w) (h d)\")\n",
    "   \n",
    "        \n",
    "        out = self.unpad(out, pad_n)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "     \n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value gridding done in: 2.22s\n"
     ]
    }
   ],
   "source": [
    "attention = MyopicAttention3(n_feats=216, head_dim=24, n_heads=12, max_keep_keys=256, chunk_window=128, causal=True)\n",
    "\n",
    "x = torch.ones(10, 5000, 216) + torch.randn(10, 5000, 216) * 0.01\n",
    "mask = torch.zeros(10, 5000).bool()\n",
    "mask[0, 0:10] = True\n",
    "mask[2, 23:45] = True\n",
    "\n",
    "attn = attention(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
       "        [ 6.,  7.,  8.,  9., 10.],\n",
       "        [ 1.,  2.,  3.,  4.,  5.]], dtype=torch.float16,\n",
       "       grad_fn=<CppNode<ScatterMul>>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_scatter import scatter\n",
    "\n",
    "src = torch.arange(1, 11, requires_grad=True, dtype=torch.float32).reshape((2, 5))\n",
    "\n",
    "index = torch.tensor([[2,2,2,2,2],[1,1,1,1,1]])\n",
    "\n",
    "out = torch.ones(3, 5, dtype=src.dtype, requires_grad=True)\n",
    "# Broadcasting in the first and last dim.\n",
    "\n",
    "out2 = scatter(\n",
    "    src=src.clone().to(torch.float16),\n",
    "    index=index.clone().long(),\n",
    "    dim=0,\n",
    "    out=out.clone().to(torch.float16),\n",
    "    reduce='mul'\n",
    ")\n",
    "\n",
    "out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
       "        [ 6.,  7.,  8.,  9., 10.],\n",
       "        [ 1.,  2.,  3.,  4.,  5.]], grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.arange(1, 11, requires_grad=True, dtype=torch.float32).reshape((2, 5))\n",
    "index = torch.tensor([[2,2,2,2,2],[1,1,1,1,1]])\n",
    "out = torch.ones(3, 5, dtype=src.dtype, requires_grad=True)\n",
    "out = out.scatter(0, index, src, reduce=\"multiply\")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num1 = 0\n",
    "num2 = 100\n",
    "torch.allclose(attn[1][num1,num2][:99],attn[0][num1,num2][:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8768,  0.5403, -0.4982, -0.6676, -0.1726, -0.2841, -0.7748,  1.2937,\n",
       "         0.0952, -0.1790,  1.2793, -0.0180,  0.9306,  0.0510,  0.3877, -0.3233,\n",
       "         0.4503,  0.1754, -0.1733, -0.5921,  0.0930, -0.1853,  0.6594, -0.5966,\n",
       "         0.3983,  1.2907, -1.0663,  0.8151,  0.4143,  0.9949, -0.8028, -0.7665,\n",
       "         0.3114, -0.3060,  0.2174, -0.0145,  0.5195, -0.0629,  0.4824, -0.2134,\n",
       "         0.6487, -0.5543, -0.3998,  0.4835,  0.0325, -0.4887,  0.2463,  0.4638,\n",
       "         0.8298, -0.3127,  0.1380, -0.2188,  0.5023, -0.7221, -0.8195, -0.4364,\n",
       "         0.8616, -0.3429,  0.1433, -0.4033, -0.3301, -0.0268, -0.0619,  1.1195,\n",
       "         0.2910, -0.1543,  0.0028, -0.2260,  0.4619, -0.6561, -1.0260,  0.9179,\n",
       "         0.4099,  0.3758, -0.3285,  0.2757,  0.1249,  0.2334, -0.4744,  0.2244,\n",
       "         0.1443,  0.5162, -0.4617,  1.6897, -0.0114, -0.1633,  1.4041,  0.0071,\n",
       "         0.1075, -0.0576, -0.3595,  1.0768, -0.2301,  0.2440, -0.2712, -0.1436,\n",
       "        -0.4499,  0.0817,  0.1923], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[0][num1,num2][:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8763,  0.5412, -0.4970, -0.6680, -0.1719, -0.2844, -0.7754,  1.2929,\n",
       "         0.0941, -0.1780,  1.2803, -0.0186,  0.9319,  0.0527,  0.3864, -0.3259,\n",
       "         0.4511,  0.1756, -0.1716, -0.5920,  0.0946, -0.1866,  0.6605, -0.5974,\n",
       "         0.3983,  1.2904, -1.0669,  0.8148,  0.4136,  0.9948, -0.8030, -0.7670,\n",
       "         0.3114, -0.3058,  0.2181, -0.0146,  0.5198, -0.0632,  0.4835, -0.2136,\n",
       "         0.6495, -0.5541, -0.3987,  0.4832,  0.0317, -0.4898,  0.2457,  0.4634,\n",
       "         0.8292, -0.3139,  0.1377, -0.2188,  0.5017, -0.7225, -0.8183, -0.4363,\n",
       "         0.8627, -0.3435,  0.1434, -0.4036, -0.3294, -0.0279, -0.0622,  1.1187,\n",
       "         0.2912, -0.1556,  0.0032, -0.2260,  0.4617, -0.6569, -1.0256,  0.9165,\n",
       "         0.4104,  0.3760, -0.3257,  0.2749,  0.1252,  0.2296, -0.4745,  0.2247,\n",
       "         0.1442,  0.5181, -0.4622,  1.6930, -0.0105, -0.1656,  1.4024,  0.0040,\n",
       "         0.1096, -0.0572, -0.3583,  1.0765, -0.2305,  0.2418, -0.2765, -0.1430,\n",
       "        -0.4499,  0.0819,  0.1923], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[1][num1,num2][:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attn[0], attn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([0,1,2,3,4,5,6]).topk(k=3, sorted=False, largest=False).indices.sort(dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 1000, 216)\n",
    "mask = torch.zeros(10, 1000).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD CHUNKGRID SHIT\n",
    "def ChunkGrid(self, Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1))\n",
    "    return chunk_grid    \n",
    "\n",
    "chunkgrid = repeat(chunkgrid, \"w n -> b h w n\", b=B, h=H).contiguous()\n",
    "MEAN = torch.tensor(0, device=q.device, dtype=q.dtype)\n",
    "STD = torch.tensor(0.125, device=q.device, dtype=q.dtype)\n",
    "uniform_dist = torch.distributions.normal.Normal(MEAN, STD).sample(chunkgrid.shape).to(q.device)\n",
    "chunkgrid += uniform_dist\n",
    "chunkgrid = repeat(chunkgrid, \"b h w n -> kv b h w n\", kv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_windowed_mask(window_number, window_size, device):\n",
    "    mask = torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_windowed_mask(3, 4, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.indices.sort().values[0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(2).unsqueeze(-1).repeat(2).expand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km[0, 0, :, 0, :100].sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.arange(0, 3008).repeat(3008,1) - torch.arange(0, 3008).repeat(3008,1).T).reshape(32, -1, 3008).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- duplicated across KV\n",
    "- each batch, head and Window have a different view of the keys\n",
    "- 94 is the number of windows i.e 94*32(win size) = 3008 (sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv[km].reshape(2, 5, 8, 3, -1, 24).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat(cg, \"W N -> KV B H W N\", B=5, H=8, KV=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(N_BLOCKS, BLOCK_SIZE):\n",
    "    chunk_grid = (torch.arange(0, N_BLOCKS).repeat(BLOCK_SIZE,1) - torch.arange(0, BLOCK_SIZE).repeat(N_BLOCKS,1).T).repeat_interleave(BLOCK_SIZE, dim=1).abs()\n",
    "    chunk_grid = chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1)\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ChunkGrid(41, 3)\n",
    "uniform_dist = torch.distributions.uniform.Uniform(0, 1).sample(cg.shape)\n",
    "cg += uniform_dist\n",
    "keep_indices = cg.topk(9, dim=-1).indices\n",
    "keep_mask = torch.zeros_like(cg).scatter_(1, keep_indices, 1).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
