{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([5, 9, 94, 32, 64]) dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = torch.randn(5, 9, 94, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkGrid(3008, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    #chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1))\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkgrid = ChunkGrid(4800, 48)\n",
    "pareto = torch.distributions.pareto.Pareto(torch.tensor(3.0), torch.tensor(2.0)).sample(chunkgrid.shape)\n",
    "chunkgrid = chunkgrid - pareto\n",
    "\n",
    "column = 0\n",
    "print(chunkgrid[column].topk(384, largest=False).indices.max())\n",
    "chunkgrid[column].topk(200, largest=False).indices.sort(-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkgrid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = torch.distributions.pareto.Pareto(3, 2).sample(torch.tensor([100]))\n",
    "plt.hist(dist)\n",
    "plt.show()\n",
    "print(dist.sort(-1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunkgrid[0].topk(40, largest=False).indices.max())\n",
    "chunkgrid[0].topk(40, largest=False).indices.sort(-1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(3, 3).triu(1).bool().repeat_interleave(3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSigmoid():\n",
    "    \"\"\"\n",
    "    TAKEN FROM: https://github.com/yandexdataschool/gumbel_lstm/blob/master/gumbel_sigmoid.py\n",
    "    A gumbel-sigmoid nonlinearity with gumbel(0,1) noize\n",
    "    In short, it's a function that mimics #[a>0] indicator where a is the logit\n",
    "    \n",
    "    Explaination and motivation: https://arxiv.org/abs/1611.01144\n",
    "    \n",
    "    Math:\n",
    "    Sigmoid is a softmax of two logits: a and 0\n",
    "    e^a / (e^a + e^0) = 1 / (1 + e^(0 - a)) = sigm(a)\n",
    "    \n",
    "    Gumbel-sigmoid is a gumbel-softmax for same logits:\n",
    "    gumbel_sigm(a) = e^([a+gumbel1]/t) / [ e^([a+gumbel1]/t) + e^(gumbel2/t)]\n",
    "    where t is temperature, gumbel1 and gumbel2 are two samples from gumbel noize: -log(-log(uniform(0,1)))\n",
    "    gumbel_sigm(a) = 1 / ( 1 +  e^(gumbel2/t - [a+gumbel1]/t) = 1 / ( 1+ e^(-[a + gumbel1 - gumbel2]/t)\n",
    "    gumbel_sigm(a) = sigm([a+gumbel1-gumbel2]/t)\n",
    "    \n",
    "    For computation reasons:\n",
    "    gumbel1-gumbel2 = -log(-log(uniform1(0,1)) +log(-log(uniform2(0,1)) = -log( log(uniform2(0,1)) / log(uniform1(0,1)) )\n",
    "    gumbel_sigm(a) = sigm([a-log(log(uniform2(0,1))/log(uniform1(0,1))]/t)\n",
    "    \n",
    "    \n",
    "    :param t: temperature of sampling. Lower means more spike-like sampling. Can be symbolic.\n",
    "    :param eps: a small number used for numerical stability\n",
    "    :returns: a callable that can (and should) be used as a nonlinearity\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, t=0.1, eps=1e-20):\n",
    "        self.temperature=t\n",
    "        self.eps=eps\n",
    "         \n",
    "    def __call__(self,logits):\n",
    "        \"\"\"computes a gumbel sigmoid sample\"\"\"\n",
    "                \n",
    "        #sample from Gumbel(0, 1)\n",
    "        uniform1 = torch.rand(logits.shape)\n",
    "        uniform2 = torch.rand(logits.shape)\n",
    "        \n",
    "        noise = -torch.log(torch.log(uniform2 + self.eps)/torch.log(uniform1 + self.eps) +self.eps)\n",
    "        \n",
    "        #draw a sample from the Gumbel-Sigmoid distribution\n",
    "        return ((logits + noise) / self.temperature).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSigmoid():\n",
    "    \"\"\"\n",
    "    adapted from: https://github.com/yandexdataschool/gumbel_lstm/blob/master/gumbel_sigmoid.py\n",
    "    and https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#gumbel_softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, t=0.1, eps=None):\n",
    "        self.temperature=t\n",
    "         \n",
    "    def __call__(self,logits):\n",
    "\n",
    "        \"\"\"computes a gumbel sigmoid sample\"\"\"\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / self.temperature\n",
    "        gumbels = gumbels.sigmoid()\n",
    "        return gumbels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random mask with 0.6 probability of being 1\n",
    "mask = (torch.randint(0, 100, (1, 10)) < 60)\n",
    "print(mask.)\n",
    "# pad\n",
    "mask = F.pad(mask, (0, 100 - 10), value=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class MyopicAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        max_keep_keys=50,\n",
    "        chunk_window=3,\n",
    "        bias=True,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.max_keep_keys = max_keep_keys\n",
    "        self.W = chunk_window\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def pad_to_window_size(self, x, window_size, axis=3, mask=None):\n",
    "        \"\"\"\n",
    "        Pad the input on two sides to be divisible by `window_size`\n",
    "        \"\"\"\n",
    "        QKV, batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "        padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "        padding = torch.zeros(QKV, batch_size, heads, padding_length, hidden_size,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        mask = F.pad(mask, (0, padding_length), value=True) \n",
    "        return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "    def unpad(self, x, padding_length):\n",
    "        \"\"\"\n",
    "        Undo padding.\n",
    "        \"\"\"\n",
    "        if padding_length > 0:\n",
    "            return x[:, :-padding_length]\n",
    "        return x\n",
    "\n",
    "    def ChunkGrid(self, Total_Size, Block_Size):\n",
    "        Psize = Total_Size // Block_Size\n",
    "        chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "        #chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1)) # don't normalize cus it'll stretch the distribution by sequence length\n",
    "        return chunk_grid    \n",
    "\n",
    "    def causal_windowed_mask(self, window_number, window_size, device):\n",
    "        '''\n",
    "        Create a block diagonal causal mask, to prevent selecting future tokens in the topk key selection\n",
    "        '''\n",
    "        return torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "\n",
    "    def standard_forward(self, qkv, mask):\n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        positions = self.positional_bias(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        dots += positions\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal:\n",
    "            # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, value)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, return_attention=False):\n",
    "        assert mask is not None, 'pls wear a mask'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        tokeep = min(self.max_keep_keys, N) if self.max_keep_keys != -1 else N # number of keys to keep\n",
    "        W = min(self.W, N) if self.W != -1 else N # window size\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "\n",
    "        qkv, pad_n, mask = self.pad_to_window_size(qkv, W, axis=3, mask=mask) # add padding so it's divisible by W\n",
    "        q, kv = qkv[0], qkv[1:] # separate q and kv, we keep kv together for now as we apply the same operations to both\n",
    "        \n",
    "        q = rearrange(q, \"b h (n w) d -> b h n w d\", w=W)# split q into windows/chunks of size W\n",
    "      \n",
    "        q_mask = repeat(rearrange(mask, \"b (n w) -> b n w\", w=W), \"b n w -> b h n w\", h=H) # do the same for the mask\n",
    "            \n",
    "        kv = repeat(kv, \"kv b h n d -> kv b h nw n d\", nw=q.shape[2]) # duplicate k and v for total number of windows\n",
    "        #print(q.shape, kv.shape)\n",
    "        KV, B, H, NW, N, D = kv.shape\n",
    "\n",
    "        chunkgrid = self.ChunkGrid(Total_Size=N, Block_Size=W).to(q.device)\n",
    "        chunkgrid = repeat(chunkgrid, \"w n -> b h w n\", b=B, h=H).contiguous()\n",
    "\n",
    "        SCALE = torch.tensor(3.0, device=q.device, dtype=q.dtype)\n",
    "        ALPHA = torch.tensor(2.0, device=q.device, dtype=q.dtype)\n",
    "        pareto_dist = torch.distributions.pareto.Pareto(SCALE, ALPHA).sample(chunkgrid.shape).to(q.device)\n",
    "        chunkgrid = chunkgrid - pareto_dist\n",
    "\n",
    "        chunkgrid = repeat(chunkgrid, \"b h w n -> kv b h w n\", kv=2)\n",
    "\n",
    "        cmask = repeat(mask, 'b n -> kv b h nw n', kv=2, h=H, nw=NW)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = self.causal_windowed_mask(window_number=NW, window_size=W, device=q.device)\n",
    "            cmask = torch.logical_or(cmask, causal_mask)\n",
    "        \n",
    "        chunkgrid = chunkgrid.masked_fill(cmask, torch.finfo(q.dtype).max) # max cus we topk in reverse order \n",
    "\n",
    "        keep_indices = chunkgrid.topk(k=tokeep, dim=-1, sorted=False, largest=False).indices.sort(dim=-1).values\n",
    "        KV, B, H, NW, N, D = kv.shape \n",
    "        kv = kv.gather(-2, repeat(keep_indices, \"kv b h w n -> kv b h w n d\", d=D))\n",
    "\n",
    "        kv_mask = repeat(mask, \"b n -> b h nw n\", h=H, nw=NW)\n",
    "     \n",
    "        kv_mask = kv_mask.gather(-1, keep_indices[0])\n",
    "\n",
    "        k, v = kv\n",
    "        # nw (number of windows) = p (in the einsum below)\n",
    "        dots = einsum(\"b h n p d, b h n z d -> b h n p z \", q, k) * self.scale # Z is number of chunks in Q, N is max sequence length after dropping\n",
    "       \n",
    "        ## positional stuff\n",
    "        pos_bias = self.positional_bias(N, device=dots.device, dtype=dots.dtype)\n",
    "        pos_bias = repeat(pos_bias, 'h i j -> b h i j', b = B)\n",
    "        pos_bias = rearrange(pos_bias, 'b h (n w) j -> b h n w j', w = W)\n",
    "\n",
    "        keep_indices = repeat(keep_indices, \"kv b h nw n -> kv b h nw w n\", w=W)[0] \n",
    "        pos_bias = pos_bias.gather(-1, keep_indices)\n",
    "        \n",
    "        dots = dots + pos_bias\n",
    "\n",
    "        mask_val = -torch.finfo(dots.dtype).max\n",
    "        \n",
    "        qk_mask = rearrange(q_mask, \"b h n w -> b h n w ()\") * rearrange(kv_mask, \"b h w n -> b h w () n\")\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = keep_indices > rearrange(torch.arange(0, N, device=q.device), \"(nw w) -> w nw ()\", w=NW, nw=W)\n",
    "            qk_mask = torch.logical_or(qk_mask, causal_mask)\n",
    "    \n",
    "        dots.masked_fill_(qk_mask, mask_val)\n",
    "      \n",
    "        #print(dots.shape)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "      \n",
    "\n",
    "        normal_attn = self.standard_forward(qkv=qkv, mask=mask)\n",
    "        normal_attn = rearrange(normal_attn, \"b h n d -> b n (h d)\")\n",
    "     \n",
    "\n",
    "        out = einsum(\"b h n w z, b h n z d -> b h n w d\", attn, v) \n",
    "\n",
    "        out = rearrange(out, \"b h n w d -> b (n w) (h d)\")\n",
    "   \n",
    "        \n",
    "        out = self.unpad(out, pad_n)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "     \n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class MyopicAttention3(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        max_keep_keys=50,\n",
    "        chunk_window=3,\n",
    "        bias=True,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.max_keep_keys = max_keep_keys\n",
    "        self.W = chunk_window\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.grid_pos_projection = nn.Linear(1, head_dim*n_heads)\n",
    "        print(self.grid_pos_projection.weight.shape)\n",
    "        self.grid_k_projection = nn.Linear(head_dim, head_dim)\n",
    "        grid_activation = kwargs.get(\"grid_activation\", 'relu')\n",
    "        assert grid_activation in ['relu', 'silu'], \"grid_activation must be relu or silu\"\n",
    "     \n",
    "        self.grid_activation = nn.SiLU() if grid_activation == 'silu' else nn.ReLU()\n",
    "        print(\"Using grid activation\", self.grid_activation)\n",
    "        self.grid_scaler_projection = nn.Linear(head_dim, 1)\n",
    "        self.gumbel_sigmoid = GumbelSigmoid(t=0.1, eps=1e-20)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def pad_to_window_size(self, x, window_size, axis=3, mask=None):\n",
    "        \"\"\"\n",
    "        Pad the input on two sides to be divisible by `window_size`\n",
    "        \"\"\"\n",
    "        QKV, batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "        padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "        padding = torch.zeros(QKV, batch_size, heads, padding_length, hidden_size,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        mask = F.pad(mask, (0, padding_length), value=True) \n",
    "        return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "    def unpad(self, x, padding_length):\n",
    "        \"\"\"\n",
    "        Undo padding.\n",
    "        \"\"\"\n",
    "        if padding_length > 0:\n",
    "            return x[:, :-padding_length]\n",
    "        return x\n",
    "\n",
    "    def valuegrid(self, total_size, block_size, k):\n",
    "        n = total_size // block_size\n",
    "        device, dtype = k.device, k.dtype\n",
    "        # get time\n",
    "        t = time.time()\n",
    "        indices = (rearrange(torch.arange(n, device = device), 'i -> i 1') - rearrange(torch.arange(n, device = device), 'j -> 1 j')) + (n - 1)\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = torch.float32)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "        pos = self.grid_pos_projection(pos)[indices]\n",
    "    \n",
    "        pos = pos.repeat_interleave(block_size, dim=1)\n",
    "        pos = rearrange(pos, \"p n (h d) -> () h p n d\", h = self.n_heads, d = self.head_dim)\n",
    "        print(\"pos\", time.time() - t)\n",
    "    \n",
    "        # get time\n",
    "        t = time.time()\n",
    "        k_voting = self.grid_k_projection(k)\n",
    "        print(\"k_voting k projection\", time.time() - t)\n",
    "        k_voting = k_voting + pos\n",
    "        t = time.time()\n",
    "        k_voting = self.grid_activation(k_voting)\n",
    "        print(\"k_voting activation\", time.time() - t)\n",
    "        t = time.time()\n",
    "        k_voting = self.grid_scaler_projection(k_voting).squeeze(-1)\n",
    "        print(\"k_voting scaler projection\", time.time() - t)\n",
    "        t = time.time()\n",
    "        k_voting = k_voting / k_voting.sum(dim=-1, keepdim=True)\n",
    "        print(\"normalization\", time.time() - t)\n",
    "        t = time.time()\n",
    "        k_voting = self.gumbel_sigmoid(k_voting)\n",
    "        print(\"gumbel sigmoid\", time.time() - t)\n",
    "        return k_voting    \n",
    "\n",
    "    def causal_windowed_mask(self, window_number, window_size, device):\n",
    "        '''Create a block diagonal causal mask, to prevent selecting future tokens in the topk key selection'''\n",
    "        return torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "\n",
    "    def standard_forward(self, qkv, mask):\n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        positions = self.positional_bias(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        dots += positions\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal:\n",
    "            # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, value)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, return_attention=False):\n",
    "        assert mask is not None, 'pls wear a mask'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        tokeep = min(self.max_keep_keys, N) if self.max_keep_keys != -1 else N # number of keys to keep\n",
    "        W = min(self.W, N) if self.W != -1 else N # window size\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "\n",
    "        qkv, pad_n, mask = self.pad_to_window_size(qkv, W, axis=3, mask=mask) # add padding so it's divisible by W\n",
    "        q, kv = qkv[0], qkv[1:] # separate q and kv, we keep kv together for now as we apply the same operations to both\n",
    "        \n",
    "        q = rearrange(q, \"b h (n w) d -> b h n w d\", w=W)# split q into windows/chunks of size W\n",
    "      \n",
    "        q_mask = repeat(rearrange(mask, \"b (n w) -> b n w\", w=W), \"b n w -> b h n w\", h=H) # do the same for the mask\n",
    "            \n",
    "        kv = repeat(kv, \"kv b h n d -> kv b h nw n d\", nw=q.shape[2]) # duplicate k and v for total number of windows\n",
    "        #print(q.shape, kv.shape)\n",
    "        KV, B, H, NW, N, D = kv.shape\n",
    "\n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        valuegrid = self.valuegrid(total_size=N, block_size=W, k=kv[0]).to(kv.device)\n",
    "        valuegrid = repeat(valuegrid, \"b h w n -> kv b h w n\", kv=2)\n",
    "        kv = kv * valuegrid.unsqueeze(-1) # maybe just do this for the keys? idk\n",
    "        print(f'value gridding done in: {time.time() - t:.2f}s')\n",
    "\n",
    "        cmask = repeat(mask, 'b n -> kv b h nw n', kv=2, h=H, nw=NW)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = self.causal_windowed_mask(window_number=NW, window_size=W, device=q.device)\n",
    "            cmask = torch.logical_or(cmask, causal_mask)\n",
    "        \n",
    "        valuegrid = valuegrid.masked_fill(cmask, -torch.finfo(q.dtype).max) \n",
    "\n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        keep_indices = valuegrid.topk(k=tokeep, dim=-1, sorted=False, largest=True).indices.sort(dim=-1).values\n",
    "        print(f'topk done in: {time.time() - t:.2f}s')\n",
    "        KV, B, H, NW, N, D = kv.shape \n",
    "       \n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        kv = kv.gather(-2, repeat(keep_indices, \"kv b h w n -> kv b h w n d\", d=D))\n",
    "        print(f'gather done in: {time.time() - t:.2f}s')\n",
    "\n",
    "        kv_mask = repeat(mask, \"b n -> b h nw n\", h=H, nw=NW)\n",
    "     \n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        kv_mask = kv_mask.gather(-1, keep_indices[0])\n",
    "        print(f'kv mask done in: {time.time() - t:.2f}s')\n",
    "\n",
    "        k, v = kv\n",
    "        # nw (number of windows) = p (in the einsum below)\n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        dots = einsum(\"b h n p d, b h n z d -> b h n p z \", q, k) * self.scale # Z is number of chunks in Q, N is max sequence length after dropping\n",
    "        print(f'dots done in: {time.time() - t:.2f}s')\n",
    "\n",
    "\n",
    "        ## positional stuff\n",
    "        # get current time\n",
    "        t = time.time()\n",
    "        pos_bias = self.positional_bias(N, device=dots.device, dtype=dots.dtype)\n",
    "        pos_bias = repeat(pos_bias, 'h i j -> b h i j', b = B)\n",
    "        pos_bias = rearrange(pos_bias, 'b h (n w) j -> b h n w j', w = W)\n",
    "\n",
    "        keep_indices = repeat(keep_indices, \"kv b h nw n -> kv b h nw w n\", w=W)[0] \n",
    "        pos_bias = pos_bias.gather(-1, keep_indices)\n",
    "        \n",
    "        dots = dots + pos_bias\n",
    "        print(f'pos bias done in: {time.time() - t:.2f}s')\n",
    "\n",
    "        mask_val = -torch.finfo(dots.dtype).max\n",
    "        \n",
    "        qk_mask = rearrange(q_mask, \"b h n w -> b h n w ()\") * rearrange(kv_mask, \"b h w n -> b h w () n\")\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = keep_indices > rearrange(torch.arange(0, N, device=q.device), \"(nw w) -> w nw ()\", w=NW, nw=W)\n",
    "            qk_mask = torch.logical_or(qk_mask, causal_mask)\n",
    "    \n",
    "        dots.masked_fill_(qk_mask, mask_val)\n",
    "      \n",
    "        #print(dots.shape)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "      \n",
    "\n",
    "        #normal_attn = self.standard_forward(qkv=qkv, mask=mask)\n",
    "        #normal_attn = rearrange(normal_attn, \"b h n d -> b n (h d)\")\n",
    "     \n",
    "\n",
    "        out = einsum(\"b h n w z, b h n z d -> b h n w d\", attn, v) \n",
    "\n",
    "        out = rearrange(out, \"b h n w d -> b (n w) (h d)\")\n",
    "   \n",
    "        \n",
    "        out = self.unpad(out, pad_n)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "     \n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, 10, (1, 1, max(50 // 4,1))).repeat_interleave(5, dim=-1).topk(k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDropoutAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        sequence_dropout=0.4,\n",
    "        bias=False,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.sequence_dropout = sequence_dropout\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "     \n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def standard_forward(self, qkv, mask, pos_fn): \n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        dots += pos_fn(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal: # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = dots.softmax(dim=-1)        \n",
    "        attn = self.dropout(attn)\n",
    "        return torch.einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "        \n",
    "\n",
    "    def forward(self, x, pos_fn, mask=None, return_attention=False, standard_attention=False):\n",
    "        assert pos_fn is not None, 'pls provide a position function'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, N, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "        if not standard_attention:\n",
    "            q, kv = qkv[0], qkv[1:] # separate q and kv, we keep kv together for now as we apply the same operations to both\n",
    "            # get a random sequence of zero or ones that is a quarter of the sequence length\n",
    "            print(N)\n",
    "            seq_mask = torch.randint(0, 10, (B, H, max(N // 4,1)), device=x.device, dtype=torch.int8) # int8 to save memory\n",
    "            seq_mask = seq_mask.repeat_interleave(5, dim=-1)[..., :N] # repeat the mask to be the same length as the sequence\n",
    "            if seq_mask.shape[-1] != N:\n",
    "                seq_mask = F.pad(seq_mask, (0, N - seq_mask.shape[-1]), value=4) # do this better\n",
    "            \n",
    "            k = int(N * (1 - self.sequence_dropout)) # get the number of tokens to keep\n",
    "            # mask out seq_mask to prioritize keeping non-masked values\n",
    "            print(seq_mask.shape, mask.shape)\n",
    "            seq_mask = seq_mask.masked_fill(mask.unsqueeze(1), -1)\n",
    "            keep_indices = torch.topk(seq_mask, k, dim=-1, largest=True, sorted=False).indices.sort(dim=-1).values # get the indices to keep\n",
    "            _, kv_B, kv_H, kv_N, kv_D = kv.shape\n",
    "            # get the keys and values to keep\n",
    "            kv = kv.gather(-2, repeat(keep_indices, \"b h n -> kv b h n d\", kv=2, d=D)) # get the keys and values to keep\n",
    "        \n",
    "            k_mask = repeat(mask, \"b n -> b h n\", h=H).gather(-1, keep_indices) # get the mask for the keys\n",
    "            k, v = kv # separate the keys and values\n",
    "\n",
    "            dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale # dot product between the queries and keys\n",
    "            \n",
    "            # positional stuff\n",
    "            pos_bias = pos_fn(N, device=dots.device, dtype=dots.dtype)\n",
    "            pos_bias = repeat(pos_bias, 'h i j -> b h i j', b = B)\n",
    "            keep_indices = repeat(keep_indices, \"b h n -> b h i n\", i=N)\n",
    "            pos_bias = pos_bias.gather(-1, keep_indices)\n",
    "            dots = dots + pos_bias\n",
    "        \n",
    "            qk_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(k_mask, \"b h n -> b h () n\")\n",
    "            \n",
    "            if self.causal:\n",
    "                causal_mask = keep_indices > rearrange(torch.arange(0, N, device=q.device), \"n -> n ()\", n=N)\n",
    "                qk_mask = torch.logical_or(qk_mask, causal_mask)\n",
    "            \n",
    "            dots.masked_fill_(qk_mask, -torch.finfo(dots.dtype).max)\n",
    "            attn = dots.softmax(dim=-1)\n",
    "            attn = self.dropout(attn)\n",
    "            out = einsum(\"bhij,bhjd->bhid\", attn, v)\n",
    "        else:\n",
    "            out = self.standard_forward(qkv, mask, pos_fn)\n",
    "\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, 10, (3, 12, max(6 // 4,1)), device=x.device, dtype=torch.int8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceMaskingAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        sequence_dropout=1.0,\n",
    "        bias=False,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.sequence_dropout = sequence_dropout\n",
    "        self.activation = torch.nn.Softmax(dim=-1) \n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def standard_forward(self, qkv, mask, pos_fn): \n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        dots += pos_fn(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal: # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = self.activation(dots)   \n",
    "        attn = self.dropout(attn)\n",
    "        return torch.einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "        \n",
    "\n",
    "    def forward(self, x, pos_fn, mask=None, return_attention=False, standard_attention=False):\n",
    "        assert pos_fn is not None, 'pls provide a position function'\n",
    "       \n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        standard_attention = standard_attention if self.training else True # we don't want to use dropout during inference\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, N, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "        if not standard_attention:\n",
    "            q, k, v = qkv\n",
    "            # get a random sequence of zero or ones that is a quarter of the sequence length\n",
    "            seq_mask = torch.randint(0, 100, (B, H, max(N // 4,1)), device=x.device, dtype=torch.int8) < (self.sequence_dropout * 100)\n",
    "            seq_mask = seq_mask.repeat_interleave(5, dim=-1)[..., :N] # repeat the mask to be the same length as the sequence\n",
    "            if seq_mask.shape[-1] != N:\n",
    "                diff = N - seq_mask.shape[-1]\n",
    "                pad_seq = torch.randint(0, 100, (B, H, diff), device=x.device, dtype=torch.int8) < (self.sequence_dropout * 100)\n",
    "                seq_mask = torch.cat((seq_mask, pad_seq), dim=-1)\n",
    "\n",
    "            dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale # dot product between the queries and keys\n",
    "            \n",
    "            # positional stuff\n",
    "            pos_bias = pos_fn(N, device=dots.device, dtype=dots.dtype)\n",
    "            dots = dots + pos_bias\n",
    "            \n",
    "            attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "            \n",
    "            if self.causal:\n",
    "                causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "                qk_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "            \n",
    "            dots.masked_fill_(qk_mask, -torch.finfo(dots.dtype).max)\n",
    "            attn = self.activation(dots)\n",
    "            # apply the sequence mask\n",
    "            attn = attn.masked_fill(rearrange(seq_mask, \"b h n -> b h () n\"), 0)\n",
    "\n",
    "            attn = self.dropout(attn)\n",
    "            out = einsum(\"bhij,bhjd->bhid\", attn, v)\n",
    "        else:\n",
    "            out = self.standard_forward(qkv, mask, pos_fn)\n",
    "\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([30, 12, 10, 10]) torch.Size([30, 12, 10])\n"
     ]
    }
   ],
   "source": [
    "positional_bias = DynamicPositionBias(\n",
    "    dim = 216,\n",
    "    heads = 12,\n",
    "    depth = 2,\n",
    "    log_distance = False,\n",
    "    norm = False\n",
    ")\n",
    "\n",
    "attention = SequenceMaskingAttention(n_feats=216, head_dim=24, n_heads=12, causal=True, sequence_dropout=1.0)\n",
    "N = 10\n",
    "B = 30\n",
    "x = torch.ones(B, N, 216) + torch.randn(B, N, 216) * 0.01\n",
    "mask = torch.zeros(B, N).bool()\n",
    "#mask[0, 0:10] = True\n",
    "#mask[2, 23:45] = True\n",
    "#attention.eval()\n",
    "attention.requires_grad_(False)\n",
    "print()\n",
    "a = attention(x, pos_fn=positional_bias, mask=mask, standard_attention=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(1,100):\n",
    "    print(i)\n",
    "    attention = SequenceDropoutAttention(n_feats=216, head_dim=24, n_heads=12, causal=True, sequence_dropout=0.5)\n",
    "    N = i\n",
    "    B = 3\n",
    "    x = torch.ones(B, N, 216) + torch.randn(B, N, 216) * 0.01\n",
    "    mask = torch.zeros(B, N).bool()\n",
    "    #mask[0, 0:10] = True\n",
    "    #mask[2, 23:45] = True\n",
    "    attention.eval()\n",
    "    attention.requires_grad_(False)\n",
    "    print()\n",
    "    attn= attention(x, pos_fn=attention.positional_bias, mask=mask, standard_attention=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn= attention(x, pos_fn=attention.positional_bias, mask=mask, standard_attention=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn1 = attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyopicAttention2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        max_keep_keys=50,\n",
    "        chunk_window=3,\n",
    "        bias=True,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.max_keep_keys = max_keep_keys\n",
    "        self.W = chunk_window\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.half_precision_mode = kwargs.get(\"half_precision_mode\", 'float16') # 'float16' or 'bfloat16' or float32\n",
    "\n",
    "        self.scale = nn.Parameter(torch.tensor(kwargs.get('scale', 3.0), requires_grad=True))\n",
    "        self.alpha = nn.Parameter(torch.tensor(kwargs.get('alpha', 2.0), requires_grad=True))\n",
    "        self.distance_multiplier = nn.Parameter(torch.tensor([kwargs.get('distance_multiplier_prior', 1.0)]*n_heads, requires_grad=True))\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def pad_to_window_size(self, x, window_size, axis=3, mask=None):\n",
    "        \"\"\"\n",
    "        Pad the input on two sides to be divisible by `window_size`\n",
    "        \"\"\"\n",
    "        QKV, batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "        padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "        padding = torch.zeros(QKV, batch_size, heads, padding_length, hidden_size,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        mask = F.pad(mask, (0, padding_length), value=True) \n",
    "        return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "    def unpad(self, x, padding_length):\n",
    "        \"\"\"\n",
    "        Undo padding.\n",
    "        \"\"\"\n",
    "        if padding_length > 0:\n",
    "            return x[:, :-padding_length]\n",
    "        return x\n",
    "\n",
    "    def ChunkGrid(self, Total_Size, Block_Size):\n",
    "        Psize = Total_Size // Block_Size\n",
    "        chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "        return chunk_grid #* self.distance_multiplier.to(chunk_grid.device)\n",
    "\n",
    "    def causal_windowed_mask(self, window_number, window_size, device):\n",
    "        '''\n",
    "        Create a block diagonal causal mask, to prevent selecting future tokens in the topk key selection\n",
    "        '''\n",
    "        return torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "\n",
    "\n",
    "    def half_precision_if_on_cuda(self, x, is_cuda):\n",
    "        if not is_cuda:\n",
    "            return x\n",
    "        elif self.half_precision_mode == 'float16':\n",
    "            return x.half()\n",
    "        elif self.half_precision_mode == 'bfloat16':\n",
    "            return x.bfloat16()\n",
    "        else:\n",
    "            return x\n",
    "      \n",
    "\n",
    "    def standard_forward(self, qkv, mask):\n",
    "        query, key, value = qkv\n",
    "        dots = torch.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        positions = self.positional_bias(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        dots += positions\n",
    "        attn_mask = rearrange(mask, \"b n -> b () n ()\") * rearrange(mask, \"b n -> b () () n\")\n",
    "    \n",
    "        if self.causal:\n",
    "            # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.einsum('bhij,bhjd->bhid', attn, value)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def forward(self, x, mask, return_attention=False):\n",
    "        assert mask is not None, 'pls wear a mask'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "\n",
    "        tokeep = min(self.max_keep_keys, N) if self.max_keep_keys != -1 else N # number of keys to keep\n",
    "        W = min(self.W, N) if self.W != -1 else N # window size\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "\n",
    "        qkv, pad_n, mask = self.pad_to_window_size(qkv, W, axis=3, mask=mask) # add padding so it's divisible by W\n",
    "        q, kv = qkv[0], qkv[1:] # separate q and kv, we keep kv together for now as we apply the same operations to both\n",
    "        \n",
    "        q = rearrange(q, \"b h (n w) d -> b h n w d\", w=W) # split q into windows/chunks of size W\n",
    "      \n",
    "        q_mask = repeat(rearrange(mask, \"b (n w) -> b n w\", w=W), \"b n w -> b h n w\", h=H) # do the same for the mask\n",
    "        kv = repeat(kv, \"kv b h n d -> kv b h nw n d\", nw=q.shape[2]) # duplicate k and v for total number of windows\n",
    "        \n",
    "        KV, B, H, NW, N, D = kv.shape\n",
    "\n",
    "        chunkgrid = self.ChunkGrid(Total_Size=N, Block_Size=W).to(q.device)\n",
    "        chunkgrid = self.half_precision_if_on_cuda(chunkgrid, q.is_cuda)\n",
    "        chunkgrid = repeat(chunkgrid, \"w n -> b h w n\", b=B, h=H).contiguous() \n",
    "        distance_multiplier = self.half_precision_if_on_cuda(self.distance_multiplier, q.is_cuda).to(q.device)\n",
    "        distance_multiplier = repeat(self.distance_multiplier, \"h -> b h w n\", b=B, w=NW, n=N)\n",
    "        chunkgrid = chunkgrid * distance_multiplier\n",
    "        \n",
    "\n",
    "        SCALE = self.scale.to(q.device).to(chunkgrid.dtype)\n",
    "        ALPHA = self.alpha.to(q.device).to(chunkgrid.dtype)\n",
    "        pareto_dist = torch.distributions.pareto.Pareto(SCALE, ALPHA).rsample(chunkgrid.shape).to(chunkgrid.device) # rsample so we can backprop\n",
    "        chunkgrid = chunkgrid - pareto_dist\n",
    "\n",
    "        chunkgrid = repeat(chunkgrid, \"b h w n -> kv b h w n\", kv=2)\n",
    "\n",
    "        cmask = repeat(mask, 'b n -> kv b h nw n', kv=2, h=H, nw=NW)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = self.causal_windowed_mask(window_number=NW, window_size=W, device=q.device)\n",
    "            cmask = torch.logical_or(cmask, causal_mask)\n",
    "        \n",
    "        chunkgrid = chunkgrid.masked_fill(cmask, torch.finfo(chunkgrid.dtype).max) # max cus we topk in reverse order \n",
    "\n",
    "        keep_indices = chunkgrid.topk(k=tokeep, dim=-1, sorted=False, largest=False)\n",
    "        '''\n",
    "         we want to take half of the keep indices (the ones with the largest vals) and apply a softmax to the values\n",
    "         then scatter the values with a multiply reduction to the k tensor\n",
    "         this allows the model to learn which keys to keep using the parametized pareto distribution\n",
    "         so kinda works like a relu (but not really)\n",
    "        '''\n",
    "        sorted_vals_with_indices = keep_indices.values.sort(-1)\n",
    "        num_to_scatter = tokeep // 4 # number of keys to scatter to kv\n",
    "        scatter_indices = sorted_vals_with_indices.indices[..., -num_to_scatter:].long() # indices of the keys to scatter\n",
    "        scatter_indices = repeat(scatter_indices[0], 'b h w n -> b h w n d', d=D)\n",
    "        scatter_vals = sorted_vals_with_indices.values[..., -num_to_scatter:] # values of the keys to scatter\n",
    "        scatter_vals = scatter_vals[0].softmax(-1) #* -1 + 1 # softmax but we want the smallest values to be the largest\n",
    "        scatter_vals = repeat(scatter_vals, 'b h w n -> b h w n d', d=D)\n",
    "        scatter_vals = scatter_vals * torch.randn(D, device=scatter_vals.device, dtype=scatter_vals.dtype) # add some noise to the values\n",
    "\n",
    "        print(scatter_vals.shape, 'scatter_vals')\n",
    "        print(kv.shape, 'kv')\n",
    "\n",
    "        kv = kv.contiguous() # we need kv to be contigous so we can scatter properly\n",
    "        kv[0] = scatter(\n",
    "            src = scatter_vals,\n",
    "            index = scatter_indices,\n",
    "            dim = -2,\n",
    "            out = kv[0].clone(),\n",
    "            reduce = 'mul'\n",
    "        )\n",
    "\n",
    "        ###\n",
    "        keep_indices = keep_indices.indices.sort(dim=-1).values\n",
    "        KV, B, H, NW, N, D = kv.shape \n",
    "        kv = kv.gather(-2, repeat(keep_indices, \"kv b h w n -> kv b h w n d\", d=D))\n",
    "\n",
    "        kv_mask = repeat(mask, \"b n -> b h nw n\", h=H, nw=NW)\n",
    "     \n",
    "        kv_mask = kv_mask.gather(-1, keep_indices[0])\n",
    "\n",
    "        k, v = kv\n",
    "        # nw (number of windows) = p (in the einsum below)\n",
    "        dots = einsum(\"b h n p d, b h n z d -> b h n p z \", q, k) * self.scale # Z is number of chunks in Q, N is max sequence length after dropping\n",
    "       \n",
    "        ## positional stuff\n",
    "        pos_bias = self.positional_bias(N, device=dots.device, dtype=dots.dtype)\n",
    "        pos_bias = repeat(pos_bias, 'h i j -> b h i j', b = B)\n",
    "        pos_bias = rearrange(pos_bias, 'b h (n w) j -> b h n w j', w = W)\n",
    "\n",
    "        keep_indices = repeat(keep_indices, \"kv b h nw n -> kv b h nw w n\", w=W)[0] \n",
    "        pos_bias = pos_bias.gather(-1, keep_indices)\n",
    "        \n",
    "        dots = dots + pos_bias\n",
    "\n",
    "        mask_val = -torch.finfo(dots.dtype).max\n",
    "        \n",
    "        qk_mask = rearrange(q_mask, \"b h n w -> b h n w ()\") * rearrange(kv_mask, \"b h w n -> b h w () n\")\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = keep_indices > rearrange(torch.arange(0, N, device=q.device), \"(nw w) -> w nw ()\", w=NW, nw=W)\n",
    "            qk_mask = torch.logical_or(qk_mask, causal_mask)\n",
    "    \n",
    "        dots.masked_fill_(qk_mask, mask_val)\n",
    "      \n",
    "        #print(dots.shape)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "      \n",
    "\n",
    "        normal_attn = self.standard_forward(qkv=qkv, mask=mask)\n",
    "        normal_attn = rearrange(normal_attn, \"b h n d -> b n (h d)\")\n",
    "     \n",
    "\n",
    "        out = einsum(\"b h n w z, b h n z d -> b h n w d\", attn, v) \n",
    "\n",
    "        out = rearrange(out, \"b h n w d -> b (n w) (h d)\")\n",
    "   \n",
    "        \n",
    "        out = self.unpad(out, pad_n)\n",
    "        \n",
    "        out = self.out_proj(out)\n",
    "     \n",
    "        return out if not return_attention else (out, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = SequenceDropoutAttention(n_feats=216, head_dim=24, n_heads=12, max_keep_keys=256, chunk_window=128, causal=True)\n",
    "\n",
    "x = torch.ones(10, 5000, 216) + torch.randn(10, 5000, 216) * 0.01\n",
    "mask = torch.zeros(10, 5000).bool()\n",
    "mask[0, 0:10] = True\n",
    "mask[2, 23:45] = True\n",
    "\n",
    "attn = attention(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter\n",
    "\n",
    "src = torch.arange(1, 11, requires_grad=True, dtype=torch.float32).reshape((2, 5))\n",
    "\n",
    "index = torch.tensor([[2,2,2,2,2],[1,1,1,1,1]])\n",
    "\n",
    "out = torch.ones(3, 5, dtype=src.dtype, requires_grad=True)\n",
    "# Broadcasting in the first and last dim.\n",
    "\n",
    "out2 = scatter(\n",
    "    src=src.clone().to(torch.float16),\n",
    "    index=index.clone().long(),\n",
    "    dim=0,\n",
    "    out=out.clone().to(torch.float16),\n",
    "    reduce='mul'\n",
    ")\n",
    "\n",
    "out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.arange(1, 11, requires_grad=True, dtype=torch.float32).reshape((2, 5))\n",
    "index = torch.tensor([[2,2,2,2,2],[1,1,1,1,1]])\n",
    "out = torch.ones(3, 5, dtype=src.dtype, requires_grad=True)\n",
    "out = out.scatter(0, index, src, reduce=\"multiply\")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1 = 0\n",
    "num2 = 100\n",
    "torch.allclose(attn[1][num1,num2][:99],attn[0][num1,num2][:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn[0][num1,num2][:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn[1][num1,num2][:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(attn[0], attn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([0,1,2,3,4,5,6]).topk(k=3, sorted=False, largest=False).indices.sort(dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 1000, 216)\n",
    "mask = torch.zeros(10, 1000).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD CHUNKGRID SHIT\n",
    "def ChunkGrid(self, Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1))\n",
    "    return chunk_grid    \n",
    "\n",
    "chunkgrid = repeat(chunkgrid, \"w n -> b h w n\", b=B, h=H).contiguous()\n",
    "MEAN = torch.tensor(0, device=q.device, dtype=q.dtype)\n",
    "STD = torch.tensor(0.125, device=q.device, dtype=q.dtype)\n",
    "uniform_dist = torch.distributions.normal.Normal(MEAN, STD).sample(chunkgrid.shape).to(q.device)\n",
    "chunkgrid += uniform_dist\n",
    "chunkgrid = repeat(chunkgrid, \"b h w n -> kv b h w n\", kv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_windowed_mask(window_number, window_size, device):\n",
    "    mask = torch.ones(window_number, window_number, device=device).triu(1).bool().repeat_interleave(window_size, dim=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_windowed_mask(3, 4, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.indices.sort().values[0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(2).unsqueeze(-1).repeat(2).expand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km[0, 0, :, 0, :100].sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.arange(0, 3008).repeat(3008,1) - torch.arange(0, 3008).repeat(3008,1).T).reshape(32, -1, 3008).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- duplicated across KV\n",
    "- each batch, head and Window have a different view of the keys\n",
    "- 94 is the number of windows i.e 94*32(win size) = 3008 (sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv[km].reshape(2, 5, 8, 3, -1, 24).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat(cg, \"W N -> KV B H W N\", B=5, H=8, KV=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(N_BLOCKS, BLOCK_SIZE):\n",
    "    chunk_grid = (torch.arange(0, N_BLOCKS).repeat(BLOCK_SIZE,1) - torch.arange(0, BLOCK_SIZE).repeat(N_BLOCKS,1).T).repeat_interleave(BLOCK_SIZE, dim=1).abs()\n",
    "    chunk_grid = chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1)\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ChunkGrid(41, 3)\n",
    "uniform_dist = torch.distributions.uniform.Uniform(0, 1).sample(cg.shape)\n",
    "cg += uniform_dist\n",
    "keep_indices = cg.topk(9, dim=-1).indices\n",
    "keep_mask = torch.zeros_like(cg).scatter_(1, keep_indices, 1).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
