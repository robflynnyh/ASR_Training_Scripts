{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def shift(t, amount, mask = None):\n",
    "    if amount == 0:\n",
    "        return t\n",
    "    else:\n",
    "        amount = min(amount, t.shape[1])\n",
    "\n",
    "    if exists(mask):\n",
    "        t = t.masked_fill(~mask[..., None], 0.)\n",
    "\n",
    "    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n",
    "\n",
    "class ShiftTokens(nn.Module):\n",
    "    def __init__(self, shifts, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.shifts = tuple(shifts)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        mask = kwargs.get('mask', None)\n",
    "        shifts = self.shifts\n",
    "        segments = len(shifts)\n",
    "        feats_per_shift = x.shape[-1] // segments\n",
    "        splitted = x.split(feats_per_shift, dim = -1)\n",
    "        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n",
    "        segments_to_shift = list(map(lambda args: shift(*args, mask = mask), zip(segments_to_shift, shifts)))\n",
    "        x = torch.cat((*segments_to_shift, *rest), dim = -1)\n",
    "        return self.fn(x, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 320, 320])\n",
      "torch.Size([10, 1, 320, 128])\n",
      "torch.Size([10, 8, 320, 128])\n"
     ]
    }
   ],
   "source": [
    "test = torch.randn(10, 16, 320, 320)\n",
    "l = nn.Conv2d(16, 8, (1,1))\n",
    "\n",
    "test = l(test)\n",
    "print(test.shape)\n",
    "v = torch.randn(10, 1, 320, 128)\n",
    "print(v.shape)\n",
    "out = torch.matmul(test, v)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 1, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[14.2882]],\n",
       "\n",
       "         [[25.2246]],\n",
       "\n",
       "         [[15.2835]],\n",
       "\n",
       "         [[ 9.7837]],\n",
       "\n",
       "         [[15.9891]],\n",
       "\n",
       "         [[ 4.2996]],\n",
       "\n",
       "         [[18.0781]],\n",
       "\n",
       "         [[14.7954]]]], requires_grad=True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.ones(1, 8, 1, 1) * 15.5 + torch.randn(1, 8, 1, 1) * 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 300, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_shift = ShiftTokens(range(0, 2), nn.Identity())\n",
    "inputs = torch.randn(10, 300, 768)\n",
    "tk_shift(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "# constant\n",
    "\n",
    "TOKEN_SELF_ATTN_VALUE = -5e4 # carefully set for half precision to work\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(value, d):\n",
    "    return d if not exists(value) else value\n",
    "\n",
    "def to(t):\n",
    "    return {'device': t.device, 'dtype': t.dtype}\n",
    "\n",
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max\n",
    "\n",
    "def merge_dims(ind_from, ind_to, tensor):\n",
    "    shape = list(tensor.shape)\n",
    "    arr_slice = slice(ind_from, ind_to + 1)\n",
    "    shape[arr_slice] = [reduce(mul, shape[arr_slice])]\n",
    "    return tensor.reshape(*shape)\n",
    "\n",
    "def expand_dim(t, dim, k, unsqueeze=True):\n",
    "    if unsqueeze:\n",
    "        t = t.unsqueeze(dim)\n",
    "    expand_shape = [-1] * len(t.shape)\n",
    "    expand_shape[dim] = k\n",
    "    return t.expand(*expand_shape)\n",
    "\n",
    "def pad_to_multiple(tensor, multiple, dim=-1, value=0):\n",
    "    seqlen = tensor.shape[dim]\n",
    "    m = seqlen / multiple\n",
    "    if m.is_integer():\n",
    "        return tensor\n",
    "    remainder = math.ceil(m) * multiple - seqlen\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "    return F.pad(tensor, (*pad_offset, 0, remainder), value=value)\n",
    "\n",
    "def look_around(x, backward = 1, forward = 0, pad_value = -1, dim = 2):\n",
    "    t = x.shape[1]\n",
    "    dims = (len(x.shape) - dim) * (0, 0)\n",
    "    padded_x = F.pad(x, (*dims, backward, forward), value= pad_value)\n",
    "    tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)]\n",
    "    return torch.cat(tensors, dim=dim)\n",
    "\n",
    "# main class\n",
    "\n",
    "class LocalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size,\n",
    "        causal = False,\n",
    "        look_backward = 1,\n",
    "        look_forward = None,\n",
    "        dropout = 0.,\n",
    "        shared_qk = False,\n",
    "        rel_pos_emb_config = None,\n",
    "        dim = None,\n",
    "        autopad = False,\n",
    "        exact_windowsize = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        look_forward = default(look_forward, 0 if causal else 1)\n",
    "        assert not (causal and look_forward > 0), 'you cannot look forward if causal'\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.causal = causal\n",
    "        self.look_backward = look_backward\n",
    "        self.look_forward = look_forward\n",
    "        self.exact_windowsize = exact_windowsize\n",
    "        self.autopad = autopad\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.shared_qk = shared_qk\n",
    "\n",
    "       \n",
    "\n",
    "    def forward(self, q, k, v, input_mask = None):\n",
    "        shape = q.shape\n",
    "\n",
    "        merge_into_batch = lambda t: t.reshape(-1, *t.shape[-2:])\n",
    "        q, k, v = map(merge_into_batch, (q, k, v))\n",
    "\n",
    "     \n",
    "\n",
    "        if self.autopad:\n",
    "            orig_t = q.shape[1]\n",
    "            q, k, v = map(lambda t: pad_to_multiple(t, self.window_size, dim = -2), (q, k, v))\n",
    "\n",
    "        window_size, causal, look_backward, look_forward, shared_qk = self.window_size, self.causal, self.look_backward, self.look_forward, self.shared_qk\n",
    "        b, t, e, device, dtype = *q.shape, q.device, q.dtype\n",
    "        assert (t % window_size) == 0, f'sequence length {t} must be divisible by window size {window_size} for local attention'\n",
    "\n",
    "        windows = t // window_size\n",
    "\n",
    "        if shared_qk:\n",
    "            k = F.normalize(k, 2, dim=-1).type_as(q)\n",
    "\n",
    "        ticker = torch.arange(t, device=device, dtype=torch.long)[None, :]\n",
    "        b_t = ticker.reshape(1, windows, window_size)\n",
    "\n",
    "        bucket_fn = lambda t: t.reshape(b, windows, window_size, -1)\n",
    "        bq, bk, bv = map(bucket_fn, (q, k, v))\n",
    "\n",
    "        look_around_kwargs = {'backward': look_backward, 'forward': look_forward}\n",
    "        bk = look_around(bk, **look_around_kwargs)\n",
    "        bv = look_around(bv, **look_around_kwargs)\n",
    "\n",
    "        bq_t = b_t\n",
    "        bq_k = look_around(b_t, **look_around_kwargs)\n",
    "\n",
    "        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (e ** -0.5)\n",
    "\n",
    "        mask_value = max_neg_value(dots)\n",
    "\n",
    "        if shared_qk:\n",
    "            mask = bq_t[:, :, :, None] == bq_k[:, :, None, :]\n",
    "            dots.masked_fill_(mask, TOKEN_SELF_ATTN_VALUE)\n",
    "            del mask\n",
    "\n",
    "        if causal:\n",
    "            mask = bq_t[:, :, :, None] < bq_k[:, :, None, :]\n",
    "\n",
    "            if self.exact_windowsize:\n",
    "                max_causal_window_size = (self.window_size * self.look_backward)\n",
    "                mask = mask | (bq_t[:, :, :, None] > (bq_k[:, :, None, :] + max_causal_window_size))\n",
    "\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        mask = bq_k[:, :, None, :] == -1\n",
    "        dots.masked_fill_(mask, mask_value)\n",
    "        del mask\n",
    "\n",
    "        if input_mask is not None:\n",
    "            h = b // input_mask.shape[0]\n",
    "            if self.autopad:\n",
    "                input_mask = pad_to_multiple(input_mask, window_size, dim=-1, value=False)\n",
    "            input_mask = input_mask.reshape(-1, windows, window_size)\n",
    "            mq = mk = input_mask\n",
    "            mk = look_around(mk, pad_value=False, **look_around_kwargs)\n",
    "            mask = (mq[:, :, :, None] * mk[:, :, None, :])\n",
    "            mask = merge_dims(0, 1, expand_dim(mask, 1, h))\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.einsum('bhij,bhje->bhie', attn, bv)\n",
    "        print(out.shape)\n",
    "        out = out.reshape(-1, t, e)\n",
    "\n",
    "        if self.autopad:\n",
    "            out = out[:, :orig_t, :]\n",
    "     \n",
    "        return out.reshape(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 8, 128, 20])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1024, 22]' is invalid for input of size 819200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [49], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      6\u001b[0m q,k,v \u001b[38;5;241m=\u001b[39m rearrange(inputs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m la(q,k,v)\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [37], line 163\u001b[0m, in \u001b[0;36mLocalAttention.forward\u001b[0;34m(self, q, k, v, input_mask)\u001b[0m\n\u001b[1;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbhij,bhje->bhie\u001b[39m\u001b[38;5;124m'\u001b[39m, attn, bv)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 163\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautopad:\n\u001b[1;32m    166\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[:, :orig_t, :]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1024, 22]' is invalid for input of size 819200"
     ]
    }
   ],
   "source": [
    "la = LocalAttention(\n",
    "    window_size=128,\n",
    "    causal=True,\n",
    ")\n",
    "inputs = torch.randn(10, 1024, 256)\n",
    "q,k,v = rearrange(inputs, 'b n (h d) -> b h n d', h = 4).chunk(3, dim=-1)\n",
    "la(q,k,v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out': tensor([[[ 0.2689,  0.6911, -0.5602,  ..., -0.3779,  0.6310, -0.2672],\n",
       "          [ 0.3529,  0.8222, -0.0128,  ..., -1.3357, -0.6433, -0.3723],\n",
       "          [ 0.8861, -0.3546, -0.8761,  ..., -0.0750,  0.4937,  0.2132],\n",
       "          ...,\n",
       "          [ 0.2952, -0.8775, -0.7833,  ...,  0.3043,  1.0915,  0.3812],\n",
       "          [ 0.1907,  0.2562,  0.1314,  ...,  0.8126, -0.2748,  0.4826],\n",
       "          [-0.2756, -1.3920,  1.3631,  ..., -0.5297,  0.3971,  1.0681]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'interim_logits': tensor([[[[-1.3953e-01, -8.2908e-02, -5.8428e-01,  ...,  5.3182e-02,\n",
       "             6.9809e-01, -2.3322e-01],\n",
       "           [-2.6622e-01,  2.8328e-01, -1.5494e-01,  ..., -1.2958e+00,\n",
       "            -3.5514e-01, -4.9902e-01],\n",
       "           [ 7.5474e-01, -7.4899e-01, -1.4107e+00,  ...,  3.1767e-01,\n",
       "             3.0542e-01,  5.9867e-03],\n",
       "           ...,\n",
       "           [ 5.9971e-01, -7.5500e-01, -1.1011e+00,  ...,  3.0627e-01,\n",
       "             4.2294e-01, -1.1166e-01],\n",
       "           [ 2.2694e-03, -5.0096e-01,  1.0677e-01,  ...,  8.7749e-01,\n",
       "            -3.6798e-01,  2.0913e-01],\n",
       "           [ 1.4183e-01, -1.1260e+00,  7.5588e-01,  ..., -3.9132e-01,\n",
       "            -4.5081e-01,  4.3311e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.8366e-01,  6.1571e-02, -5.5374e-01,  ...,  2.1513e-02,\n",
       "             3.1297e-01, -5.1690e-01],\n",
       "           [-3.1535e-01,  2.8901e-01, -2.4978e-01,  ..., -1.2512e+00,\n",
       "            -9.2539e-01, -6.1743e-01],\n",
       "           [ 8.1827e-01, -6.1525e-01, -1.3328e+00,  ...,  2.9819e-01,\n",
       "            -7.4963e-02, -9.7177e-02],\n",
       "           ...,\n",
       "           [ 6.9652e-01, -7.7627e-01, -1.0754e+00,  ...,  1.9608e-01,\n",
       "             4.4580e-01,  2.0199e-04],\n",
       "           [ 4.9833e-02, -4.0193e-01,  1.9350e-01,  ...,  6.1408e-01,\n",
       "            -3.6936e-01,  9.1230e-02],\n",
       "           [ 2.0513e-01, -9.8353e-01,  1.0054e+00,  ..., -5.7551e-01,\n",
       "            -4.8503e-01,  6.0011e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.2268e-01,  1.1584e-01, -6.6339e-01,  ..., -1.4406e-01,\n",
       "             2.5069e-03, -3.7983e-01],\n",
       "           [-3.5974e-02,  5.0196e-01, -3.3639e-01,  ..., -1.5601e+00,\n",
       "            -1.2668e+00, -6.3995e-01],\n",
       "           [ 9.7269e-01, -4.3690e-01, -1.4072e+00,  ..., -6.6880e-02,\n",
       "            -2.3438e-01, -7.9989e-02],\n",
       "           ...,\n",
       "           [ 4.7284e-01, -9.3140e-01, -1.2123e+00,  ...,  8.6272e-02,\n",
       "             5.3974e-01, -1.7638e-01],\n",
       "           [ 1.4485e-02, -4.0127e-01,  2.3891e-01,  ...,  6.4609e-01,\n",
       "            -4.6987e-01,  4.3260e-02],\n",
       "           [ 1.6916e-01, -1.2216e+00,  1.1761e+00,  ..., -5.3873e-01,\n",
       "            -2.2087e-01,  5.1129e-01]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 3.8350e-01,  6.4065e-01, -7.0600e-01,  ..., -2.8533e-01,\n",
       "             1.2270e-02, -5.4125e-01],\n",
       "           [ 3.2358e-01,  9.1336e-01,  9.0289e-02,  ..., -1.1975e+00,\n",
       "            -9.7946e-01, -5.3863e-01],\n",
       "           [ 1.0350e+00, -3.1435e-01, -7.2801e-01,  ...,  2.0918e-01,\n",
       "             4.1572e-02, -1.9996e-02],\n",
       "           ...,\n",
       "           [ 4.3817e-01, -6.9908e-01, -9.3737e-01,  ...,  1.9439e-01,\n",
       "             8.1802e-01,  4.1647e-01],\n",
       "           [ 1.3298e-01,  4.0167e-02,  1.9882e-01,  ...,  8.8215e-01,\n",
       "            -3.3069e-01,  3.0235e-01],\n",
       "           [-2.3525e-01, -1.2414e+00,  1.4129e+00,  ..., -6.0259e-01,\n",
       "             4.0462e-01,  7.7536e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.9341e-01,  7.8627e-01, -5.7680e-01,  ..., -3.3391e-01,\n",
       "             1.5309e-01, -3.0780e-01],\n",
       "           [ 2.1852e-01,  1.0797e+00,  5.3869e-02,  ..., -1.3903e+00,\n",
       "            -9.6086e-01, -3.1955e-01],\n",
       "           [ 8.7143e-01, -1.7300e-01, -8.8430e-01,  ...,  3.9397e-02,\n",
       "             1.1383e-01,  1.6110e-01],\n",
       "           ...,\n",
       "           [ 3.6983e-01, -8.1042e-01, -8.2743e-01,  ...,  3.3447e-01,\n",
       "             1.0064e+00,  4.5927e-01],\n",
       "           [ 2.7358e-02,  8.6087e-02,  5.7958e-02,  ...,  9.7611e-01,\n",
       "            -3.4267e-01,  4.4943e-01],\n",
       "           [-2.7360e-01, -1.1540e+00,  1.4292e+00,  ..., -5.2089e-01,\n",
       "             5.2641e-01,  9.5070e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.3165e-01,  7.7492e-01, -4.7472e-01,  ..., -3.6194e-01,\n",
       "             3.2013e-01, -2.7601e-01],\n",
       "           [ 2.1097e-01,  1.0441e+00,  1.2469e-01,  ..., -1.4869e+00,\n",
       "            -8.0663e-01, -4.0053e-01],\n",
       "           [ 7.8451e-01, -3.1806e-01, -8.3526e-01,  ..., -7.6524e-02,\n",
       "             3.5084e-01,  1.8824e-01],\n",
       "           ...,\n",
       "           [ 1.9440e-01, -9.3252e-01, -8.3636e-01,  ...,  3.1322e-01,\n",
       "             1.0053e+00,  1.7031e-01],\n",
       "           [ 3.7396e-02,  1.6332e-01,  1.8644e-01,  ...,  8.9982e-01,\n",
       "            -4.5223e-01,  3.5967e-01],\n",
       "           [-2.1580e-01, -1.2829e+00,  1.3776e+00,  ..., -5.2293e-01,\n",
       "             3.3633e-01,  9.5798e-01]]]], grad_fn=<StackBackward0>)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformer_lm(\n",
    "    dim=256,\n",
    "    vocab_size=29,\n",
    "    depth=12,\n",
    "    heads=8,\n",
    "    dim_head=32,\n",
    "    causal=True,\n",
    "    shared_kv=True\n",
    ")\n",
    "inputs = torch.randint(0, 29, (1, 512))\n",
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "from torch.utils.checkpoint import checkpoint # # gradient/activation checkpointing\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1,))\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device = device).type_as(self.inv_freq)\n",
    "        sinu = einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n",
    "        return emb * self.scale\n",
    "\n",
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(t, groups = 1, dim = -1):\n",
    "    if groups == 1:\n",
    "        return F.normalize(t, p = 2, dim = dim)\n",
    "    t = rearrange(t, '... (g d) -> ... g d', g = groups)\n",
    "    t = F.normalize(t, p = 2, dim = dim)\n",
    "    return rearrange(t, '... g d -> ... (g d)')\n",
    "\n",
    "class CosineAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def head_diversity(self, dots):\n",
    "\n",
    "        dots_cc = einsum('b h i j -> b i j h', dots) #  permute heads to last dim\n",
    "        dots_cc = rearrange(dots_cc, 'b i j h -> b i j h ()')\n",
    "        dots_cc = einsum('b i j h e, b i j o e -> b i j h o', dots_cc, dots_cc) #  compute pairwise dot products\n",
    "        # (T = seq_len)\n",
    "        dots_cc = dots_cc.sum(dim=1) / dots.shape[1]\n",
    "        print(dots_cc.shape)\n",
    "        dots_cc = rearrange(dots_cc, 'b i j h -> b h i j')\n",
    "        dots_identity = dots * torch.eye(dots.shape[-1], device=dots.device)\n",
    "        print(dots_identity.shape)\n",
    "        dots_dv = dots_cc - dots_identity\n",
    "        dots_dv = dots_dv.sum((2, 3)) / (dots.shape[2] * dots.shape[3])\n",
    "       \n",
    "        return dots_dv.mean((1,0))\n",
    "\n",
    "       \n",
    "\n",
    "    def attend(self, qkv, mask, pos_fn):\n",
    "        query, key, value = qkv\n",
    "        \n",
    "        query, key = map(l2norm, (query, key))\n",
    "\n",
    "        dots = einsum('bhid,bhjd->bhij', query, key) * self.temperature\n",
    "        self.head_diversity(dots)\n",
    "        dots += pos_fn(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "        qkmask = ~mask\n",
    "        attn_mask = ~(rearrange(qkmask, \"b n -> b () n ()\") * rearrange(qkmask, \"b n -> b () () n\"))\n",
    "    \n",
    "        if self.causal: # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = self.activation(dots)   \n",
    "        attn = self.dropout(attn)\n",
    "        return einsum(\"bhij,bhjd->bhid\", attn, value)\n",
    "\n",
    "\n",
    "    def forward(self, x, pos_fn, mask=None):\n",
    "        assert pos_fn is not None, 'pls provide a position function'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "        #print(x.shape, mask.shape)\n",
    "\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, N, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b n (h d qkv) -> qkv b h n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "    \n",
    "        out = self.attend(qkv, mask, pos_fn)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(self.norm(x), *args, **kwargs)\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim = -1)\n",
    "        return x * self.act(gate)\n",
    "\n",
    "\n",
    "\n",
    "class transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim, \n",
    "            depth, \n",
    "            heads, \n",
    "            dim_head, \n",
    "            causal=True,\n",
    "            temperature=15.5,\n",
    "            shared_temperture=False,\n",
    "            intermediate_loss=True,\n",
    "            dropout = 0.1,\n",
    "            checkpoint = True,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            intermediate_loss = False\n",
    "\n",
    "        ff_mult = kwargs.get('ff_mult', 4)\n",
    "\n",
    "     \n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature), requires_grad=True) if shared_temperture else temperature\n",
    "\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.depth = depth\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = dim // 4,\n",
    "            heads = heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "        self.grad_checkpointing = checkpoint\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, CosineAttention(\n",
    "                    dim, \n",
    "                    n_heads=heads, \n",
    "                    head_dim=dim_head, \n",
    "                    causal=causal,\n",
    "                    temperature=self.temperature,\n",
    "                    dropout=dropout,\n",
    "                    **kwargs\n",
    "                )),\n",
    "                PreNorm(dim, self.ff(dim, mult=ff_mult))\n",
    "            ]))\n",
    "\n",
    "    @staticmethod\n",
    "    def ff(dim, mult=4, dropout=0.1):\n",
    "        return nn.Sequential(\n",
    "            GLU(dim, dim * mult, nn.SiLU()),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_custom_forward(module):\n",
    "        def custom_forward(*args, **kwargs):\n",
    "            return module(*args, **kwargs)\n",
    "        return custom_forward\n",
    "\n",
    "    def checkpoint(self, layer, module, *args, **kwargs):\n",
    "        condition = self.training and self.grad_checkpointing and layer < self.depth - 1\n",
    "        return checkpoint(self.create_custom_forward(module), *args, **kwargs) if condition else module(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None, self_condtioning=None):\n",
    "        intermediate_logits = []\n",
    "        for i, (attn, ff) in enumerate(self.layers):\n",
    "            x = self.checkpoint(i, attn, x, self.positional_bias, mask) + x\n",
    "            x = self.checkpoint(i, ff, x) + x\n",
    "\n",
    "            if i < self.depth - 1 and self_condtioning is not None:\n",
    "                x, logits = self_condtioning(x)\n",
    "                intermediate_logits.append(logits)\n",
    "\n",
    "        # stack intermediate logits\n",
    "        if len(intermediate_logits) > 0:\n",
    "            intermediate_logits = torch.stack(intermediate_logits, dim=0) # D x B x N x V\n",
    "\n",
    "        return x, intermediate_logits\n",
    "\n",
    "class shared_embedding_output_layer(nn.Module):\n",
    "    '''\n",
    "    Pass a embedding layer and then use this module as the output layer\n",
    "    '''\n",
    "    def __init__(self, embedding_layer, bias=False):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.use_bias = bias\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(embedding_layer.weight.shape[0]))#\n",
    "            nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, weight=self.embedding_layer.weight, bias=self.bias if self.use_bias else None)\n",
    "\n",
    "\n",
    "class transformer_lm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        causal=True,\n",
    "        temperature=15.5,\n",
    "        dropout=0.,\n",
    "        shared_temperture=True,\n",
    "        self_conditioning=False,\n",
    "        intermediate_loss=True,\n",
    "        use_abs_pos=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self_conditioning == False\n",
    "\n",
    "        self.self_conditioning = True if self_conditioning else None\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.use_abs_pos = use_abs_pos\n",
    "        if self.use_abs_pos:\n",
    "            self.abs_pos_fn = ScaledSinuEmbedding(dim=dim)\n",
    "        self.abs_pos = lambda x: x + self.abs_pos_fn(x) if self.use_abs_pos else x\n",
    "\n",
    "        if self_conditioning:\n",
    "            self.reprojection_layer = nn.Linear(vocab_size, dim)\n",
    "\n",
    "        self.layers = transformer(\n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = heads, \n",
    "            dim_head = dim_head, \n",
    "            causal = causal, \n",
    "            dropout = dropout,\n",
    "            temperature = temperature,\n",
    "            shared_temperture = shared_temperture,\n",
    "            intermediate_loss = intermediate_loss,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.tie_embedding = kwargs.get('tie_embedding', False)\n",
    "        print('Tie embedding:', self.tie_embedding) if self.tie_embedding else None\n",
    " \n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.to_logits = shared_embedding_output_layer(self.embedding) if self.tie_embedding else nn.Linear(dim, vocab_size)\n",
    "        \n",
    "\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "\n",
    "    def self_condition_fn(self):\n",
    "        def self_condition(x):\n",
    "            logits = self.to_logits(self.post_norm(x))\n",
    "            if self.self_conditioning:\n",
    "                z = F.softmax(logits, dim=-1)\n",
    "                z = self.reprojection_layer(z)\n",
    "                x = z + x\n",
    "            return x, logits\n",
    "        return self_condition if (self.self_conditioning or self.intermediate_loss) and self.training else None\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.abs_pos(x)\n",
    "        x, interim_logits = self.layers(x, mask=~mask if mask is not None else None, self_condtioning=self.self_condition_fn())\n",
    "        x = self.post_norm(x)\n",
    "        x = self.to_logits(x)\n",
    "\n",
    "        return  { 'out': x, 'interim_logits': interim_logits } if self.training else x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 190, 8, 8])\n",
      "torch.Size([30, 8, 190, 190])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (190) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [434], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m transformer_lm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, dim_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m model(x, mask)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [433], line 362\u001b[0m, in \u001b[0;36mtransformer_lm.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    360\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m    361\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabs_pos(x)\n\u001b[0;32m--> 362\u001b[0m x, interim_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_condtioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_condition_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_norm(x)\n\u001b[1;32m    364\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_logits(x)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [433], line 264\u001b[0m, in \u001b[0;36mtransformer.forward\u001b[0;34m(self, x, mask, self_condtioning)\u001b[0m\n\u001b[1;32m    262\u001b[0m intermediate_logits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (attn, ff) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 264\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    265\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint(i, ff, x) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m self_condtioning \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [433], line 258\u001b[0m, in \u001b[0;36mtransformer.checkpoint\u001b[0;34m(self, layer, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer, module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    257\u001b[0m     condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_checkpointing \u001b[38;5;129;01mand\u001b[39;00m layer \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_custom_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;28;01melse\u001b[39;00m module(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/utils/checkpoint.py:235\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    238\u001b[0m         function,\n\u001b[1;32m    239\u001b[0m         preserve,\n\u001b[1;32m    240\u001b[0m         \u001b[39m*\u001b[39margs\n\u001b[1;32m    241\u001b[0m     )\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/utils/checkpoint.py:96\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     93\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 96\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[0;32mIn [433], line 253\u001b[0m, in \u001b[0;36mtransformer.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_forward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [433], line 177\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [433], line 164\u001b[0m, in \u001b[0;36mCosineAttention.forward\u001b[0;34m(self, x, pos_fn, mask)\u001b[0m\n\u001b[1;32m    160\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(B, N, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m    162\u001b[0m qkv \u001b[38;5;241m=\u001b[39m rearrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv_proj(x), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n (h d qkv) -> qkv b h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, qkv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39mH, d\u001b[38;5;241m=\u001b[39mD) \u001b[38;5;66;03m# qkv projection\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb h n d -> b n (h d)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(out)\n",
      "Cell \u001b[0;32mIn [433], line 138\u001b[0m, in \u001b[0;36mCosineAttention.attend\u001b[0;34m(self, qkv, mask, pos_fn)\u001b[0m\n\u001b[1;32m    135\u001b[0m query, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(l2norm, (query, key))\n\u001b[1;32m    137\u001b[0m dots \u001b[38;5;241m=\u001b[39m einsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbhid,bhjd->bhij\u001b[39m\u001b[38;5;124m'\u001b[39m, query, key) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_diversity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdots\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m dots \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pos_fn(dots\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mdots\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdots\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    140\u001b[0m qkmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mmask\n",
      "Cell \u001b[0;32mIn [433], line 125\u001b[0m, in \u001b[0;36mCosineAttention.head_diversity\u001b[0;34m(self, dots)\u001b[0m\n\u001b[1;32m    123\u001b[0m dots_identity \u001b[38;5;241m=\u001b[39m dots \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(dots\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mdots\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(dots_identity\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 125\u001b[0m dots_dv \u001b[38;5;241m=\u001b[39m \u001b[43mdots_cc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdots_identity\u001b[49m\n\u001b[1;32m    126\u001b[0m dots_dv \u001b[38;5;241m=\u001b[39m dots_dv\u001b[38;5;241m.\u001b[39msum((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m/\u001b[39m (dots\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m dots\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dots_dv\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (190) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "model = transformer_lm(dim=256, vocab_size=128, depth=6, heads=8, dim_head=32, causal=True)\n",
    "model(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 190\n",
    "B = 30\n",
    "x = torch.randint(0, 128, (B, N))\n",
    "mask = torch.ones(B, N).bool()\n",
    "mask[:, 50:] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "from torch.utils.checkpoint import checkpoint # # gradient/activation checkpointing\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim, \n",
    "            depth, \n",
    "            heads, \n",
    "            dim_head, \n",
    "            causal=True,\n",
    "            temperature=15.5,\n",
    "            shared_temperture=False,\n",
    "            intermediate_loss=True,\n",
    "            dropout = 0.1,\n",
    "            checkpoint = True,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            intermediate_loss = False\n",
    "\n",
    "        ff_mult = kwargs.get('ff_mult', 4)\n",
    "     \n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature), requires_grad=True) if shared_temperture else temperature\n",
    "\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.depth = depth\n",
    " \n",
    "        self.grad_checkpointing = checkpoint\n",
    "        self.MLP_fnet = PreNorm(dim, MLPAttenion(\n",
    "            dim, \n",
    "            n_heads=heads, \n",
    "            head_dim=dim_head, \n",
    "            causal=causal,\n",
    "            temperature=self.temperature,\n",
    "            dropout=dropout,\n",
    "            **kwargs\n",
    "        ))\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                self.MLP_fnet,\n",
    "                PreNorm(dim, self.ff(dim, mult=ff_mult))\n",
    "            ]))\n",
    "\n",
    "    @staticmethod\n",
    "    def ff(dim, mult=4, dropout=0.1):\n",
    "        return nn.Sequential(\n",
    "            GLU(dim, dim * mult, nn.SiLU()),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_custom_forward(module):\n",
    "        def custom_forward(*args, **kwargs):\n",
    "            return module(*args, **kwargs)\n",
    "        return custom_forward\n",
    "\n",
    "    def checkpoint(self, layer, module, *args, **kwargs):\n",
    "        condition = self.training and self.grad_checkpointing and layer < self.depth - 1\n",
    "        return checkpoint(self.create_custom_forward(module), *args, **kwargs) if condition else module(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None, self_condtioning=None):\n",
    "        intermediate_logits = []\n",
    "        for i, (attn, ff) in enumerate(self.layers):\n",
    "            x = self.checkpoint(i, attn, x, mask) + x\n",
    "            x = self.checkpoint(i, ff, x) + x\n",
    "\n",
    "            if i < self.depth - 1 and self_condtioning is not None:\n",
    "                x, logits = self_condtioning(x)\n",
    "                intermediate_logits.append(logits)\n",
    "\n",
    "        # stack intermediate logits\n",
    "        if len(intermediate_logits) > 0:\n",
    "            intermediate_logits = torch.stack(intermediate_logits, dim=0) # D x B x N x V\n",
    "   \n",
    "        return x, intermediate_logits\n",
    "\n",
    "\n",
    "class transformer_lm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        causal=True,\n",
    "        temperature=15.5,\n",
    "        dropout=0.,\n",
    "        shared_temperture=True,\n",
    "        self_conditioning=False,\n",
    "        intermediate_loss=True,\n",
    "        use_abs_pos=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self_conditioning == False\n",
    "\n",
    "        self.self_conditioning = True if self_conditioning else None\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.use_abs_pos = use_abs_pos\n",
    "        if self.use_abs_pos:\n",
    "            self.abs_pos_fn = ScaledSinuEmbedding(dim=dim)\n",
    "        self.abs_pos = lambda x: x + self.abs_pos_fn(x) if self.use_abs_pos else x\n",
    "\n",
    "        if self_conditioning:\n",
    "            self.reprojection_layer = nn.Linear(vocab_size, dim)\n",
    "\n",
    "        self.layers = transformer(\n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = heads, \n",
    "            dim_head = dim_head, \n",
    "            causal = causal, \n",
    "            dropout = dropout,\n",
    "            temperature = temperature,\n",
    "            shared_temperture = shared_temperture,\n",
    "            intermediate_loss = intermediate_loss,\n",
    "            **kwargs\n",
    "        )\n",
    " \n",
    "        self.to_logits = nn.Linear(dim, vocab_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def self_condition_fn(self):\n",
    "        def self_condition(x):\n",
    "            logits = self.to_logits(self.post_norm(x))\n",
    "            if self.self_conditioning:\n",
    "                z = F.softmax(logits, dim=-1)\n",
    "                z = self.reprojection_layer(z)\n",
    "                x = z + x\n",
    "            return x, logits\n",
    "        return self_condition if (self.self_conditioning or self.intermediate_loss) and self.training else None\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.abs_pos(x)\n",
    "        x, interim_logits = self.layers(x, mask=~mask if mask is not None else None, self_condtioning=self.self_condition_fn())\n",
    "        x = self.post_norm(x)\n",
    "        x = self.to_logits(x)\n",
    "\n",
    "        return  { 'out': x, 'interim_logits': interim_logits } if self.training else x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unpad(x, padding_length):\n",
    "    \"\"\"\n",
    "    Undo padding.\n",
    "    \"\"\"\n",
    "    if padding_length > 0:\n",
    "        return x[:, :-padding_length]\n",
    "    return x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(self.norm(x), *args, **kwargs)\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim = -1)\n",
    "        return x * self.act(gate)\n",
    "\n",
    "class MLPAttenion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.in_proj = nn.Linear(n_feats, n_heads * head_dim)\n",
    "\n",
    "        window_sizes = [4, 8, 16]\n",
    "        self.RS_layers = nn.ModuleList(\n",
    "            [recurrent_shift(dim_head=head_dim, window_size=ws, n_heads=n_heads, dropout=dropout) for ws in window_sizes]\n",
    "        )\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "       \n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, N, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        x = self.in_proj(x)\n",
    "        x = rearrange(x, 'b n (h d) -> b h n d', h=H)       \n",
    "    \n",
    "        #mask = rearrange(mask, 'b n -> b () n ()')\n",
    "        for rs in self.RS_layers:\n",
    "            x = rs(x, mask=mask)\n",
    "\n",
    "        out = rearrange(x, 'b h n d -> b n (h d)')\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "      \n",
    "       \n",
    "        return out\n",
    "\n",
    "def pad_to_window_size(x, window_size, axis=3, mask=None):\n",
    "    \"\"\"\n",
    "    Pad the input on two sides to be divisible by `window_size`\n",
    "    \"\"\"\n",
    "    batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "    if sequence_length % window_size == 0:\n",
    "        return x, 0, mask\n",
    "    padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "    padding = torch.zeros(batch_size, heads, padding_length, hidden_size,\n",
    "        device=x.device,\n",
    "        dtype=x.dtype,\n",
    "    )\n",
    "    mask = F.pad(mask, (0, padding_length), value=True) if mask is not None else None\n",
    "    return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "class OffsetScale(nn.Module):\n",
    "    def __init__(self, dim, heads = 8):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, heads, 1, 1, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, heads, 1, 1, dim))\n",
    "        nn.init.normal_(self.gamma, std = 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.gamma + self.beta\n",
    "\n",
    "class FNetBlock(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class recurrent_shift(nn.Module):\n",
    "    def __init__(self, dim_head, n_heads, window_size, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.WINDOW_SIZE = window_size\n",
    "     \n",
    "\n",
    "        self.project_out = nn.Linear(self.dim * 2, self.dim, bias=bias)\n",
    "\n",
    "        self.fnet = FNetBlock()\n",
    "        self.zero_vector = torch.zeros([1, n_heads, 1, 1, self.dim])\n",
    "\n",
    "        self.v_offset = OffsetScale(dim_head, n_heads)\n",
    "\n",
    " \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        WINDOW_SIZE = self.WINDOW_SIZE\n",
    "      \n",
    "        x, pad_n, new_mask = pad_to_window_size(x, window_size=WINDOW_SIZE, axis=-2, mask=mask) \n",
    "\n",
    "       \n",
    "        x = rearrange(x, 'b h (w n) d -> b h w n d', w=x.shape[-2]// WINDOW_SIZE, n=WINDOW_SIZE) # group into windows\n",
    "        v = self.v_offset(x)\n",
    "        v_mask = rearrange(new_mask, 'b (w n) -> b 1 w n', w=new_mask.shape[-1]// WINDOW_SIZE, n=WINDOW_SIZE) # group into windows\n",
    "        v = self.fnet(v)\n",
    "        v.masked_fill_(v_mask.unsqueeze(-1), 0)\n",
    "     \n",
    "       \n",
    "        zero_vector = self.zero_vector.to(v.device).expand(v.shape[0], -1, -1, v.shape[-2], -1)\n",
    "    \n",
    "        v = torch.cat([zero_vector, v], dim=-3)[:,:,:-1] # shift by one\n",
    "      \n",
    "        x = torch.cat([x, v], dim=-1)\n",
    "       \n",
    "        x = self.project_out(x)\n",
    "        x = rearrange(x, 'b h w n d -> b h (w n) d')\n",
    "        \n",
    "        if pad_n > 0:\n",
    "            x = x[:,:,:-pad_n]\n",
    "\n",
    "        print(x.shape, mask.shape)\n",
    "        x.masked_fill_(rearrange(mask, 'b n -> b () n ()'), 0)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n",
      "torch.Size([30, 8, 190, 32]) torch.Size([30, 190])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'out': tensor([[[-0.2982,  0.1873, -1.9930,  ..., -0.3897, -0.3533, -1.2466],\n",
       "          [-0.6645,  0.1250, -1.3112,  ...,  0.6729,  0.1355, -0.7009],\n",
       "          [-0.9118, -0.2027, -1.1339,  ...,  0.5862, -0.2717, -1.0989],\n",
       "          ...,\n",
       "          [-0.4473, -0.8250, -0.5216,  ...,  0.4069,  0.3177, -1.0214],\n",
       "          [-0.7444,  1.0751,  0.0451,  ...,  0.0867, -0.3953, -0.5858],\n",
       "          [-0.4822, -0.2591, -0.3168,  ...,  0.1166,  0.7044, -1.1715]],\n",
       " \n",
       "         [[-0.2937,  0.5568, -0.9290,  ...,  0.4349, -0.1473, -1.1261],\n",
       "          [-0.1165, -0.8903, -0.3944,  ...,  0.8364, -0.7816, -0.3695],\n",
       "          [-0.5482,  0.4587,  0.3291,  ...,  0.0801, -0.9207, -1.1145],\n",
       "          ...,\n",
       "          [-0.4250, -0.6316, -0.2543,  ..., -0.5047,  0.5773, -0.4286],\n",
       "          [-0.1323, -0.0953, -0.7231,  ..., -0.1950,  0.0868, -0.6367],\n",
       "          [-0.2676,  0.5273, -0.8669,  ...,  0.0447,  0.1551, -0.7647]],\n",
       " \n",
       "         [[-0.6735, -0.1389, -0.9193,  ..., -0.0415, -0.2693,  0.5203],\n",
       "          [-1.0036, -0.3880, -0.8646,  ..., -0.2053,  0.3962, -0.9355],\n",
       "          [-0.0530,  0.3239, -0.5064,  ...,  0.5914, -0.5789, -0.3462],\n",
       "          ...,\n",
       "          [-0.3876, -0.0501,  0.8354,  ..., -0.3287,  0.3281,  0.2239],\n",
       "          [-0.1473, -0.3230, -0.6555,  ...,  0.7056, -0.1086, -0.4137],\n",
       "          [ 0.5615,  0.3543, -0.2935,  ..., -0.2027, -0.0047,  0.2167]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.9543, -0.1596, -1.1697,  ...,  0.0761,  0.2154, -1.2401],\n",
       "          [ 0.8352, -0.4705, -0.4167,  ...,  1.0328, -0.1492, -0.4941],\n",
       "          [-1.1726, -0.3384, -0.9925,  ...,  0.6565,  0.7490, -0.3902],\n",
       "          ...,\n",
       "          [ 0.0271, -0.6256, -0.1734,  ...,  0.0527,  0.5166, -1.4817],\n",
       "          [-0.4440,  0.2519, -0.1936,  ..., -0.0461, -0.2663, -0.3489],\n",
       "          [-0.1316,  0.2964,  0.7401,  ..., -0.1024, -0.6615, -0.9207]],\n",
       " \n",
       "         [[-0.7083,  0.1790, -0.4786,  ..., -0.2373,  0.2860, -0.1623],\n",
       "          [-1.2434, -0.1098, -1.4140,  ..., -0.5216,  0.1150, -1.2900],\n",
       "          [ 0.2104, -0.5500, -0.3292,  ...,  0.7513, -0.4971, -0.6394],\n",
       "          ...,\n",
       "          [-0.4953, -0.7761, -0.5114,  ...,  0.5492,  0.3759, -0.9611],\n",
       "          [ 0.2666,  0.0033, -0.9362,  ..., -0.8986,  0.2557, -0.1245],\n",
       "          [ 0.2510, -0.8090, -0.4233,  ..., -0.2793, -0.1571, -0.6478]],\n",
       " \n",
       "         [[-1.1934, -0.5480, -1.6084,  ...,  0.2279,  0.0288, -0.5536],\n",
       "          [-0.6321, -0.3623, -1.9788,  ...,  0.3054,  0.3950, -0.5737],\n",
       "          [-0.3481,  0.5698, -1.1335,  ...,  0.2624,  0.0368, -1.0709],\n",
       "          ...,\n",
       "          [-0.4156,  0.3418, -0.8365,  ..., -0.5464,  0.6375, -0.6800],\n",
       "          [ 0.0684, -0.1806, -0.5471,  ..., -0.3703,  0.6636, -0.6711],\n",
       "          [-0.1058, -1.0047, -0.9136,  ..., -0.4916, -0.0577, -0.7838]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'interim_logits': tensor([[[[ 5.4988e-01,  7.7017e-01, -7.0823e-01,  ..., -7.1984e-01,\n",
       "            -5.6780e-01, -3.5267e-01],\n",
       "           [ 1.8338e-01,  4.4599e-02, -4.7695e-01,  ...,  4.2834e-01,\n",
       "            -3.2189e-01, -1.4705e-01],\n",
       "           [-5.3890e-01, -2.5101e-01, -8.9060e-01,  ...,  3.3886e-01,\n",
       "             8.3693e-02, -4.8633e-01],\n",
       "           ...,\n",
       "           [ 3.4434e-02, -4.5274e-01, -4.1859e-01,  ...,  8.6218e-02,\n",
       "             8.0540e-02, -7.0617e-01],\n",
       "           [-4.4337e-01,  8.4986e-01,  6.9329e-01,  ...,  2.1155e-01,\n",
       "            -5.1656e-01, -4.6099e-01],\n",
       "           [-8.1620e-01, -2.2594e-01,  1.8985e-01,  ...,  1.0361e-01,\n",
       "             9.2366e-01, -5.2283e-01]],\n",
       " \n",
       "          [[-9.4153e-02,  9.6670e-01, -3.7419e-01,  ...,  9.0877e-03,\n",
       "            -1.2456e-01, -5.4398e-01],\n",
       "           [ 6.2033e-01, -9.2885e-01,  4.5428e-01,  ...,  5.5533e-01,\n",
       "            -8.6889e-01,  2.5532e-01],\n",
       "           [-5.4218e-03,  6.9397e-01,  1.2197e+00,  ..., -2.0506e-01,\n",
       "            -8.8598e-01, -3.3694e-01],\n",
       "           ...,\n",
       "           [ 6.4354e-02, -1.0578e-01,  3.0346e-02,  ..., -6.9125e-01,\n",
       "             7.6312e-01,  1.9256e-01],\n",
       "           [-2.8197e-01,  2.2048e-02, -4.4977e-01,  ..., -5.3044e-02,\n",
       "            -3.3135e-02, -4.7174e-01],\n",
       "           [-1.2964e-01,  9.7972e-01, -3.4000e-01,  ..., -5.5609e-02,\n",
       "            -4.1572e-02, -5.4634e-01]],\n",
       " \n",
       "          [[-2.3311e-02,  2.1926e-02, -4.2350e-02,  ..., -2.0173e-01,\n",
       "            -1.4706e-01,  7.9385e-01],\n",
       "           [ 1.0470e-01, -7.0941e-02, -2.9136e-02,  ..., -6.8543e-01,\n",
       "             7.2172e-01,  1.6872e-01],\n",
       "           [ 5.4797e-01,  4.1545e-01,  3.8102e-01,  ...,  1.3716e-01,\n",
       "            -8.3249e-01,  3.6950e-01],\n",
       "           ...,\n",
       "           [-3.8343e-01, -2.6055e-01,  1.0014e+00,  ..., -5.5970e-01,\n",
       "             4.7290e-01,  7.8214e-01],\n",
       "           [ 3.0397e-01,  4.9567e-02, -4.3291e-01,  ...,  4.0319e-01,\n",
       "            -3.1699e-01, -1.1751e-01],\n",
       "           [ 6.7500e-01,  6.0657e-01, -8.9958e-02,  ..., -1.7315e-01,\n",
       "             1.6585e-01,  5.5727e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.6702e-01, -3.5383e-01, -4.9897e-01,  ..., -2.4685e-01,\n",
       "             2.4048e-01, -1.7078e-01],\n",
       "           [ 1.1642e+00, -1.0848e-01,  2.7423e-01,  ...,  7.5412e-01,\n",
       "            -3.0491e-02,  3.5385e-01],\n",
       "           [-6.3171e-01, -2.7393e-02, -5.7122e-01,  ...,  2.2913e-01,\n",
       "             1.0478e+00,  3.0516e-01],\n",
       "           ...,\n",
       "           [ 1.9458e-01, -5.2809e-02,  2.1949e-01,  ..., -1.5626e-01,\n",
       "             4.1849e-01, -8.4112e-01],\n",
       "           [-4.3272e-01,  4.3859e-01,  3.2192e-01,  ..., -3.3239e-01,\n",
       "            -3.2664e-01, -1.2410e-01],\n",
       "           [ 2.2743e-02,  6.9825e-01,  1.2824e+00,  ..., -2.3786e-01,\n",
       "            -8.1678e-01, -3.2517e-01]],\n",
       " \n",
       "          [[-1.6856e-01,  1.4195e-01,  1.0526e-01,  ..., -3.0695e-01,\n",
       "             1.1513e-01,  5.0102e-01],\n",
       "           [-1.9133e-01, -1.1264e-01, -6.3269e-01,  ..., -1.1192e+00,\n",
       "             4.6722e-01, -6.5373e-01],\n",
       "           [ 5.9969e-01, -5.1313e-01,  3.3381e-01,  ...,  3.4075e-01,\n",
       "            -3.7703e-01, -3.4686e-01],\n",
       "           ...,\n",
       "           [ 2.5379e-04, -4.2610e-01, -3.8689e-01,  ...,  1.5536e-01,\n",
       "             4.7451e-02, -6.9987e-01],\n",
       "           [ 4.4382e-01,  3.2414e-01, -6.3600e-01,  ..., -8.3759e-01,\n",
       "             3.0376e-01,  1.8620e-01],\n",
       "           [-3.1958e-02, -3.9315e-01, -2.6410e-01,  ..., -8.2731e-01,\n",
       "             1.4739e-01, -1.3481e-01]],\n",
       " \n",
       "          [[-7.8678e-01, -6.8727e-01, -1.0742e+00,  ...,  4.3796e-01,\n",
       "            -9.8850e-02, -2.9082e-01],\n",
       "           [-6.3558e-02, -3.2708e-01, -1.5087e+00,  ..., -1.7857e-01,\n",
       "             5.7312e-01, -2.5875e-01],\n",
       "           [-1.1228e-01,  9.3855e-01, -3.9078e-01,  ..., -3.6116e-02,\n",
       "            -5.6232e-02, -5.5909e-01],\n",
       "           ...,\n",
       "           [ 1.0765e-03,  4.0393e-01, -2.9050e-01,  ..., -5.0387e-01,\n",
       "             5.7726e-01, -3.2874e-01],\n",
       "           [ 1.9885e-01, -1.7892e-02, -2.9272e-01,  ..., -5.6840e-01,\n",
       "             3.3325e-01, -2.3606e-01],\n",
       "           [ 2.6919e-01, -9.1359e-01, -4.5821e-01,  ..., -1.0410e+00,\n",
       "             9.1971e-04, -4.0648e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 4.2521e-01,  6.2950e-01, -8.6951e-01,  ..., -7.6742e-01,\n",
       "            -5.6191e-01, -4.7366e-01],\n",
       "           [ 1.0406e-01,  1.4089e-02, -6.2093e-01,  ...,  4.8351e-01,\n",
       "            -2.6844e-01, -2.6891e-01],\n",
       "           [-7.3431e-01, -2.0191e-01, -9.0789e-01,  ...,  4.2778e-01,\n",
       "             1.1304e-01, -5.1588e-01],\n",
       "           ...,\n",
       "           [ 3.0296e-02, -5.1850e-01, -4.9362e-01,  ...,  1.8742e-01,\n",
       "             5.2836e-02, -7.9507e-01],\n",
       "           [-4.6178e-01,  9.0234e-01,  5.9356e-01,  ...,  2.2071e-01,\n",
       "            -5.4249e-01, -5.3195e-01],\n",
       "           [-7.5637e-01, -3.1129e-01,  2.3921e-01,  ...,  1.6480e-01,\n",
       "             8.8024e-01, -5.9762e-01]],\n",
       " \n",
       "          [[-2.2954e-01,  1.0136e+00, -4.4648e-01,  ...,  1.8065e-02,\n",
       "            -6.9642e-02, -6.0898e-01],\n",
       "           [ 5.2353e-01, -1.0166e+00,  4.6269e-01,  ...,  6.0956e-01,\n",
       "            -8.2286e-01,  1.9664e-01],\n",
       "           [-8.1761e-02,  6.4931e-01,  1.2081e+00,  ..., -2.0957e-01,\n",
       "            -9.0465e-01, -4.3339e-01],\n",
       "           ...,\n",
       "           [ 3.4303e-02, -2.6983e-01, -3.8534e-02,  ..., -8.2005e-01,\n",
       "             7.1386e-01,  7.3412e-02],\n",
       "           [-2.4715e-01, -5.3999e-02, -5.0715e-01,  ..., -8.3327e-02,\n",
       "            -2.7366e-02, -4.9245e-01],\n",
       "           [-2.7225e-01,  1.0411e+00, -4.0853e-01,  ..., -7.3384e-02,\n",
       "            -1.5948e-02, -5.7041e-01]],\n",
       " \n",
       "          [[-1.7752e-01,  2.8724e-02, -1.3396e-01,  ..., -1.2679e-01,\n",
       "            -2.3132e-01,  7.7404e-01],\n",
       "           [-2.8884e-02, -2.0983e-01, -1.6119e-01,  ..., -8.2173e-01,\n",
       "             6.0317e-01, -3.4444e-02],\n",
       "           [ 4.7738e-01,  3.3881e-01,  3.2882e-01,  ...,  1.3340e-01,\n",
       "            -6.9814e-01,  2.3487e-01],\n",
       "           ...,\n",
       "           [-4.3382e-01, -2.6082e-01,  9.3807e-01,  ..., -4.9630e-01,\n",
       "             5.3586e-01,  7.2928e-01],\n",
       "           [ 2.9124e-01, -1.2615e-02, -4.8813e-01,  ...,  3.9065e-01,\n",
       "            -2.6522e-01, -1.9860e-01],\n",
       "           [ 7.0769e-01,  6.3651e-01,  2.4578e-02,  ..., -1.4673e-01,\n",
       "             5.0408e-02,  4.3114e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.0003e-01, -2.8410e-01, -5.1901e-01,  ..., -2.4429e-01,\n",
       "             1.7481e-01, -2.8950e-01],\n",
       "           [ 1.0749e+00, -1.6516e-01,  2.0521e-01,  ...,  7.7844e-01,\n",
       "             5.9942e-02,  3.0267e-01],\n",
       "           [-7.6743e-01, -1.9392e-02, -4.7309e-01,  ...,  3.6816e-01,\n",
       "             9.9429e-01,  2.7613e-01],\n",
       "           ...,\n",
       "           [ 1.5060e-01, -1.3112e-01,  1.6896e-01,  ..., -1.5506e-01,\n",
       "             4.7591e-01, -7.9354e-01],\n",
       "           [-4.6537e-01,  5.0829e-01,  3.1483e-01,  ..., -4.0013e-01,\n",
       "            -3.4131e-01, -1.7602e-01],\n",
       "           [ 4.0106e-02,  6.5578e-01,  1.3277e+00,  ..., -2.5883e-01,\n",
       "            -8.2657e-01, -3.5313e-01]],\n",
       " \n",
       "          [[-2.3790e-01,  1.0969e-01, -1.2691e-02,  ..., -2.7628e-01,\n",
       "             1.8244e-01,  4.5901e-01],\n",
       "           [-2.9911e-01, -5.9063e-02, -7.0411e-01,  ..., -1.0145e+00,\n",
       "             4.5066e-01, -7.2681e-01],\n",
       "           [ 6.2068e-01, -5.2624e-01,  2.6484e-01,  ...,  3.5485e-01,\n",
       "            -5.1176e-01, -2.9135e-01],\n",
       "           ...,\n",
       "           [ 6.4724e-04, -4.3234e-01, -4.0662e-01,  ...,  2.4608e-01,\n",
       "             5.6911e-02, -7.1799e-01],\n",
       "           [ 4.5342e-01,  4.3814e-01, -6.1303e-01,  ..., -7.8931e-01,\n",
       "             2.3265e-01,  1.9674e-01],\n",
       "           [-2.4364e-02, -4.7409e-01, -2.5943e-01,  ..., -7.5201e-01,\n",
       "            -2.1869e-03, -2.1608e-01]],\n",
       " \n",
       "          [[-7.8379e-01, -7.1788e-01, -1.0900e+00,  ...,  4.7834e-01,\n",
       "            -1.6833e-01, -2.2526e-01],\n",
       "           [-1.3513e-01, -3.3287e-01, -1.5262e+00,  ..., -1.3760e-01,\n",
       "             5.7440e-01, -3.1685e-01],\n",
       "           [-2.4392e-01,  1.0171e+00, -4.9551e-01,  ..., -6.5928e-02,\n",
       "            -4.4806e-02, -6.2298e-01],\n",
       "           ...,\n",
       "           [-4.0735e-02,  5.5088e-01, -3.7383e-01,  ..., -4.2402e-01,\n",
       "             5.5984e-01, -3.5938e-01],\n",
       "           [ 4.1769e-02, -9.5614e-02, -3.2776e-01,  ..., -5.3613e-01,\n",
       "             3.6415e-01, -3.4817e-01],\n",
       "           [ 2.1899e-01, -8.8732e-01, -5.3554e-01,  ..., -9.2001e-01,\n",
       "            -1.0302e-01, -4.7181e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 3.6675e-01,  6.5739e-01, -1.0140e+00,  ..., -7.8798e-01,\n",
       "            -5.0475e-01, -4.8652e-01],\n",
       "           [ 1.2026e-02,  5.9489e-02, -6.9695e-01,  ...,  5.3471e-01,\n",
       "            -3.1687e-01, -3.5045e-01],\n",
       "           [-7.3632e-01, -2.1159e-01, -9.4789e-01,  ...,  4.3331e-01,\n",
       "             1.1868e-01, -4.1183e-01],\n",
       "           ...,\n",
       "           [-1.3813e-02, -6.0880e-01, -4.5602e-01,  ...,  2.2430e-01,\n",
       "             1.6831e-01, -8.5924e-01],\n",
       "           [-4.6219e-01,  8.6038e-01,  5.8550e-01,  ...,  2.6269e-01,\n",
       "            -5.5630e-01, -5.5075e-01],\n",
       "           [-7.6301e-01, -3.8855e-01,  8.3417e-02,  ...,  1.3206e-01,\n",
       "             8.9570e-01, -6.5552e-01]],\n",
       " \n",
       "          [[-2.2512e-01,  8.9905e-01, -5.5768e-01,  ...,  1.1364e-01,\n",
       "            -1.1579e-01, -6.3265e-01],\n",
       "           [ 3.6987e-01, -1.0895e+00,  3.5951e-01,  ...,  6.3994e-01,\n",
       "            -8.3650e-01,  1.0992e-01],\n",
       "           [-6.3343e-02,  5.8574e-01,  1.1475e+00,  ..., -1.8239e-01,\n",
       "            -9.7141e-01, -5.1076e-01],\n",
       "           ...,\n",
       "           [-3.0267e-02, -2.8388e-01, -1.3862e-01,  ..., -9.2955e-01,\n",
       "             7.2712e-01,  1.5963e-02],\n",
       "           [-1.8079e-01, -1.2406e-01, -5.8507e-01,  ..., -6.0500e-02,\n",
       "             4.0909e-02, -5.5736e-01],\n",
       "           [-2.1892e-01,  9.2669e-01, -5.3570e-01,  ..., -5.2688e-02,\n",
       "            -1.4045e-02, -5.9538e-01]],\n",
       " \n",
       "          [[-2.1874e-01, -8.0151e-02, -1.7882e-01,  ..., -1.3966e-01,\n",
       "            -3.1893e-01,  7.6299e-01],\n",
       "           [-1.0956e-01, -2.5084e-01, -3.1781e-01,  ..., -9.0300e-01,\n",
       "             6.1056e-01, -1.3180e-01],\n",
       "           [ 3.4037e-01,  3.8680e-01,  1.6813e-01,  ...,  2.3669e-01,\n",
       "            -7.0794e-01,  1.5914e-01],\n",
       "           ...,\n",
       "           [-4.6822e-01, -2.1195e-01,  1.0197e+00,  ..., -5.3244e-01,\n",
       "             4.8482e-01,  5.6500e-01],\n",
       "           [ 3.0663e-01,  3.4366e-02, -5.3298e-01,  ...,  4.4093e-01,\n",
       "            -3.1119e-01, -1.3427e-01],\n",
       "           [ 6.4508e-01,  7.0819e-01,  3.1322e-02,  ..., -1.9023e-01,\n",
       "             1.1384e-01,  4.7324e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.0626e-01, -2.7358e-01, -7.0359e-01,  ..., -1.7718e-01,\n",
       "             1.4261e-01, -3.9706e-01],\n",
       "           [ 1.0812e+00, -2.2817e-01,  1.6298e-01,  ...,  8.7410e-01,\n",
       "             1.5357e-02,  2.2003e-01],\n",
       "           [-8.9457e-01,  6.9072e-03, -5.9387e-01,  ...,  4.6944e-01,\n",
       "             1.0656e+00,  2.9816e-01],\n",
       "           ...,\n",
       "           [ 2.2237e-01, -1.8075e-01,  1.2494e-01,  ..., -1.8700e-01,\n",
       "             4.6576e-01, -9.5221e-01],\n",
       "           [-5.7383e-01,  4.4899e-01,  3.3544e-01,  ..., -3.0152e-01,\n",
       "            -3.4604e-01, -1.6572e-01],\n",
       "           [ 1.1977e-01,  6.3491e-01,  1.3610e+00,  ..., -2.7163e-01,\n",
       "            -8.3549e-01, -3.9884e-01]],\n",
       " \n",
       "          [[-3.5454e-01,  1.5353e-01, -2.6456e-02,  ..., -2.7731e-01,\n",
       "             1.8976e-01,  2.8062e-01],\n",
       "           [-3.0790e-01, -7.5645e-02, -7.3805e-01,  ..., -9.9904e-01,\n",
       "             4.8125e-01, -8.0517e-01],\n",
       "           [ 6.1637e-01, -5.5962e-01,  2.1799e-01,  ...,  4.6699e-01,\n",
       "            -6.0418e-01, -3.3438e-01],\n",
       "           ...,\n",
       "           [-6.9572e-02, -5.1797e-01, -4.0350e-01,  ...,  3.1297e-01,\n",
       "             1.3970e-01, -7.4362e-01],\n",
       "           [ 4.5442e-01,  4.1255e-01, -6.8967e-01,  ..., -8.2326e-01,\n",
       "             3.3327e-01,  1.7787e-01],\n",
       "           [-4.8805e-02, -4.7437e-01, -2.8128e-01,  ..., -7.1320e-01,\n",
       "            -1.5721e-01, -2.5146e-01]],\n",
       " \n",
       "          [[-7.9999e-01, -7.5151e-01, -1.2083e+00,  ...,  3.6834e-01,\n",
       "            -2.2771e-01, -2.7622e-01],\n",
       "           [-1.4484e-01, -4.0724e-01, -1.5829e+00,  ..., -9.5660e-02,\n",
       "             6.4414e-01, -3.8371e-01],\n",
       "           [-2.5932e-01,  9.0183e-01, -6.5759e-01,  ..., -1.1605e-02,\n",
       "            -7.8105e-02, -6.4593e-01],\n",
       "           ...,\n",
       "           [-1.9839e-01,  5.0817e-01, -5.6606e-01,  ..., -3.6772e-01,\n",
       "             6.2963e-01, -4.6220e-01],\n",
       "           [ 1.2559e-01, -6.6282e-02, -2.9693e-01,  ..., -6.4075e-01,\n",
       "             4.0947e-01, -3.1869e-01],\n",
       "           [ 2.0717e-01, -8.8006e-01, -5.2635e-01,  ..., -9.2101e-01,\n",
       "            -1.6404e-01, -4.8263e-01]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.0088e-01,  3.5340e-01, -1.7288e+00,  ..., -4.2418e-01,\n",
       "            -3.7285e-01, -9.5960e-01],\n",
       "           [-4.8795e-01,  8.5224e-03, -9.4431e-01,  ...,  6.5246e-01,\n",
       "             9.3873e-02, -6.4179e-01],\n",
       "           [-9.0154e-01, -2.2283e-01, -1.0251e+00,  ...,  4.6557e-01,\n",
       "            -1.5124e-01, -6.5801e-01],\n",
       "           ...,\n",
       "           [-3.6682e-01, -8.8198e-01, -5.4887e-01,  ...,  3.7455e-01,\n",
       "             2.3882e-01, -1.0668e+00],\n",
       "           [-5.3507e-01,  9.7612e-01,  2.3704e-01,  ..., -2.8719e-02,\n",
       "            -4.4795e-01, -5.0830e-01],\n",
       "           [-4.7024e-01, -2.9281e-01, -7.2293e-02,  ...,  5.4765e-02,\n",
       "             8.6487e-01, -9.4349e-01]],\n",
       " \n",
       "          [[-2.7587e-01,  5.8710e-01, -7.2023e-01,  ...,  3.3738e-01,\n",
       "            -1.9260e-01, -1.0022e+00],\n",
       "           [ 3.1460e-02, -1.0441e+00, -3.0330e-01,  ...,  7.1470e-01,\n",
       "            -7.6671e-01, -3.0653e-01],\n",
       "           [-3.7344e-01,  3.8323e-01,  5.9604e-01,  ...,  1.0404e-01,\n",
       "            -9.6647e-01, -8.7633e-01],\n",
       "           ...,\n",
       "           [-3.4869e-01, -5.1705e-01, -2.2425e-01,  ..., -5.3283e-01,\n",
       "             6.8093e-01, -2.1371e-01],\n",
       "           [ 9.5063e-03, -6.6674e-02, -6.4686e-01,  ..., -2.0037e-01,\n",
       "             1.5069e-01, -7.1234e-01],\n",
       "           [-1.6131e-01,  5.2627e-01, -6.9250e-01,  ...,  6.7670e-03,\n",
       "             1.0482e-02, -7.4847e-01]],\n",
       " \n",
       "          [[-5.1583e-01, -1.1061e-01, -7.3078e-01,  ...,  3.1778e-02,\n",
       "            -3.2341e-01,  6.9426e-01],\n",
       "           [-7.9026e-01, -3.3187e-01, -6.8322e-01,  ..., -3.3207e-01,\n",
       "             5.7903e-01, -6.3336e-01],\n",
       "           [-9.7933e-03,  2.4862e-01, -3.7397e-01,  ...,  5.7455e-01,\n",
       "            -5.7348e-01, -2.0305e-01],\n",
       "           ...,\n",
       "           [-5.4230e-01, -9.1263e-02,  8.2190e-01,  ..., -3.4423e-01,\n",
       "             4.6240e-01,  5.0837e-01],\n",
       "           [ 1.4892e-01, -3.7115e-01, -4.0191e-01,  ...,  5.1897e-01,\n",
       "            -6.6756e-02, -3.0224e-01],\n",
       "           [ 6.5472e-01,  4.8424e-01, -2.3863e-01,  ..., -2.7848e-01,\n",
       "             2.2569e-01,  3.3667e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-8.1349e-01, -3.2421e-01, -1.0867e+00,  ..., -5.4861e-02,\n",
       "             1.9088e-01, -9.7748e-01],\n",
       "           [ 9.7138e-01, -4.0596e-01, -2.2134e-01,  ...,  8.1674e-01,\n",
       "            -1.1603e-01, -1.4526e-01],\n",
       "           [-1.1152e+00, -3.6415e-01, -8.0772e-01,  ...,  7.8241e-01,\n",
       "             9.0261e-01, -2.1288e-01],\n",
       "           ...,\n",
       "           [ 1.7885e-01, -5.6283e-01, -2.1483e-01,  ..., -1.3040e-01,\n",
       "             5.4773e-01, -1.3069e+00],\n",
       "           [-3.0108e-01,  2.2530e-01, -3.1347e-02,  ..., -1.2533e-01,\n",
       "            -3.2186e-01, -2.1229e-01],\n",
       "           [ 1.0060e-01,  3.2687e-01,  1.0011e+00,  ...,  1.5734e-02,\n",
       "            -6.9763e-01, -7.1134e-01]],\n",
       " \n",
       "          [[-5.4644e-01,  8.8806e-03, -3.1270e-01,  ..., -1.6051e-01,\n",
       "             4.3529e-01, -5.1619e-02],\n",
       "           [-9.6440e-01, -1.5338e-01, -1.2370e+00,  ..., -5.4260e-01,\n",
       "             8.8203e-02, -1.1361e+00],\n",
       "           [ 3.8420e-01, -6.0704e-01, -1.3714e-01,  ...,  6.4124e-01,\n",
       "            -5.2659e-01, -6.1454e-01],\n",
       "           ...,\n",
       "           [-4.5207e-01, -7.2241e-01, -4.7768e-01,  ...,  4.8994e-01,\n",
       "             3.2625e-01, -9.9716e-01],\n",
       "           [ 2.5301e-01,  9.2732e-02, -1.0157e+00,  ..., -9.2364e-01,\n",
       "             2.7328e-01, -1.3646e-01],\n",
       "           [ 2.3352e-01, -6.5327e-01, -3.6441e-01,  ..., -2.7578e-01,\n",
       "            -1.1403e-01, -5.5834e-01]],\n",
       " \n",
       "          [[-1.0989e+00, -5.6116e-01, -1.4898e+00,  ...,  1.9917e-01,\n",
       "            -3.2980e-02, -4.6978e-01],\n",
       "           [-4.5026e-01, -2.6928e-01, -1.8448e+00,  ...,  8.9865e-02,\n",
       "             4.9458e-01, -5.4637e-01],\n",
       "           [-3.6457e-01,  5.8760e-01, -9.3513e-01,  ...,  1.6395e-01,\n",
       "            -6.2485e-02, -9.9217e-01],\n",
       "           ...,\n",
       "           [-2.6930e-01,  3.5633e-01, -7.1690e-01,  ..., -4.6294e-01,\n",
       "             7.2096e-01, -5.0659e-01],\n",
       "           [ 1.0348e-01, -7.0270e-02, -4.7354e-01,  ..., -5.8573e-01,\n",
       "             6.4810e-01, -6.4576e-01],\n",
       "           [ 1.0660e-01, -9.2155e-01, -8.6229e-01,  ..., -6.1839e-01,\n",
       "            -3.3455e-03, -5.7513e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.2409e-01,  3.1483e-01, -1.7818e+00,  ..., -3.5973e-01,\n",
       "            -3.7621e-01, -1.0746e+00],\n",
       "           [-6.0840e-01,  3.3790e-02, -1.0615e+00,  ...,  6.8141e-01,\n",
       "             1.5411e-01, -6.6838e-01],\n",
       "           [-9.2453e-01, -1.5393e-01, -9.9693e-01,  ...,  4.7833e-01,\n",
       "            -2.2522e-01, -8.7360e-01],\n",
       "           ...,\n",
       "           [-4.5976e-01, -8.6170e-01, -5.3225e-01,  ...,  3.9508e-01,\n",
       "             3.0889e-01, -9.5745e-01],\n",
       "           [-6.3027e-01,  9.9945e-01,  1.0671e-01,  ...,  2.1087e-02,\n",
       "            -4.1583e-01, -5.6060e-01],\n",
       "           [-4.8144e-01, -2.0431e-01, -1.8997e-01,  ...,  4.3724e-02,\n",
       "             7.6714e-01, -1.0063e+00]],\n",
       " \n",
       "          [[-3.3507e-01,  5.4332e-01, -8.6027e-01,  ...,  3.0686e-01,\n",
       "            -1.5233e-01, -1.0938e+00],\n",
       "           [-8.1867e-02, -9.3235e-01, -3.0382e-01,  ...,  7.7719e-01,\n",
       "            -6.9077e-01, -3.0056e-01],\n",
       "           [-4.0158e-01,  3.6020e-01,  5.4682e-01,  ...,  1.2362e-01,\n",
       "            -9.8266e-01, -9.7316e-01],\n",
       "           ...,\n",
       "           [-3.5363e-01, -5.1033e-01, -1.6897e-01,  ..., -5.6623e-01,\n",
       "             6.8879e-01, -2.9737e-01],\n",
       "           [-5.4761e-02, -6.1593e-02, -6.7525e-01,  ..., -9.5074e-02,\n",
       "             1.0503e-01, -8.0401e-01],\n",
       "           [-2.2206e-01,  4.7272e-01, -8.5537e-01,  ..., -3.6234e-02,\n",
       "             1.2088e-01, -8.2771e-01]],\n",
       " \n",
       "          [[-5.4860e-01, -1.0935e-01, -8.4869e-01,  ..., -1.4943e-02,\n",
       "            -3.1193e-01,  6.9209e-01],\n",
       "           [-8.4383e-01, -3.5878e-01, -7.1303e-01,  ..., -2.5004e-01,\n",
       "             4.7890e-01, -7.6337e-01],\n",
       "           [-1.1214e-01,  2.2031e-01, -4.2289e-01,  ...,  6.1363e-01,\n",
       "            -5.2153e-01, -3.0303e-01],\n",
       "           ...,\n",
       "           [-3.9254e-01, -1.0501e-01,  7.7278e-01,  ..., -3.7070e-01,\n",
       "             3.9127e-01,  4.7233e-01],\n",
       "           [ 8.4736e-02, -3.7168e-01, -3.9745e-01,  ...,  5.7863e-01,\n",
       "             1.1104e-03, -2.9904e-01],\n",
       "           [ 6.2772e-01,  4.6381e-01, -2.4049e-01,  ..., -2.9042e-01,\n",
       "             2.1682e-01,  2.2903e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-8.0772e-01, -2.2399e-01, -1.0531e+00,  ..., -6.6694e-03,\n",
       "             2.3259e-01, -1.1202e+00],\n",
       "           [ 8.5453e-01, -3.6574e-01, -2.6820e-01,  ...,  8.8247e-01,\n",
       "            -1.0999e-01, -2.5588e-01],\n",
       "           [-1.1502e+00, -3.8136e-01, -8.4325e-01,  ...,  8.1035e-01,\n",
       "             7.3788e-01, -2.9477e-01],\n",
       "           ...,\n",
       "           [ 7.5997e-02, -4.9151e-01, -1.7122e-01,  ..., -5.1435e-02,\n",
       "             5.3891e-01, -1.3965e+00],\n",
       "           [-3.2544e-01,  1.8608e-01, -1.3767e-01,  ..., -8.9980e-02,\n",
       "            -2.7635e-01, -2.2143e-01],\n",
       "           [ 4.5345e-02,  2.8212e-01,  9.2995e-01,  ..., -4.5739e-02,\n",
       "            -7.5324e-01, -7.7646e-01]],\n",
       " \n",
       "          [[-5.6811e-01,  1.6376e-01, -3.8131e-01,  ..., -1.3870e-01,\n",
       "             3.3442e-01, -8.8842e-02],\n",
       "           [-1.0351e+00, -1.6041e-01, -1.3561e+00,  ..., -5.9514e-01,\n",
       "             1.4475e-01, -1.2194e+00],\n",
       "           [ 3.1113e-01, -6.0780e-01, -2.5204e-01,  ...,  5.9639e-01,\n",
       "            -4.5770e-01, -5.9834e-01],\n",
       "           ...,\n",
       "           [-5.3447e-01, -7.3014e-01, -4.8977e-01,  ...,  5.1892e-01,\n",
       "             4.1741e-01, -9.0714e-01],\n",
       "           [ 2.4016e-01,  3.7802e-02, -9.3320e-01,  ..., -9.4468e-01,\n",
       "             2.4056e-01, -8.4292e-02],\n",
       "           [ 2.9196e-01, -6.9854e-01, -4.1201e-01,  ..., -3.5763e-01,\n",
       "            -1.3197e-01, -5.2770e-01]],\n",
       " \n",
       "          [[-1.1603e+00, -6.2221e-01, -1.4962e+00,  ...,  2.1812e-01,\n",
       "             1.7332e-03, -4.9548e-01],\n",
       "           [-4.4289e-01, -2.7540e-01, -1.8811e+00,  ...,  1.7067e-01,\n",
       "             5.1196e-01, -5.4893e-01],\n",
       "           [-4.0166e-01,  5.3725e-01, -1.0650e+00,  ...,  1.2766e-01,\n",
       "             3.8793e-02, -1.0659e+00],\n",
       "           ...,\n",
       "           [-3.9892e-01,  3.2527e-01, -7.9844e-01,  ..., -5.5758e-01,\n",
       "             6.9984e-01, -5.8045e-01],\n",
       "           [ 9.7620e-02, -1.8737e-02, -5.0171e-01,  ..., -5.5448e-01,\n",
       "             7.1300e-01, -6.9694e-01],\n",
       "           [ 4.9830e-02, -1.0355e+00, -8.2117e-01,  ..., -5.6134e-01,\n",
       "            -7.2063e-02, -5.9854e-01]]],\n",
       " \n",
       " \n",
       "         [[[-3.1012e-01,  2.0128e-01, -1.8664e+00,  ..., -3.0816e-01,\n",
       "            -4.0118e-01, -1.1680e+00],\n",
       "           [-6.1585e-01,  1.0196e-01, -1.1602e+00,  ...,  7.3714e-01,\n",
       "             1.1765e-01, -6.5298e-01],\n",
       "           [-8.6826e-01, -1.8611e-01, -1.0590e+00,  ...,  5.9859e-01,\n",
       "            -2.5276e-01, -9.7859e-01],\n",
       "           ...,\n",
       "           [-5.1040e-01, -8.8278e-01, -5.0582e-01,  ...,  3.4977e-01,\n",
       "             2.1162e-01, -1.0175e+00],\n",
       "           [-7.2237e-01,  9.8608e-01,  1.4655e-01,  ...,  5.0236e-02,\n",
       "            -4.7377e-01, -5.8672e-01],\n",
       "           [-3.9695e-01, -2.0594e-01, -2.6899e-01,  ...,  1.0069e-01,\n",
       "             7.0461e-01, -1.0956e+00]],\n",
       " \n",
       "          [[-2.7097e-01,  5.1873e-01, -9.3307e-01,  ...,  3.8617e-01,\n",
       "            -2.1762e-01, -1.1315e+00],\n",
       "           [-1.0418e-01, -9.1260e-01, -3.8079e-01,  ...,  7.7418e-01,\n",
       "            -7.2491e-01, -3.9878e-01],\n",
       "           [-4.7686e-01,  4.4271e-01,  4.7844e-01,  ...,  1.3523e-01,\n",
       "            -9.6595e-01, -1.0519e+00],\n",
       "           ...,\n",
       "           [-4.9721e-01, -5.9778e-01, -2.5273e-01,  ..., -5.8636e-01,\n",
       "             6.3049e-01, -3.3522e-01],\n",
       "           [-4.5074e-02, -1.0996e-01, -6.7945e-01,  ..., -1.2322e-01,\n",
       "             5.0168e-02, -6.9512e-01],\n",
       "           [-1.7097e-01,  4.6278e-01, -8.6974e-01,  ...,  2.5025e-02,\n",
       "             1.2719e-01, -7.8627e-01]],\n",
       " \n",
       "          [[-6.2891e-01, -1.8751e-01, -8.7249e-01,  ..., -3.1654e-02,\n",
       "            -3.0974e-01,  5.4691e-01],\n",
       "           [-9.2012e-01, -3.9942e-01, -7.8800e-01,  ..., -2.3752e-01,\n",
       "             4.0046e-01, -8.3471e-01],\n",
       "           [-1.4630e-01,  2.9016e-01, -5.0527e-01,  ...,  6.0437e-01,\n",
       "            -6.0039e-01, -2.7997e-01],\n",
       "           ...,\n",
       "           [-4.5133e-01, -8.7572e-02,  7.5460e-01,  ..., -2.9092e-01,\n",
       "             3.2900e-01,  3.2581e-01],\n",
       "           [ 2.8244e-03, -3.0607e-01, -4.9051e-01,  ...,  6.7767e-01,\n",
       "            -1.3339e-01, -3.7195e-01],\n",
       "           [ 6.1669e-01,  4.2221e-01, -2.4564e-01,  ..., -2.9661e-01,\n",
       "             1.4813e-01,  2.2382e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-8.2892e-01, -2.1887e-01, -1.1489e+00,  ...,  1.4336e-02,\n",
       "             2.4064e-01, -1.1652e+00],\n",
       "           [ 8.4804e-01, -4.2301e-01, -3.2800e-01,  ...,  9.4218e-01,\n",
       "            -1.7901e-01, -3.9228e-01],\n",
       "           [-1.1648e+00, -3.6356e-01, -8.8476e-01,  ...,  7.9198e-01,\n",
       "             7.5178e-01, -3.6390e-01],\n",
       "           ...,\n",
       "           [ 8.0222e-02, -5.2534e-01, -2.1525e-01,  ...,  2.4915e-02,\n",
       "             4.7907e-01, -1.4025e+00],\n",
       "           [-4.1042e-01,  2.2698e-01, -2.0610e-01,  ..., -9.9372e-02,\n",
       "            -3.4074e-01, -2.9492e-01],\n",
       "           [-5.2395e-02,  3.6606e-01,  9.3176e-01,  ..., -6.6673e-02,\n",
       "            -7.0853e-01, -8.8049e-01]],\n",
       " \n",
       "          [[-5.8502e-01,  1.6469e-01, -4.0606e-01,  ..., -1.6687e-01,\n",
       "             2.9217e-01, -1.0024e-01],\n",
       "           [-1.1904e+00, -1.8819e-01, -1.3134e+00,  ..., -5.7552e-01,\n",
       "             9.0368e-02, -1.3054e+00],\n",
       "           [ 2.5185e-01, -5.2809e-01, -2.4017e-01,  ...,  6.4588e-01,\n",
       "            -5.1994e-01, -6.0290e-01],\n",
       "           ...,\n",
       "           [-5.5018e-01, -8.2850e-01, -5.1929e-01,  ...,  4.5910e-01,\n",
       "             3.1444e-01, -9.7096e-01],\n",
       "           [ 3.6144e-01,  4.3743e-02, -9.1456e-01,  ..., -8.5690e-01,\n",
       "             1.8931e-01, -2.7126e-02],\n",
       "           [ 2.3354e-01, -7.3531e-01, -3.5030e-01,  ..., -3.3179e-01,\n",
       "            -1.1398e-01, -6.1926e-01]],\n",
       " \n",
       "          [[-1.1788e+00, -5.8661e-01, -1.5218e+00,  ...,  2.5878e-01,\n",
       "             6.6614e-03, -5.3126e-01],\n",
       "           [-5.4451e-01, -3.3973e-01, -1.9177e+00,  ...,  3.8301e-01,\n",
       "             3.6035e-01, -5.6396e-01],\n",
       "           [-3.3388e-01,  5.3057e-01, -1.1277e+00,  ...,  2.0767e-01,\n",
       "            -3.1460e-02, -1.0891e+00],\n",
       "           ...,\n",
       "           [-3.6106e-01,  2.5412e-01, -8.0603e-01,  ..., -5.4369e-01,\n",
       "             7.0805e-01, -6.1920e-01],\n",
       "           [ 5.9348e-02, -1.0516e-01, -4.9009e-01,  ..., -4.8970e-01,\n",
       "             6.7409e-01, -7.3719e-01],\n",
       "           [-7.5747e-02, -9.3209e-01, -8.8075e-01,  ..., -5.2025e-01,\n",
       "            -5.3182e-02, -7.5040e-01]]]], grad_fn=<StackBackward0>)}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformer_lm(dim=256, vocab_size=8000, depth=12, heads=8, dim_head=32, causal=True)\n",
    "model(x, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
