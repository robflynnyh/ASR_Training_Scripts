{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.randn(2, 2, 35, 64)\n",
    "att = torch.randn(2, 1, 35, 35)\n",
    "out = einsum('b n h d, b n h w -> b n d w', att, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "uses code from Phil Wang's implementation. \n",
    "https://github.com/lucidrains/FLASH-pytorch/blob/main/flash_pytorch/flash_pytorch.py\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2202.10447.pdf\n",
    "\n",
    "though this is slightly different, I remove the offset scale, as I am using l2 norm for q and k\n",
    "and I keep softmax with the learnt temperature rather than relu squeared\n",
    "I will test the proper implementation later but I think this will be better\n",
    "'''\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "from torch.utils.checkpoint import checkpoint # # gradient/activation checkpointing\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(x, dim = -1):\n",
    "    return F.normalize(x, p = 2, dim = dim)\n",
    "\n",
    "\n",
    "class OffsetScale(nn.Module):\n",
    "    def __init__(self, dim, heads = 1):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(heads, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(heads, dim))\n",
    "        nn.init.normal_(self.gamma, std = 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = einsum('b h n d, h d -> b h n d', x, self.gamma) + self.beta\n",
    "        return out\n",
    "\n",
    "'''\n",
    "uses code from Phil Wang's implementation. \n",
    "https://github.com/lucidrains/FLASH-pytorch/blob/main/flash_pytorch/flash_pytorch.py\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2202.10447.pdf\n",
    "\n",
    "though this is slightly different, I remove the offset scale, as I am using l2 norm for q and k\n",
    "and I keep softmax with the learnt temperature rather than relu squeared\n",
    "I will test the proper implementation later but I think this will be better\n",
    "'''\n",
    "\n",
    "class CosineGatedAttentionUnit(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads = 1,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        expansion_factor=2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.expansion_factor = expansion_factor\n",
    "\n",
    "        self.norm = nn.LayerNorm(n_feats)\n",
    "        \n",
    "        self.to_vgate = nn.Sequential(nn.Linear(n_feats, n_feats * expansion_factor * 2), nn.SiLU())\n",
    "        self.to_query_key = nn.Sequential(nn.Linear(n_feats, head_dim * n_heads * 2), nn.SiLU())\n",
    "        self.out_projection = nn.Linear(n_feats * expansion_factor, n_feats)\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation_type = activation\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        self.offset_scale = OffsetScale(head_dim*2, heads=n_heads)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Linear(n_feats * expansion_factor, n_feats), nn.Dropout(0.1))\n",
    "\n",
    "\n",
    "    def forward(self, x, pos_fn, mask=None):\n",
    "        assert pos_fn is not None, 'pls provide a position function'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, N, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        v, gate = self.to_vgate(x).chunk(2, dim=-1)\n",
    "     \n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=H)\n",
    "     \n",
    "\n",
    "        q, k = self.offset_scale(self.to_query_key(x)).chunk(2, dim=-1)\n",
    "        q, k = map(l2norm, (q, k)) # qk norm attention\n",
    "\n",
    "        dots = einsum('bhid,bhjd->bhij', q, k) * self.temperature\n",
    "        dots += pos_fn(dots.shape[-1], device=dots.device, dtype=dots.dtype)\n",
    "\n",
    "        qkmask = ~mask\n",
    "        attn_mask = ~(rearrange(qkmask, \"b n -> b () n ()\") * rearrange(qkmask, \"b n -> b () () n\"))\n",
    "\n",
    "        if self.causal: # create a regular causal mask\n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device, dtype=torch.bool).triu(1)\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "\n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "\n",
    "        attn = self.activation(dots) \n",
    "\n",
    "    \n",
    "        out = einsum(\"bhij,bhjd->bhid\", attn, v)\n",
    "      \n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        out = out * gate\n",
    "\n",
    "        out = self.to_out(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim, \n",
    "            depth, \n",
    "            heads, \n",
    "            dim_head, \n",
    "            causal=True,\n",
    "            temperature=15.5,\n",
    "            shared_temperture=False,\n",
    "            intermediate_loss=True,\n",
    "            dropout = 0.1,\n",
    "            checkpoint = False,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            intermediate_loss = False\n",
    "\n",
    "        ff_mult = kwargs.get('ff_mult', 4)\n",
    "     \n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature), requires_grad=True) if shared_temperture else temperature\n",
    "\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.depth = depth\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = dim // 4,\n",
    "            heads = heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.grad_checkpointing = checkpoint\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                CosineGatedAttentionUnit(\n",
    "                    dim, \n",
    "                    n_heads=heads, \n",
    "                    head_dim=dim_head, \n",
    "                    causal=causal,\n",
    "                    temperature=self.temperature,\n",
    "                    dropout=dropout,\n",
    "                    **kwargs\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_custom_forward(module):\n",
    "        def custom_forward(*args, **kwargs):\n",
    "            return module(*args, **kwargs)\n",
    "        return custom_forward\n",
    "\n",
    "    def checkpoint(self, layer, module, *args, **kwargs):\n",
    "        condition = self.training and self.grad_checkpointing and layer < self.depth - 1\n",
    "        return checkpoint(self.create_custom_forward(module), *args, **kwargs) if condition else module(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x, mask=None, self_condtioning=None):\n",
    "        intermediate_logits = []\n",
    "        for i, attn in enumerate(self.layers):\n",
    "            #x = attn(x, self.positional_bias, mask=mask) \n",
    "            x = self.checkpoint(i, attn, x, self.positional_bias, mask) + x\n",
    "            \n",
    "            if i < self.depth - 1 and self_condtioning is not None:\n",
    "                x, logits = self_condtioning(x)\n",
    "                intermediate_logits.append(logits)\n",
    "\n",
    "        # stack intermediate logits\n",
    "        if len(intermediate_logits) > 0:\n",
    "            intermediate_logits = torch.stack(intermediate_logits, dim=0) # D x B x N x V\n",
    "\n",
    "        return x, intermediate_logits\n",
    "\n",
    "\n",
    "class transformer_lm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        causal=True,\n",
    "        temperature=15.5,\n",
    "        dropout=0.,\n",
    "        shared_temperture=True,\n",
    "        self_conditioning=False,\n",
    "        intermediate_loss=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self_conditioning == False\n",
    "\n",
    "        self.self_conditioning = True if self_conditioning else None\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "\n",
    "        if self_conditioning:\n",
    "            self.reprojection_layer = nn.Linear(vocab_size, dim)\n",
    "\n",
    "        self.layers = transformer(\n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = heads, \n",
    "            dim_head = dim_head, \n",
    "            causal = causal, \n",
    "            dropout = dropout,\n",
    "            temperature = temperature,\n",
    "            shared_temperture = shared_temperture,\n",
    "            intermediate_loss = intermediate_loss,\n",
    "            **kwargs\n",
    "        )\n",
    " \n",
    "        self.to_logits = nn.Linear(dim, vocab_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def self_condition_fn(self):\n",
    "        def self_condition(x):\n",
    "            logits = self.to_logits(self.post_norm(x))\n",
    "            if self.self_conditioning:\n",
    "                z = F.softmax(logits, dim=-1)\n",
    "                z = self.reprojection_layer(z)\n",
    "                x = z + x\n",
    "            return x, logits\n",
    "        if (self.self_conditioning or self.intermediate_loss) and self.training:\n",
    "            return self_condition\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x, interim_logits = self.layers(x, mask=~mask if mask is not None else None, self_condtioning=self.self_condition_fn())\n",
    "        x = self.post_norm(x)\n",
    "        x = self.to_logits(x)\n",
    "\n",
    "        return  { 'out': x, 'interim_logits': interim_logits } if self.training else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_lm(\n",
    "    dim = 256,\n",
    "    vocab_size = 128,\n",
    "    depth = 6,\n",
    "    heads = 2,\n",
    "    dim_head = 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2400\n",
    "B = 30\n",
    "x = torch.randint(0, 128, (B, N))\n",
    "mask = torch.zeros(B, 1).bool()\n",
    "mask[0, :5] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THERE\n",
      "THERE\n",
      "THERE\n",
      "THERE\n",
      "THERE\n",
      "THERE\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model(x, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]]])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
