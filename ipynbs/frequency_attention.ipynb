{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum\n",
    "from torch.utils.checkpoint import checkpoint # # gradient/activation checkpointing\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1,))\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device = device).type_as(self.inv_freq)\n",
    "        sinu = einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n",
    "        return emb * self.scale\n",
    "\n",
    "class ReLUSquared(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.pow(F.relu(x), 2)\n",
    "\n",
    "def l2norm(t, groups = 1, dim = -1):\n",
    "    if groups == 1:\n",
    "        return F.normalize(t, p = 2, dim = dim)\n",
    "    t = rearrange(t, '... (g d) -> ... g d', g = groups)\n",
    "    t = F.normalize(t, p = 2, dim = dim)\n",
    "    return rearrange(t, '... g d -> ... (g d)')\n",
    "\n",
    "\n",
    "\n",
    "class transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim, \n",
    "            depth, \n",
    "            heads, \n",
    "            dim_head, \n",
    "            causal=True,\n",
    "            temperature=15.5,\n",
    "            shared_temperture=False,\n",
    "            intermediate_loss=True,\n",
    "            dropout = 0.1,\n",
    "            checkpoint = True,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            intermediate_loss = False\n",
    "\n",
    "        ff_mult = kwargs.get('ff_mult', 4)\n",
    "     \n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature), requires_grad=True) if shared_temperture else temperature\n",
    "\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.depth = depth\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = dim // 4,\n",
    "            heads = heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "        self.grad_checkpointing = checkpoint\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, CosineAttention(\n",
    "                    dim, \n",
    "                    n_heads=heads, \n",
    "                    head_dim=dim_head, \n",
    "                    causal=causal,\n",
    "                    temperature=self.temperature,\n",
    "                    dropout=dropout,\n",
    "                    **kwargs\n",
    "                )),\n",
    "                PreNorm(dim, self.ff(dim, mult=ff_mult))\n",
    "            ]))\n",
    "\n",
    "    @staticmethod\n",
    "    def ff(dim, mult=4, dropout=0.1):\n",
    "        return nn.Sequential(\n",
    "            GLU(dim, dim * mult, nn.SiLU()),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def create_custom_forward(module):\n",
    "        def custom_forward(*args, **kwargs):\n",
    "            return module(*args, **kwargs)\n",
    "        return custom_forward\n",
    "\n",
    "    def checkpoint(self, layer, module, *args, **kwargs):\n",
    "        condition = self.training and self.grad_checkpointing and layer < self.depth - 1\n",
    "        return checkpoint(self.create_custom_forward(module), *args, **kwargs) if condition else module(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None, self_condtioning=None):\n",
    "        intermediate_logits = []\n",
    "        for i, (attn, ff) in enumerate(self.layers):\n",
    "            x = self.checkpoint(i, attn, x, self.positional_bias, mask) + x\n",
    "            x = self.checkpoint(i, ff, x) + x\n",
    "\n",
    "            if i < self.depth - 1 and self_condtioning is not None:\n",
    "                x, logits = self_condtioning(x)\n",
    "                intermediate_logits.append(logits)\n",
    "\n",
    "        # stack intermediate logits\n",
    "        if len(intermediate_logits) > 0:\n",
    "            intermediate_logits = torch.stack(intermediate_logits, dim=0) # D x B x N x V\n",
    "   \n",
    "        return x, intermediate_logits\n",
    "\n",
    "\n",
    "class transformer_lm(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        vocab_size,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        causal=True,\n",
    "        temperature=15.5,\n",
    "        dropout=0.,\n",
    "        shared_temperture=True,\n",
    "        self_conditioning=False,\n",
    "        intermediate_loss=True,\n",
    "        use_abs_pos=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            self_conditioning == False\n",
    "\n",
    "        self.self_conditioning = True if self_conditioning else None\n",
    "        self.intermediate_loss = intermediate_loss\n",
    "\n",
    "        self.use_abs_pos = use_abs_pos\n",
    "        if self.use_abs_pos:\n",
    "            self.abs_pos_fn = ScaledSinuEmbedding(dim=dim)\n",
    "        self.abs_pos = lambda x: x + self.abs_pos_fn(x) if self.use_abs_pos else x\n",
    "\n",
    "        if self_conditioning:\n",
    "            self.reprojection_layer = nn.Linear(vocab_size, dim)\n",
    "\n",
    "        self.layers = transformer(\n",
    "            dim = dim, \n",
    "            depth = depth, \n",
    "            heads = heads, \n",
    "            dim_head = dim_head, \n",
    "            causal = causal, \n",
    "            dropout = dropout,\n",
    "            temperature = temperature,\n",
    "            shared_temperture = shared_temperture,\n",
    "            intermediate_loss = intermediate_loss,\n",
    "            **kwargs\n",
    "        )\n",
    " \n",
    "        self.to_logits = nn.Linear(dim, vocab_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def self_condition_fn(self):\n",
    "        def self_condition(x):\n",
    "            logits = self.to_logits(self.post_norm(x))\n",
    "            if self.self_conditioning:\n",
    "                z = F.softmax(logits, dim=-1)\n",
    "                z = self.reprojection_layer(z)\n",
    "                x = z + x\n",
    "            return x, logits\n",
    "        return self_condition if (self.self_conditioning or self.intermediate_loss) and self.training else None\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.abs_pos(x)\n",
    "        x, interim_logits = self.layers(x, mask=~mask if mask is not None else None, self_condtioning=self.self_condition_fn())\n",
    "        x = self.post_norm(x)\n",
    "        x = self.to_logits(x)\n",
    "\n",
    "        return  { 'out': x, 'interim_logits': interim_logits } if self.training else x\n",
    "\n",
    "\n",
    "def pad_to_window_size(x, window_size, axis=3, mask=None):\n",
    "    \"\"\"\n",
    "    Pad the input on two sides to be divisible by `window_size`\n",
    "    \"\"\"\n",
    "    batch_size, sequence_length, hidden_size = x.shape\n",
    "    if sequence_length % window_size == 0:\n",
    "        return x, 0, mask\n",
    "    padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "    padding = torch.zeros(batch_size, padding_length, hidden_size,\n",
    "        device=x.device,\n",
    "        dtype=x.dtype,\n",
    "    )\n",
    "    mask = F.pad(mask, (0, padding_length), value=True) \n",
    "    return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "def unpad(x, padding_length):\n",
    "    \"\"\"\n",
    "    Undo padding.\n",
    "    \"\"\"\n",
    "    if padding_length > 0:\n",
    "        return x[:, :-padding_length]\n",
    "    return x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(self.norm(x), *args, **kwargs)\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation):\n",
    "        super().__init__()\n",
    "        self.act = activation\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim = -1)\n",
    "        return x * self.act(gate)\n",
    "\n",
    "class CosineAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        bias=False,\n",
    "        temperature=15.5,\n",
    "        return_attention=False,\n",
    "        causal=False,\n",
    "        activation='softmax',\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert activation in ['relusq', 'softmax']\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.temperature = torch.nn.Parameter(torch.tensor(temperature), requires_grad=True) if isinstance(temperature, float) else temperature\n",
    "\n",
    "        self.activation = ReLUSquared() if activation == 'relusq' else nn.Softmax(dim=-1)\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear((n_heads * head_dim) + (n_heads * head_dim)//4 , n_feats, bias=bias)\n",
    "\n",
    "        self.WE_layer = window_embeddings(n_feats=n_feats, dropout=dropout, bias=bias,)\n",
    "\n",
    "\n",
    "    def attend(self, qkv, mask, pos_fn):\n",
    "        query, key, value = qkv\n",
    "        \n",
    "        query, key = map(l2norm, (query, key))\n",
    "\n",
    "        dots = einsum('bhwid,bhwjd->bhwij', query, key) * self.temperature\n",
    "        \n",
    "        dots += pos_fn(dots.shape[-1], device=dots.device, dtype=dots.dtype).unsqueeze(1)\n",
    "        qkmask = ~mask\n",
    " \n",
    "        attn_mask = ~(rearrange(qkmask, \"b w n -> b () w n ()\") * rearrange(qkmask, \"b w n -> b () w () n\"))\n",
    "\n",
    "\n",
    "        if self.causal: # create a regular causal mask    \n",
    "            causal_mask = torch.ones(dots.shape[-2], dots.shape[-1], device=dots.device).triu(1).bool()\n",
    "            attn_mask = torch.logical_or(attn_mask, causal_mask)\n",
    "        \n",
    "        dots.masked_fill_(attn_mask, -torch.finfo(dots.dtype).max)\n",
    "    \n",
    "        attn = self.activation(dots)   \n",
    "\n",
    "        attn = self.dropout(attn)\n",
    " \n",
    "    \n",
    "        return einsum(\"bhwij,bhwjd->bhwid\", attn, value)\n",
    "\n",
    "\n",
    "    def forward(self, x, pos_fn, mask=None):\n",
    "        assert pos_fn is not None, 'pls provide a position function'\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "       \n",
    "        if mask is None:\n",
    "            mask = torch.zeros(B, N, device=x.device, dtype=torch.bool)\n",
    "\n",
    "        WINDOW_SIZE = 32\n",
    "        x, pad_n, mask = pad_to_window_size(x, window_size=WINDOW_SIZE, axis=-2, mask=mask) # first pad so that sequence length is divisible by window size\n",
    "        B, N, C = x.shape\n",
    "        x = rearrange(x, 'b (w n) d -> b w n d', w=N// WINDOW_SIZE, n=WINDOW_SIZE) # group into windows\n",
    "        mask = rearrange(mask, 'b (w n) -> b w n', w=N// WINDOW_SIZE, n=WINDOW_SIZE)\n",
    "\n",
    "        qkv = rearrange(self.qkv_proj(x), \"b w n (h d qkv) -> qkv b h w n d\", qkv=3, h=H, d=D) # qkv projection\n",
    "    \n",
    "        out = self.attend(qkv, mask, pos_fn)\n",
    "\n",
    "\n",
    "        out = rearrange(out, 'b h w n d -> b w n (h d)')\n",
    "\n",
    "        out = self.WE_layer(out, mask)\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "        out = rearrange(out, 'b w n d -> b (w n) d')\n",
    "        out = unpad(out, pad_n)\n",
    "        return out\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    '''taken From Phil Wang's x-transformers library'''\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1,))\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, n, device):\n",
    "        t = torch.arange(n, device = device).type_as(self.inv_freq)\n",
    "        sinu = einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n",
    "        return emb * self.scale\n",
    "\n",
    "class window_embeddings(nn.Module):\n",
    "    def __init__(self, n_feats, dropout=0.1, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bias = bias\n",
    "        self.proj_dim = n_feats // 2\n",
    "        self.v_dim = n_feats // 4\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.linear_in = nn.Linear(n_feats, self.proj_dim, bias=bias)\n",
    "       \n",
    "        self.q_proj = nn.Linear(self.proj_dim, self.proj_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(self.n_feats, self.proj_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(self.n_feats, self.v_dim, bias=bias)\n",
    "        self.activation = nn.Softmax(dim=-1)\n",
    "        self.bos = nn.Parameter(torch.empty(1, 1, 1, self.v_dim))\n",
    "        nn.init.xavier_uniform_(self.bos)\n",
    "        self.pos = ScaledSinuEmbedding(self.proj_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        lengths = (~mask).sum(dim=-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        # make sure there's no zero length values\n",
    "        lengths = torch.where(lengths == 0, torch.ones_like(lengths), lengths)\n",
    "\n",
    "        we = x.masked_fill(mask.unsqueeze(-1), 0) # mask out padded tokens\n",
    "\n",
    "        we = self.linear_in(we)\n",
    "        we = we.sum(dim=-2, keepdim=True) / lengths  # average pool over window\n",
    "        \n",
    "        we = self.ReLU(we)\n",
    "        # concat zeros to the end of the sequence and remove the first index\n",
    "   \n",
    "        we = torch.cat((we, torch.zeros_like(we[:, :1])), dim=1)[:,1:]\n",
    "\n",
    "        we_q, we_v, k = self.q_proj(we), self.v_proj(x), self.k_proj(x)\n",
    "        k = k + self.pos(k.shape[-2], device=k.device)\n",
    "        \n",
    "        we_q, k = map(partial(l2norm, dim=-1, groups=16), (we_q, k)) # l2 group norm with 16 groups so similarity is bounded \n",
    "\n",
    "        dots = einsum('bwid,bwjd->bwij', we_q, k) \n",
    "\n",
    "        dots.masked_fill_(mask.unsqueeze(2), -torch.finfo(dots.dtype).max)\n",
    "        attn = self.activation(dots)\n",
    "        out = einsum(\"bwij,bwjd->bwid\", attn, we_v)\n",
    "     \n",
    "\n",
    "        out = torch.cat((self.bos.expand(x.shape[0], -1, -1, -1), out), dim=1)[:,:-1] # add bos and shift\n",
    "    \n",
    "        x = torch.cat((x, out.expand(-1, -1, x.shape[2], -1)), dim=-1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 8, 1, 64]) torch.Size([30, 1, 1, 64])\n",
      "torch.Size([30, 8, 1, 64]) torch.Size([30, 1, 1, 64])\n",
      "torch.Size([30, 8, 1, 64]) torch.Size([30, 1, 1, 64])\n",
      "torch.Size([30, 8, 1, 64]) torch.Size([30, 1, 1, 64])\n",
      "torch.Size([30, 8, 1, 64]) torch.Size([30, 1, 1, 64])\n",
      "torch.Size([30, 8, 1, 64]) torch.Size([30, 1, 1, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'out': tensor([[[-0.3220, -0.3389,  0.5299,  ...,  0.5808,  0.5994, -0.5984],\n",
       "          [ 0.6251, -0.3153, -0.1449,  ...,  0.8005, -0.1049, -0.6815],\n",
       "          [ 0.7936,  0.4714, -0.5478,  ...,  0.6174,  0.5196,  0.1064],\n",
       "          ...,\n",
       "          [-0.3391, -0.8729, -0.2008,  ..., -0.1233,  0.7185,  1.2766],\n",
       "          [ 0.4883, -0.0829,  0.3724,  ...,  0.5850, -0.8943, -0.6091],\n",
       "          [ 0.7001, -0.0825, -0.5681,  ...,  0.3369, -0.9656,  0.3134]],\n",
       " \n",
       "         [[-0.1190, -0.8009, -0.4927,  ..., -0.2476, -0.9198,  0.6978],\n",
       "          [ 0.8718, -0.8740,  0.4628,  ..., -0.1061, -0.1922, -0.6630],\n",
       "          [ 0.2980, -0.6010,  0.5543,  ...,  0.4488, -0.5924, -0.8560],\n",
       "          ...,\n",
       "          [-0.0715,  0.6731, -0.5023,  ..., -0.1968, -0.2005, -0.9644],\n",
       "          [-0.7268,  0.4349,  0.1306,  ...,  0.0654, -0.2837,  0.2450],\n",
       "          [ 1.2718,  1.5270, -0.9429,  ..., -1.1302, -0.3113,  0.0258]],\n",
       " \n",
       "         [[-0.1402,  0.3574,  0.9336,  ..., -0.0868, -0.0743, -0.2556],\n",
       "          [ 0.7573,  0.4885,  0.0121,  ..., -0.4585,  0.0632, -0.0389],\n",
       "          [-0.0142, -0.2538,  0.8879,  ...,  0.3618,  0.2038, -0.7509],\n",
       "          ...,\n",
       "          [-0.0667,  0.6566, -0.0565,  ..., -0.1888, -0.3818, -0.6461],\n",
       "          [-0.6193,  0.1499, -0.4279,  ...,  0.2544, -0.5419, -0.6806],\n",
       "          [ 0.2055, -0.7626,  0.1267,  ...,  0.4614,  1.0649, -0.1981]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.5326, -0.1614,  0.5803,  ...,  0.2890, -0.5495,  0.5852],\n",
       "          [ 1.1235,  1.3187, -0.1459,  ...,  0.5096, -0.0487, -0.3542],\n",
       "          [ 0.0647,  0.0500, -0.4800,  ...,  0.3070, -0.5006, -0.5656],\n",
       "          ...,\n",
       "          [-0.3013, -0.9245,  0.0784,  ..., -0.8251,  0.0044, -1.3517],\n",
       "          [ 0.2398,  0.4063, -0.6793,  ...,  1.1884, -1.2652, -0.2757],\n",
       "          [-0.5653, -0.5067, -0.3208,  ...,  0.6650, -0.3815,  1.0075]],\n",
       " \n",
       "         [[ 0.0018, -0.0452, -1.0777,  ...,  0.5483,  0.7179, -0.2491],\n",
       "          [ 0.4494, -0.2587,  0.1741,  ...,  1.3668, -0.8049, -1.5515],\n",
       "          [ 0.7341, -0.5982, -0.0038,  ..., -0.1087, -0.8069,  0.1908],\n",
       "          ...,\n",
       "          [ 0.0051,  0.1185, -0.5889,  ..., -0.9395,  0.2928,  0.1386],\n",
       "          [-0.2036,  0.5234, -0.2274,  ...,  0.8248,  0.1315,  0.4519],\n",
       "          [-0.3929,  0.0372,  0.7899,  ...,  0.0334, -0.0735,  0.2741]],\n",
       " \n",
       "         [[ 0.4431, -0.1015,  1.0134,  ...,  0.0098, -0.1162, -1.1134],\n",
       "          [-0.0047,  0.0243,  0.0586,  ..., -0.4857,  0.5801, -0.1954],\n",
       "          [ 0.0697, -0.5556,  0.2325,  ..., -0.2218,  0.0950,  0.6029],\n",
       "          ...,\n",
       "          [-0.4797, -0.6120, -0.4049,  ...,  0.7018, -0.3790,  1.0667],\n",
       "          [ 0.5486,  0.3510, -0.3409,  ...,  0.0516, -0.4468,  0.6780],\n",
       "          [-0.4836, -0.6142,  0.6987,  ...,  0.5553,  0.4820, -0.2813]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'interim_logits': tensor([[[[-7.5109e-01, -3.9411e-01,  1.0542e+00,  ...,  7.7557e-01,\n",
       "             8.9854e-01, -1.9820e-01],\n",
       "           [ 7.5255e-01, -5.2017e-01, -8.3356e-02,  ...,  4.9970e-01,\n",
       "             2.3274e-02, -3.7448e-01],\n",
       "           [ 2.8792e-01, -1.9859e-01, -7.0350e-01,  ..., -2.8720e-01,\n",
       "             5.3741e-01, -3.2930e-02],\n",
       "           ...,\n",
       "           [-5.2430e-01, -6.8204e-01,  1.6275e-02,  ...,  1.2700e-01,\n",
       "             1.0217e+00,  1.4394e+00],\n",
       "           [ 4.2179e-01, -1.3606e-01,  3.6750e-01,  ...,  4.9407e-01,\n",
       "            -8.5974e-01, -2.3536e-01],\n",
       "           [ 6.8069e-01, -7.6988e-02, -4.0214e-01,  ...,  1.2986e-01,\n",
       "            -9.3185e-01,  3.9649e-01]],\n",
       " \n",
       "          [[-5.5732e-02, -2.8542e-01, -2.8947e-01,  ..., -2.3062e-01,\n",
       "            -8.4150e-01,  5.1985e-01],\n",
       "           [ 6.1402e-01, -6.1123e-01,  9.4542e-01,  ..., -6.1988e-02,\n",
       "             1.0457e-01, -7.3241e-01],\n",
       "           [ 2.7154e-01, -3.4210e-01,  9.6345e-01,  ...,  9.0959e-01,\n",
       "            -3.7437e-01, -1.1224e+00],\n",
       "           ...,\n",
       "           [-1.2654e-01,  8.4364e-01, -3.5031e-01,  ..., -1.6982e-01,\n",
       "            -3.0284e-01, -6.5356e-01],\n",
       "           [-8.3775e-01,  5.9444e-01, -1.5841e-01,  ..., -7.3121e-02,\n",
       "            -1.1042e-02,  2.7614e-01],\n",
       "           [ 1.0807e+00,  1.3003e+00, -9.5804e-01,  ..., -1.1401e+00,\n",
       "            -3.5072e-01,  2.0351e-01]],\n",
       " \n",
       "          [[-9.9114e-02,  7.0289e-01,  6.8237e-01,  ...,  1.3244e-01,\n",
       "             1.5008e-01, -2.7149e-01],\n",
       "           [ 9.1841e-01,  3.2508e-01,  4.3903e-02,  ..., -7.1102e-01,\n",
       "            -6.7865e-01,  3.7696e-01],\n",
       "           [ 2.1009e-01, -5.5383e-01,  6.0714e-01,  ...,  3.0584e-01,\n",
       "             1.3314e-01, -6.2807e-01],\n",
       "           ...,\n",
       "           [ 2.0924e-01,  5.6320e-01,  6.4343e-02,  ..., -2.6298e-01,\n",
       "            -3.8787e-01, -5.2480e-01],\n",
       "           [-5.4504e-01,  1.3797e-01, -1.9157e-01,  ...,  2.2130e-01,\n",
       "            -3.3266e-01, -4.5468e-01],\n",
       "           [ 3.4559e-01, -9.2665e-01, -1.2131e-02,  ...,  3.8859e-01,\n",
       "             1.0170e+00, -2.3995e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.6982e-01, -2.7121e-01,  6.2803e-01,  ..., -2.7293e-01,\n",
       "            -5.2354e-02,  2.3351e-01],\n",
       "           [ 8.9061e-01,  1.1720e+00, -1.1152e-01,  ...,  1.8638e-01,\n",
       "             2.3438e-01, -5.0193e-01],\n",
       "           [ 3.4641e-01,  5.7895e-01, -1.1954e-01,  ...,  4.5127e-01,\n",
       "            -3.7771e-02, -4.1617e-01],\n",
       "           ...,\n",
       "           [-5.3528e-01, -9.1772e-01,  2.9356e-02,  ..., -6.7599e-01,\n",
       "             1.4080e-01, -1.4451e+00],\n",
       "           [ 8.4713e-02,  4.3164e-01, -5.8864e-01,  ...,  1.2395e+00,\n",
       "            -1.3874e+00, -2.1099e-01],\n",
       "           [-6.5728e-01, -6.2371e-01, -5.4759e-01,  ...,  8.1661e-01,\n",
       "            -3.6467e-01,  9.1072e-01]],\n",
       " \n",
       "          [[ 9.7286e-02, -7.0860e-02, -1.2301e+00,  ..., -1.3216e-03,\n",
       "             4.8697e-01, -2.8019e-01],\n",
       "           [ 5.4621e-01,  3.2421e-01, -1.4667e-01,  ...,  1.0421e+00,\n",
       "            -7.8922e-01, -1.5116e+00],\n",
       "           [ 8.1778e-01, -2.1113e-01,  2.0989e-01,  ..., -5.9699e-01,\n",
       "            -4.6569e-01,  1.7271e-01],\n",
       "           ...,\n",
       "           [ 4.5744e-04,  8.6656e-02, -7.9703e-01,  ..., -1.0995e+00,\n",
       "             3.6533e-01,  1.0419e-01],\n",
       "           [-4.7566e-01,  5.4220e-01, -7.8787e-02,  ...,  8.9973e-01,\n",
       "             1.8269e-01,  5.3366e-01],\n",
       "           [-2.8704e-01, -1.2678e-02,  9.2899e-01,  ...,  1.6126e-01,\n",
       "            -4.1022e-02,  2.5458e-01]],\n",
       " \n",
       "          [[ 4.4253e-01,  4.0266e-01,  8.1733e-01,  ...,  3.6203e-01,\n",
       "             1.7049e-01, -9.8693e-01],\n",
       "           [-3.6734e-01,  1.6737e-01, -9.0489e-02,  ..., -4.3728e-01,\n",
       "             8.1468e-01, -6.1470e-02],\n",
       "           [-2.4814e-01, -4.8996e-01, -8.5241e-02,  ...,  2.4257e-01,\n",
       "             1.5108e-01,  5.7169e-01],\n",
       "           ...,\n",
       "           [-6.4589e-01, -6.3579e-01, -5.5759e-01,  ...,  8.2907e-01,\n",
       "            -3.7518e-01,  8.9295e-01],\n",
       "           [ 3.8223e-01,  4.7386e-01, -2.9901e-01,  ..., -1.1721e-01,\n",
       "            -3.5256e-01,  6.9916e-01],\n",
       "           [-6.2159e-01, -6.0609e-01,  7.0357e-01,  ...,  6.3242e-01,\n",
       "             7.8399e-01, -1.4585e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.6154e-01, -5.3827e-01,  8.3283e-01,  ...,  7.4730e-01,\n",
       "             1.0062e+00, -3.0253e-01],\n",
       "           [ 7.4152e-01, -4.1436e-01,  7.1812e-02,  ...,  5.4553e-01,\n",
       "             5.2975e-02, -6.2565e-01],\n",
       "           [ 5.5846e-01, -2.6955e-01, -4.5204e-01,  ..., -2.2249e-01,\n",
       "             6.0887e-01, -1.0884e-01],\n",
       "           ...,\n",
       "           [-4.7508e-01, -7.3562e-01, -9.1159e-02,  ...,  3.1861e-02,\n",
       "             9.9904e-01,  1.3271e+00],\n",
       "           [ 4.4962e-01, -2.5170e-01,  3.1222e-01,  ...,  4.8378e-01,\n",
       "            -8.4971e-01, -2.4455e-01],\n",
       "           [ 6.3302e-01, -9.1667e-02, -5.0471e-01,  ...,  2.2675e-01,\n",
       "            -9.5036e-01,  3.9155e-01]],\n",
       " \n",
       "          [[ 3.0564e-01, -2.5297e-01, -3.6582e-01,  ..., -6.0556e-01,\n",
       "            -6.6233e-01,  7.8931e-01],\n",
       "           [ 1.1291e+00, -7.5160e-01,  7.8970e-01,  ..., -2.6785e-01,\n",
       "             1.3028e-01, -5.1336e-01],\n",
       "           [ 4.8010e-01, -4.5362e-01,  7.4358e-01,  ...,  6.4774e-01,\n",
       "            -4.6695e-01, -1.0479e+00],\n",
       "           ...,\n",
       "           [-6.0931e-02,  8.6237e-01, -2.8942e-01,  ..., -1.8284e-01,\n",
       "            -2.0633e-01, -6.2330e-01],\n",
       "           [-8.0338e-01,  4.7345e-01, -5.8810e-02,  ..., -8.0674e-02,\n",
       "            -2.2380e-02,  2.9811e-01],\n",
       "           [ 1.1834e+00,  1.3459e+00, -8.7282e-01,  ..., -1.1415e+00,\n",
       "            -2.8703e-01,  2.5340e-01]],\n",
       " \n",
       "          [[-1.9582e-01,  5.3896e-01,  8.1409e-01,  ..., -1.6137e-02,\n",
       "            -2.4350e-03, -3.5797e-01],\n",
       "           [ 8.1944e-01,  3.0562e-01,  6.2158e-03,  ..., -5.9760e-01,\n",
       "            -6.8275e-01,  3.3901e-01],\n",
       "           [ 3.1061e-02, -5.5651e-01,  7.1466e-01,  ...,  4.4901e-01,\n",
       "            -4.0966e-03, -7.1319e-01],\n",
       "           ...,\n",
       "           [ 2.9514e-01,  5.6004e-01,  1.3263e-01,  ..., -2.3453e-01,\n",
       "            -3.8498e-01, -4.7235e-01],\n",
       "           [-6.6665e-01,  2.0423e-01, -1.8663e-01,  ...,  1.6443e-01,\n",
       "            -4.8504e-01, -4.1218e-01],\n",
       "           [ 3.3955e-01, -1.0394e+00, -3.6937e-02,  ...,  2.9457e-01,\n",
       "             9.9748e-01, -1.7948e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.3017e-01, -6.6530e-02,  6.6037e-01,  ...,  9.5411e-02,\n",
       "             1.5785e-01,  3.5336e-01],\n",
       "           [ 9.2929e-01,  1.3100e+00,  6.1504e-02,  ...,  1.5151e-01,\n",
       "             4.1697e-01, -1.2570e-01],\n",
       "           [ 4.3865e-01,  5.8243e-01, -2.0930e-01,  ...,  3.0215e-01,\n",
       "            -3.2331e-02, -1.6650e-01],\n",
       "           ...,\n",
       "           [-4.5401e-01, -9.6016e-01,  2.0580e-02,  ..., -7.7918e-01,\n",
       "             1.3544e-01, -1.3241e+00],\n",
       "           [ 1.3734e-02,  4.8914e-01, -5.9071e-01,  ...,  1.2300e+00,\n",
       "            -1.3917e+00, -2.3177e-01],\n",
       "           [-6.2459e-01, -5.9680e-01, -5.5509e-01,  ...,  7.5387e-01,\n",
       "            -3.6692e-01,  9.8096e-01]],\n",
       " \n",
       "          [[ 1.2622e-01,  4.9795e-02, -1.1945e+00,  ..., -1.9889e-01,\n",
       "             3.8654e-01, -8.0162e-02],\n",
       "           [ 8.4455e-01,  2.7409e-01, -1.3222e-01,  ...,  7.5423e-01,\n",
       "            -9.6950e-01, -1.3697e+00],\n",
       "           [ 1.0133e+00, -3.2163e-01,  5.8534e-02,  ..., -6.6887e-01,\n",
       "            -7.6078e-01,  4.4221e-01],\n",
       "           ...,\n",
       "           [-3.1794e-02,  6.9588e-02, -7.3278e-01,  ..., -1.0440e+00,\n",
       "             3.5317e-01,  2.7985e-01],\n",
       "           [-4.7345e-01,  4.7515e-01, -1.4464e-01,  ...,  8.8838e-01,\n",
       "             1.6573e-01,  5.5769e-01],\n",
       "           [-3.0960e-01, -1.7470e-02,  8.7887e-01,  ...,  5.1052e-02,\n",
       "             2.5594e-02,  2.9455e-01]],\n",
       " \n",
       "          [[ 4.5412e-01,  2.1275e-01,  9.4523e-01,  ...,  3.1706e-01,\n",
       "            -9.1309e-02, -1.1024e+00],\n",
       "           [-4.9588e-01,  7.0700e-02,  1.3606e-01,  ..., -3.5151e-01,\n",
       "             7.6627e-01, -1.1695e-02],\n",
       "           [-2.1596e-01, -6.3807e-01,  9.4792e-02,  ...,  2.4769e-01,\n",
       "             5.0432e-02,  6.5008e-01],\n",
       "           ...,\n",
       "           [-6.1570e-01, -6.5344e-01, -5.6828e-01,  ...,  8.0908e-01,\n",
       "            -3.1163e-01,  9.4652e-01],\n",
       "           [ 3.9411e-01,  4.6378e-01, -3.1187e-01,  ...,  4.4238e-02,\n",
       "            -3.1025e-01,  7.3897e-01],\n",
       "           [-6.7814e-01, -7.0318e-01,  6.9507e-01,  ...,  5.9267e-01,\n",
       "             7.8851e-01, -1.4859e-01]]],\n",
       " \n",
       " \n",
       "         [[[-9.7011e-01, -3.0829e-01,  7.1164e-01,  ...,  5.9091e-01,\n",
       "             1.0315e+00, -4.2774e-01],\n",
       "           [ 6.3745e-01, -2.0630e-01,  4.8254e-02,  ...,  6.5593e-01,\n",
       "             9.9714e-02, -6.4518e-01],\n",
       "           [ 7.4690e-01,  5.1897e-02, -4.9963e-01,  ...,  1.1531e-01,\n",
       "             6.7436e-01, -1.2810e-01],\n",
       "           ...,\n",
       "           [-3.6843e-01, -7.8509e-01, -1.4128e-01,  ...,  5.9527e-02,\n",
       "             7.3755e-01,  1.4399e+00],\n",
       "           [ 4.1963e-01, -2.0314e-01,  3.0711e-01,  ...,  3.7366e-01,\n",
       "            -7.8975e-01, -3.4592e-01],\n",
       "           [ 7.5128e-01, -5.6226e-02, -5.0422e-01,  ...,  2.1699e-01,\n",
       "            -9.4235e-01,  5.0086e-01]],\n",
       " \n",
       "          [[ 2.9452e-01, -6.1172e-01, -3.1327e-01,  ..., -3.7553e-01,\n",
       "            -6.2096e-01,  7.4395e-01],\n",
       "           [ 1.1034e+00, -9.1531e-01,  9.1902e-01,  ..., -1.6669e-01,\n",
       "             1.0841e-01, -4.2211e-01],\n",
       "           [ 4.8189e-01, -6.6786e-01,  7.5689e-01,  ...,  7.1599e-01,\n",
       "            -5.0104e-01, -7.1961e-01],\n",
       "           ...,\n",
       "           [-1.9352e-01,  7.6022e-01, -2.4363e-01,  ..., -1.3148e-01,\n",
       "            -1.0298e-01, -6.8118e-01],\n",
       "           [-7.0211e-01,  5.5013e-01, -3.1877e-03,  ..., -3.6734e-02,\n",
       "            -4.2856e-02,  2.4002e-01],\n",
       "           [ 1.1919e+00,  1.3948e+00, -9.0347e-01,  ..., -1.1662e+00,\n",
       "            -3.4351e-01,  1.9455e-01]],\n",
       " \n",
       "          [[-1.4955e-01,  3.3696e-01,  8.1186e-01,  ...,  2.4269e-01,\n",
       "            -6.8822e-02, -3.2370e-01],\n",
       "           [ 6.3013e-01,  1.9219e-01, -3.0786e-02,  ..., -3.5206e-01,\n",
       "            -5.4673e-01,  1.8336e-01],\n",
       "           [-2.2657e-01, -6.6537e-01,  4.6878e-01,  ...,  5.6510e-01,\n",
       "             1.2678e-02, -9.3718e-01],\n",
       "           ...,\n",
       "           [ 3.2586e-01,  5.4706e-01,  7.6155e-02,  ..., -3.4059e-01,\n",
       "            -3.8103e-01, -6.1824e-01],\n",
       "           [-5.6399e-01,  1.0796e-01, -2.0321e-01,  ...,  1.7246e-01,\n",
       "            -4.3669e-01, -4.5082e-01],\n",
       "           [ 3.3230e-01, -9.2133e-01, -1.2182e-02,  ...,  3.1187e-01,\n",
       "             1.0687e+00, -2.3251e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.7713e-01, -1.2508e-01,  6.0453e-01,  ...,  1.1895e-01,\n",
       "            -1.7329e-01,  4.7198e-01],\n",
       "           [ 1.2222e+00,  1.2061e+00, -9.3918e-02,  ..., -1.1474e-01,\n",
       "             2.7350e-01, -1.0654e-01],\n",
       "           [ 3.0201e-01,  4.9495e-01, -3.2579e-01,  ...,  1.7141e-02,\n",
       "            -2.2638e-01, -1.2925e-01],\n",
       "           ...,\n",
       "           [-4.2790e-01, -9.9577e-01,  2.5442e-02,  ..., -8.1397e-01,\n",
       "             1.3556e-01, -1.2266e+00],\n",
       "           [ 9.4239e-02,  4.9931e-01, -6.1960e-01,  ...,  1.0548e+00,\n",
       "            -1.4361e+00, -2.3296e-01],\n",
       "           [-6.4268e-01, -6.3166e-01, -5.3060e-01,  ...,  7.1051e-01,\n",
       "            -3.5718e-01,  1.0229e+00]],\n",
       " \n",
       "          [[ 1.8063e-01, -1.6694e-01, -1.0110e+00,  ...,  5.7325e-02,\n",
       "             6.5176e-01, -1.1740e-01],\n",
       "           [ 7.8885e-01,  2.5355e-01,  1.6679e-01,  ...,  9.2656e-01,\n",
       "            -6.6669e-01, -1.3674e+00],\n",
       "           [ 1.1370e+00, -3.4173e-01,  2.9488e-01,  ..., -4.9017e-01,\n",
       "            -6.3336e-01,  5.2362e-01],\n",
       "           ...,\n",
       "           [ 1.1741e-02,  1.3039e-01, -7.4147e-01,  ..., -9.4010e-01,\n",
       "             3.2841e-01,  3.1050e-01],\n",
       "           [-3.8632e-01,  5.5935e-01, -1.1053e-01,  ...,  9.5809e-01,\n",
       "             8.4088e-02,  5.1542e-01],\n",
       "           [-3.5352e-01,  6.0395e-02,  9.1084e-01,  ...,  1.1686e-01,\n",
       "             1.9488e-02,  3.4361e-01]],\n",
       " \n",
       "          [[ 4.7463e-01,  9.5534e-02,  1.0149e+00,  ...,  8.1923e-02,\n",
       "            -1.3100e-01, -1.1596e+00],\n",
       "           [-3.8681e-01, -8.3757e-02,  1.9083e-01,  ..., -3.4438e-01,\n",
       "             7.2209e-01, -8.9926e-02],\n",
       "           [-8.4746e-02, -6.1526e-01,  2.0286e-01,  ...,  2.5101e-01,\n",
       "             1.1155e-01,  6.0217e-01],\n",
       "           ...,\n",
       "           [-5.6767e-01, -6.3639e-01, -5.6960e-01,  ...,  7.8986e-01,\n",
       "            -3.0859e-01,  9.4773e-01],\n",
       "           [ 4.1294e-01,  5.3852e-01, -2.5475e-01,  ..., -4.6580e-02,\n",
       "            -2.7395e-01,  7.8071e-01],\n",
       "           [-5.4784e-01, -4.8317e-01,  7.3069e-01,  ...,  5.3266e-01,\n",
       "             7.8442e-01, -1.6980e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.9152e-01, -1.3184e-01,  6.7985e-01,  ...,  6.8984e-01,\n",
       "             1.0262e+00, -5.6344e-01],\n",
       "           [ 5.8552e-01, -1.0600e-01,  1.4889e-01,  ...,  8.0321e-01,\n",
       "            -5.1218e-02, -7.7818e-01],\n",
       "           [ 7.4610e-01,  1.2430e-01, -5.0895e-01,  ...,  4.0303e-01,\n",
       "             6.8882e-01, -3.1210e-02],\n",
       "           ...,\n",
       "           [-3.3441e-01, -7.9679e-01, -2.3147e-01,  ...,  4.9433e-02,\n",
       "             7.1783e-01,  1.3840e+00],\n",
       "           [ 5.0690e-01, -1.8347e-01,  2.5904e-01,  ...,  4.7011e-01,\n",
       "            -8.5610e-01, -4.0412e-01],\n",
       "           [ 7.1551e-01, -4.6112e-02, -5.8311e-01,  ...,  3.2374e-01,\n",
       "            -9.1736e-01,  4.0374e-01]],\n",
       " \n",
       "          [[ 5.7474e-02, -6.4516e-01, -2.2632e-01,  ..., -3.4293e-01,\n",
       "            -6.0103e-01,  6.8661e-01],\n",
       "           [ 1.0803e+00, -8.6067e-01,  7.5956e-01,  ..., -1.2038e-01,\n",
       "             9.3141e-02, -3.8145e-01],\n",
       "           [ 4.5801e-01, -6.9566e-01,  7.1295e-01,  ...,  6.3929e-01,\n",
       "            -4.7468e-01, -6.8692e-01],\n",
       "           ...,\n",
       "           [-9.9815e-02,  7.5648e-01, -2.5669e-01,  ..., -4.8886e-02,\n",
       "            -1.4207e-01, -8.6910e-01],\n",
       "           [-7.2953e-01,  4.7306e-01,  7.2944e-02,  ...,  4.4695e-02,\n",
       "            -1.6351e-01,  1.3608e-01],\n",
       "           [ 1.1641e+00,  1.3996e+00, -8.6860e-01,  ..., -1.0173e+00,\n",
       "            -3.1756e-01,  1.0278e-01]],\n",
       " \n",
       "          [[-2.0982e-01,  2.8792e-01,  1.0493e+00,  ...,  1.2754e-01,\n",
       "            -8.8317e-02, -3.3256e-01],\n",
       "           [ 5.1485e-01,  3.5354e-01,  1.0916e-01,  ..., -2.9297e-01,\n",
       "            -4.5191e-01,  2.5227e-01],\n",
       "           [-1.9845e-01, -6.0314e-01,  6.1303e-01,  ...,  4.8805e-01,\n",
       "            -1.8560e-02, -9.1221e-01],\n",
       "           ...,\n",
       "           [ 1.5109e-01,  5.9132e-01,  5.0771e-02,  ..., -2.4777e-01,\n",
       "            -3.4055e-01, -6.8389e-01],\n",
       "           [-5.6459e-01,  9.0444e-02, -2.5171e-01,  ...,  1.0598e-01,\n",
       "            -4.8897e-01, -5.3015e-01],\n",
       "           [ 2.6810e-01, -9.2278e-01, -1.8891e-02,  ...,  3.9664e-01,\n",
       "             1.0988e+00, -1.6151e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-4.6312e-01, -1.1867e-01,  7.7356e-01,  ...,  1.5433e-01,\n",
       "            -3.9787e-01,  5.3811e-01],\n",
       "           [ 1.4208e+00,  1.1313e+00, -5.3125e-02,  ...,  8.6836e-02,\n",
       "             1.8029e-01, -9.1945e-02],\n",
       "           [ 3.7975e-01,  3.5927e-01, -3.5508e-01,  ...,  1.9674e-01,\n",
       "            -2.5916e-01, -3.5827e-01],\n",
       "           ...,\n",
       "           [-4.0897e-01, -1.0570e+00,  1.6746e-02,  ..., -7.8780e-01,\n",
       "             7.7573e-02, -1.2613e+00],\n",
       "           [ 1.2088e-01,  3.8890e-01, -6.7642e-01,  ...,  1.1452e+00,\n",
       "            -1.3466e+00, -2.4664e-01],\n",
       "           [-5.7808e-01, -7.4542e-01, -4.6213e-01,  ...,  6.9828e-01,\n",
       "            -3.7137e-01,  1.0533e+00]],\n",
       " \n",
       "          [[ 5.3904e-02, -1.7591e-01, -1.2503e+00,  ...,  3.5218e-01,\n",
       "             6.4664e-01,  6.7842e-02],\n",
       "           [ 5.9006e-01, -1.9486e-02,  1.8543e-01,  ...,  1.0764e+00,\n",
       "            -6.6381e-01, -1.3983e+00],\n",
       "           [ 1.0793e+00, -2.9908e-01,  2.1382e-01,  ..., -5.0174e-01,\n",
       "            -8.2713e-01,  5.5436e-01],\n",
       "           ...,\n",
       "           [ 1.2637e-02,  1.2629e-01, -6.5513e-01,  ..., -9.3866e-01,\n",
       "             2.9121e-01,  2.0409e-01],\n",
       "           [-3.7266e-01,  5.4561e-01, -1.0708e-01,  ...,  9.8311e-01,\n",
       "             8.5690e-02,  4.8578e-01],\n",
       "           [-4.1917e-01,  2.4735e-02,  8.2053e-01,  ...,  2.0040e-01,\n",
       "             2.0063e-02,  2.2400e-01]],\n",
       " \n",
       "          [[ 5.2225e-01,  2.9160e-01,  1.1302e+00,  ...,  3.1990e-01,\n",
       "            -1.0802e-01, -1.1819e+00],\n",
       "           [-2.8530e-01,  5.6033e-02,  5.0056e-01,  ..., -1.6807e-01,\n",
       "             8.6234e-01, -2.5380e-01],\n",
       "           [ 3.2993e-02, -5.2963e-01,  4.2124e-01,  ...,  8.9625e-02,\n",
       "             2.0733e-01,  5.0400e-01],\n",
       "           ...,\n",
       "           [-4.5518e-01, -7.3030e-01, -5.3455e-01,  ...,  7.5228e-01,\n",
       "            -3.1505e-01,  9.9725e-01],\n",
       "           [ 4.9069e-01,  4.2840e-01, -2.9857e-01,  ...,  3.0249e-02,\n",
       "            -3.3832e-01,  6.4452e-01],\n",
       "           [-5.6063e-01, -4.9600e-01,  6.4369e-01,  ...,  5.0484e-01,\n",
       "             7.2131e-01, -1.0587e-01]]],\n",
       " \n",
       " \n",
       "         [[[-5.4109e-01, -4.0802e-01,  6.3270e-01,  ...,  6.3269e-01,\n",
       "             7.9379e-01, -5.1758e-01],\n",
       "           [ 7.2266e-01, -3.5195e-01,  1.9648e-02,  ...,  7.9120e-01,\n",
       "            -1.4776e-01, -5.9794e-01],\n",
       "           [ 8.0847e-01,  3.8113e-01, -6.0300e-01,  ...,  3.0609e-01,\n",
       "             6.1518e-01,  2.8466e-01],\n",
       "           ...,\n",
       "           [-4.1121e-01, -7.3402e-01, -1.3362e-01,  ..., -3.6560e-02,\n",
       "             7.3101e-01,  1.3107e+00],\n",
       "           [ 4.7277e-01, -2.0280e-01,  3.1584e-01,  ...,  5.2577e-01,\n",
       "            -9.0987e-01, -4.8453e-01],\n",
       "           [ 7.8412e-01, -1.0292e-01, -5.0337e-01,  ...,  3.3435e-01,\n",
       "            -8.8506e-01,  3.9839e-01]],\n",
       " \n",
       "          [[-5.2586e-03, -7.9724e-01, -3.9031e-01,  ..., -1.1100e-01,\n",
       "            -8.6108e-01,  8.2922e-01],\n",
       "           [ 9.6861e-01, -9.3773e-01,  5.9442e-01,  ...,  5.4841e-02,\n",
       "            -1.5868e-01, -4.6547e-01],\n",
       "           [ 3.3160e-01, -7.2466e-01,  6.3318e-01,  ...,  6.3092e-01,\n",
       "            -5.0213e-01, -7.2494e-01],\n",
       "           ...,\n",
       "           [-2.0655e-02,  7.6906e-01, -3.3125e-01,  ..., -2.0541e-01,\n",
       "            -1.1164e-01, -9.3424e-01],\n",
       "           [-6.9075e-01,  5.1196e-01,  6.0184e-02,  ...,  8.7643e-02,\n",
       "            -2.7054e-01,  1.4151e-01],\n",
       "           [ 1.2325e+00,  1.4679e+00, -8.2588e-01,  ..., -1.1153e+00,\n",
       "            -2.9325e-01,  3.5200e-02]],\n",
       " \n",
       "          [[-3.7670e-01,  2.4215e-01,  1.1092e+00,  ...,  1.7109e-01,\n",
       "            -1.2860e-01, -2.7428e-01],\n",
       "           [ 5.4794e-01,  1.7421e-01,  2.6559e-01,  ..., -2.7143e-01,\n",
       "            -2.5727e-01,  2.2238e-01],\n",
       "           [-3.0490e-01, -4.4831e-01,  9.7157e-01,  ...,  5.1386e-01,\n",
       "             2.7653e-02, -7.4516e-01],\n",
       "           ...,\n",
       "           [ 8.9498e-02,  6.2358e-01,  1.1037e-02,  ..., -1.9963e-01,\n",
       "            -4.2276e-01, -6.6489e-01],\n",
       "           [-5.8808e-01,  1.6629e-01, -4.5027e-01,  ...,  1.4642e-01,\n",
       "            -4.4536e-01, -5.5065e-01],\n",
       "           [ 1.9054e-01, -9.3628e-01,  8.6957e-02,  ...,  4.3220e-01,\n",
       "             1.0268e+00, -1.1138e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.0444e-01, -2.2137e-01,  7.5291e-01,  ...,  3.6183e-01,\n",
       "            -4.7098e-01,  4.5727e-01],\n",
       "           [ 1.2048e+00,  1.1489e+00,  7.6263e-02,  ...,  2.8197e-01,\n",
       "             1.7317e-01, -3.3340e-01],\n",
       "           [ 2.8416e-01,  2.1758e-01, -3.8503e-01,  ...,  3.4462e-01,\n",
       "            -3.9642e-01, -4.8471e-01],\n",
       "           ...,\n",
       "           [-3.5957e-01, -1.0044e+00,  6.3077e-02,  ..., -8.2789e-01,\n",
       "             8.5183e-02, -1.2226e+00],\n",
       "           [ 1.8687e-01,  3.9003e-01, -7.6914e-01,  ...,  1.2168e+00,\n",
       "            -1.3376e+00, -2.7068e-01],\n",
       "           [-5.9034e-01, -6.1569e-01, -3.5480e-01,  ...,  7.1976e-01,\n",
       "            -4.4005e-01,  1.0806e+00]],\n",
       " \n",
       "          [[ 8.8688e-02, -8.1914e-02, -1.1907e+00,  ...,  2.8860e-01,\n",
       "             7.7134e-01, -1.8808e-01],\n",
       "           [ 5.2788e-01, -1.0195e-01,  7.8769e-02,  ...,  1.2552e+00,\n",
       "            -6.6528e-01, -1.4291e+00],\n",
       "           [ 9.8866e-01, -3.9649e-01,  3.8571e-02,  ..., -3.1221e-01,\n",
       "            -8.6968e-01,  3.7851e-01],\n",
       "           ...,\n",
       "           [ 4.7995e-02,  1.9488e-01, -6.5137e-01,  ..., -9.7426e-01,\n",
       "             2.2907e-01,  2.3050e-01],\n",
       "           [-3.0615e-01,  5.3228e-01, -1.2755e-01,  ...,  9.8138e-01,\n",
       "             1.2283e-01,  4.3910e-01],\n",
       "           [-4.1219e-01,  1.6531e-02,  8.8969e-01,  ...,  1.7255e-01,\n",
       "            -5.2626e-02,  2.3960e-01]],\n",
       " \n",
       "          [[ 5.0430e-01, -3.8167e-02,  1.0161e+00,  ...,  3.6850e-02,\n",
       "            -1.8901e-01, -1.0472e+00],\n",
       "           [-1.5641e-01,  3.1369e-02,  3.1351e-01,  ..., -2.0705e-01,\n",
       "             7.8082e-01, -3.5433e-01],\n",
       "           [ 1.5423e-01, -6.6665e-01,  2.4870e-01,  ..., -8.4288e-02,\n",
       "             1.0304e-01,  5.3923e-01],\n",
       "           ...,\n",
       "           [-5.0060e-01, -6.4718e-01, -4.6891e-01,  ...,  7.7322e-01,\n",
       "            -4.0881e-01,  1.0492e+00],\n",
       "           [ 5.8595e-01,  3.7715e-01, -3.7775e-01,  ...,  4.5123e-02,\n",
       "            -3.8177e-01,  6.3758e-01],\n",
       "           [-4.5707e-01, -5.2392e-01,  6.2878e-01,  ...,  5.3880e-01,\n",
       "             5.9558e-01, -1.4856e-01]]]], grad_fn=<StackBackward0>)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformer_lm(dim=256, vocab_size=128, depth=6, heads=8, dim_head=32, causal=True)\n",
    "model(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 240\n",
    "B = 30\n",
    "x = torch.randint(0, 128, (B, N))\n",
    "mask = torch.ones(B, N).bool()\n",
    "mask[:, 50:] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
